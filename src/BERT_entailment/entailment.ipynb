{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Priprema okoline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ukoliko torch nije instaliran s cudatoolkit, koristiti će se CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Učitavanje competition dataframeova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bodies_data = pd.read_csv('fnc-1/train_bodies.csv')\n",
    "train_stances_data = pd.read_csv('fnc-1/train_stances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody\n",
       "0        0  A small meteorite crashed into a wooded area i...\n",
       "1        4  Last week we hinted at what was to come as Ebo...\n",
       "2        5  (NEWSER) – Wonder how long a Quarter Pounder w...\n",
       "3        6  Posting photos of a gun-toting child online, I...\n",
       "4        7  At least 25 suspected Boko Haram insurgents we..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_bodies_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  Body ID     Stance\n",
       "0  Police find mass graves with at least '15 bodi...      712  unrelated\n",
       "1  Hundreds of Palestinians flee floods in Gaza a...      158      agree\n",
       "2  Christian Bale passes on role of Steve Jobs, a...      137  unrelated\n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...     1034  unrelated\n",
       "4  Spider burrowed through tourist's stomach and ...     1923   disagree"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stances_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spajanje dataframeova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Pizza delivery man gets tipped more than $2,00...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49968</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Pizza delivery man gets $2,000 tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49969</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49972 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Body ID                                        articleBody  \\\n",
       "0            0  A small meteorite crashed into a wooded area i...   \n",
       "1            0  A small meteorite crashed into a wooded area i...   \n",
       "2            0  A small meteorite crashed into a wooded area i...   \n",
       "3            0  A small meteorite crashed into a wooded area i...   \n",
       "4            0  A small meteorite crashed into a wooded area i...   \n",
       "...        ...                                                ...   \n",
       "49967     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49968     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49969     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49970     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49971     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "\n",
       "                                                Headline     Stance  \n",
       "0      Soldier shot, Parliament locked down after gun...  unrelated  \n",
       "1      Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
       "2      Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
       "3       BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
       "4      Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  \n",
       "...                                                  ...        ...  \n",
       "49967  Pizza delivery man gets tipped more than $2,00...      agree  \n",
       "49968                 Pizza delivery man gets $2,000 tip      agree  \n",
       "49969   Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip      agree  \n",
       "49970  Ann Arbor pizza delivery driver surprised with...      agree  \n",
       "49971  Ann Arbor pizza delivery driver surprised with...      agree  \n",
       "\n",
       "[49972 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.merge(train_bodies_data, train_stances_data, left_on = 'Body ID', right_on = 'Body ID')\n",
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Funkcija za razdvajanje teksta u rečenice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Izdvajanje povezanih parova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Small Meteorite Strikes in Nicaragua's Capital...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4</td>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "      <td>It Begins: HazMat-Wearing Passenger Spotted At...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "      <td>20-Year-Old Quarter Pounder Looks About the Same</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "      <td>Two blokes dared to eat 20-year-old burger for...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>6</td>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "      <td>‘The cub of Baghdadi': ISIS reports its younge...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49967</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Pizza delivery man gets tipped more than $2,00...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49968</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Pizza delivery man gets $2,000 tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49969</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49970</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49971</th>\n",
       "      <td>2532</td>\n",
       "      <td>ANN ARBOR, Mich. – A pizza delivery man in Mic...</td>\n",
       "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13427 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Body ID                                        articleBody  \\\n",
       "24           0  A small meteorite crashed into a wooded area i...   \n",
       "36           4  Last week we hinted at what was to come as Ebo...   \n",
       "69           5  (NEWSER) – Wonder how long a Quarter Pounder w...   \n",
       "88           5  (NEWSER) – Wonder how long a Quarter Pounder w...   \n",
       "93           6  Posting photos of a gun-toting child online, I...   \n",
       "...        ...                                                ...   \n",
       "49967     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49968     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49969     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49970     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "49971     2532  ANN ARBOR, Mich. – A pizza delivery man in Mic...   \n",
       "\n",
       "                                                Headline   Stance  \n",
       "24     Small Meteorite Strikes in Nicaragua's Capital...    agree  \n",
       "36     It Begins: HazMat-Wearing Passenger Spotted At...  discuss  \n",
       "69      20-Year-Old Quarter Pounder Looks About the Same  discuss  \n",
       "88     Two blokes dared to eat 20-year-old burger for...  discuss  \n",
       "93     ‘The cub of Baghdadi': ISIS reports its younge...  discuss  \n",
       "...                                                  ...      ...  \n",
       "49967  Pizza delivery man gets tipped more than $2,00...    agree  \n",
       "49968                 Pizza delivery man gets $2,000 tip    agree  \n",
       "49969   Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip    agree  \n",
       "49970  Ann Arbor pizza delivery driver surprised with...    agree  \n",
       "49971  Ann Arbor pizza delivery driver surprised with...    agree  \n",
       "\n",
       "[13427 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_df = df_all[df_all.Stance != 'unrelated']\n",
    "related_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train - Test podjela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(columns = ['Sentence', 'Headline', 'Stance'])\n",
    "val_df = pd.DataFrame(columns = ['Sentence', 'Headline', 'Stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "related_df.reset_index()\n",
    "\n",
    "i = 0\n",
    "k = 0\n",
    "t = 1\n",
    "for index, row in related_df.iterrows():\n",
    "    sentences = split_into_sentences(row['articleBody'])    \n",
    "\n",
    "    if(len(sentences) != 0):\n",
    "        sentence = sentences[0]\n",
    "        j = 1\n",
    "        while(len(sentence.split()) < 7):\n",
    "            if(j >= len(sentences)):\n",
    "                break\n",
    "            sentence += sentences[j]\n",
    "            j += 1\n",
    "    else:\n",
    "        sentence = row['articleBody']\n",
    "\n",
    "    if(t < 5):\n",
    "        train_df.loc[i] = [sentence, row['Headline'], row['Stance']]\n",
    "        t += 1\n",
    "        i += 1\n",
    "    else:\n",
    "        val_df.loc[k] = [sentence, row['Headline'], row['Stance']]\n",
    "        t = 1\n",
    "        k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Small Meteorite Strikes in Nicaragua's Capital...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Last week we hinted at what was to come as Ebo...</td>\n",
       "      <td>It Begins: HazMat-Wearing Passenger Spotted At...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "      <td>20-Year-Old Quarter Pounder Looks About the Same</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(NEWSER) – Wonder how long a Quarter Pounder w...</td>\n",
       "      <td>Two blokes dared to eat 20-year-old burger for...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>At least 25 suspected Boko Haram insurgents we...</td>\n",
       "      <td>Insurgents killed in Nigeria despite alleged t...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10737</th>\n",
       "      <td>ANN ARBOR, Mich.– A pizza delivery man in Mich...</td>\n",
       "      <td>Pizza delivery driver surprised with $2,000 tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10738</th>\n",
       "      <td>ANN ARBOR, Mich.– A pizza delivery man in Mich...</td>\n",
       "      <td>Pizza delivery man gets tipped more than $2,00...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10739</th>\n",
       "      <td>ANN ARBOR, Mich.– A pizza delivery man in Mich...</td>\n",
       "      <td>Pizza delivery man gets $2,000 tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10740</th>\n",
       "      <td>ANN ARBOR, Mich.– A pizza delivery man in Mich...</td>\n",
       "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10741</th>\n",
       "      <td>ANN ARBOR, Mich.– A pizza delivery man in Mich...</td>\n",
       "      <td>Ann Arbor pizza delivery driver surprised with...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10742 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sentence  \\\n",
       "0      A small meteorite crashed into a wooded area i...   \n",
       "1      Last week we hinted at what was to come as Ebo...   \n",
       "2      (NEWSER) – Wonder how long a Quarter Pounder w...   \n",
       "3      (NEWSER) – Wonder how long a Quarter Pounder w...   \n",
       "4      At least 25 suspected Boko Haram insurgents we...   \n",
       "...                                                  ...   \n",
       "10737  ANN ARBOR, Mich.– A pizza delivery man in Mich...   \n",
       "10738  ANN ARBOR, Mich.– A pizza delivery man in Mich...   \n",
       "10739  ANN ARBOR, Mich.– A pizza delivery man in Mich...   \n",
       "10740  ANN ARBOR, Mich.– A pizza delivery man in Mich...   \n",
       "10741  ANN ARBOR, Mich.– A pizza delivery man in Mich...   \n",
       "\n",
       "                                                Headline   Stance  \n",
       "0      Small Meteorite Strikes in Nicaragua's Capital...    agree  \n",
       "1      It Begins: HazMat-Wearing Passenger Spotted At...  discuss  \n",
       "2       20-Year-Old Quarter Pounder Looks About the Same  discuss  \n",
       "3      Two blokes dared to eat 20-year-old burger for...  discuss  \n",
       "4      Insurgents killed in Nigeria despite alleged t...  discuss  \n",
       "...                                                  ...      ...  \n",
       "10737    Pizza delivery driver surprised with $2,000 tip    agree  \n",
       "10738  Pizza delivery man gets tipped more than $2,00...    agree  \n",
       "10739                 Pizza delivery man gets $2,000 tip    agree  \n",
       "10740  Ann Arbor pizza delivery driver surprised with...    agree  \n",
       "10741  Ann Arbor pizza delivery driver surprised with...    agree  \n",
       "\n",
       "[10742 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Posting photos of a gun-toting child online, I...</td>\n",
       "      <td>‘The cub of Baghdadi': ISIS reports its younge...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- A meteorite crashed down in Managua, ...</td>\n",
       "      <td>Small Meteorite Strikes in Nicaragua's Capital...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mboxCreate('FoxNews-Politics-Autoplay-Videos-I...</td>\n",
       "      <td>US probing claims ISIS fighters seized airdrop...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO's subscription streaming service will be c...</td>\n",
       "      <td>HBO streaming service could launch in April fo...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eran Cicurel, an editor at Voice of Israel, ha...</td>\n",
       "      <td>YPG Confirms: Gill Rosenberg Not Captured in K...</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2680</th>\n",
       "      <td>More than 200 schoolgirls were kidnapped in Ap...</td>\n",
       "      <td>Nigeria claims deal with Boko Haram on ceasefi...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2681</th>\n",
       "      <td>More than 200 schoolgirls were kidnapped in Ap...</td>\n",
       "      <td>Boko Haram claims to have German hostage, deni...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2682</th>\n",
       "      <td>A Guantanamo Bay prisoner released last year a...</td>\n",
       "      <td>Senator: Detainees swapped for Bergdahl have c...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2683</th>\n",
       "      <td>A Guantanamo Bay prisoner released last year a...</td>\n",
       "      <td>Official: Gitmo prisoner traded for Bergdahl r...</td>\n",
       "      <td>discuss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2684</th>\n",
       "      <td>ANN ARBOR, Mich.– A pizza delivery man in Mich...</td>\n",
       "      <td>Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip</td>\n",
       "      <td>agree</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2685 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Sentence  \\\n",
       "0     Posting photos of a gun-toting child online, I...   \n",
       "1     (CNN) -- A meteorite crashed down in Managua, ...   \n",
       "2     mboxCreate('FoxNews-Politics-Autoplay-Videos-I...   \n",
       "3     HBO's subscription streaming service will be c...   \n",
       "4     Eran Cicurel, an editor at Voice of Israel, ha...   \n",
       "...                                                 ...   \n",
       "2680  More than 200 schoolgirls were kidnapped in Ap...   \n",
       "2681  More than 200 schoolgirls were kidnapped in Ap...   \n",
       "2682  A Guantanamo Bay prisoner released last year a...   \n",
       "2683  A Guantanamo Bay prisoner released last year a...   \n",
       "2684  ANN ARBOR, Mich.– A pizza delivery man in Mich...   \n",
       "\n",
       "                                               Headline   Stance  \n",
       "0     ‘The cub of Baghdadi': ISIS reports its younge...  discuss  \n",
       "1     Small Meteorite Strikes in Nicaragua's Capital...    agree  \n",
       "2     US probing claims ISIS fighters seized airdrop...  discuss  \n",
       "3     HBO streaming service could launch in April fo...  discuss  \n",
       "4     YPG Confirms: Gill Rosenberg Not Captured in K...    agree  \n",
       "...                                                 ...      ...  \n",
       "2680  Nigeria claims deal with Boko Haram on ceasefi...  discuss  \n",
       "2681  Boko Haram claims to have German hostage, deni...  discuss  \n",
       "2682  Senator: Detainees swapped for Bergdahl have c...  discuss  \n",
       "2683  Official: Gitmo prisoner traded for Bergdahl r...  discuss  \n",
       "2684   Luckiest Pizza Delivery Guy Ever Gets $2,000 Tip    agree  \n",
       "\n",
       "[2685 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priprema podataka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10742\n",
      "2685\n"
     ]
    }
   ],
   "source": [
    "class MNLIDataBert(Dataset):\n",
    "\n",
    "  def __init__(self, train_df, val_df):\n",
    "    self.label_dict = {'disagree': 0, 'discuss': 1, 'agree': 2}\n",
    "\n",
    "    self.train_df = train_df\n",
    "    self.val_df = val_df\n",
    "\n",
    "    self.base_path = '/content/'\n",
    "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # Using a pre-trained BERT tokenizer to encode sentences\n",
    "    self.train_data = None\n",
    "    self.val_data = None\n",
    "    self.init_data()\n",
    "\n",
    "  def init_data(self):\n",
    "    self.train_data = self.load_data(self.train_df)\n",
    "    self.val_data = self.load_data(self.val_df)\n",
    "\n",
    "  def load_data(self, df):\n",
    "    MAX_LEN = 512\n",
    "    token_ids = []\n",
    "    mask_ids = []\n",
    "    seg_ids = []\n",
    "    y = []\n",
    "\n",
    "    premise_list = df['Sentence'].to_list()\n",
    "    hypothesis_list = df['Headline'].to_list()\n",
    "    label_list = df['Stance'].to_list()\n",
    "\n",
    "    for (premise, hypothesis, label) in zip(premise_list, hypothesis_list, label_list):\n",
    "      premise_id = self.tokenizer.encode(premise, add_special_tokens = False)\n",
    "      hypothesis_id = self.tokenizer.encode(hypothesis, add_special_tokens = False)\n",
    "      pair_token_ids = [self.tokenizer.cls_token_id] + premise_id + [self.tokenizer.sep_token_id] + hypothesis_id + [self.tokenizer.sep_token_id]\n",
    "      premise_len = len(premise_id)\n",
    "      hypothesis_len = len(hypothesis_id)\n",
    "\n",
    "      segment_ids = torch.tensor([0] * (premise_len + 2) + [1] * (hypothesis_len + 1))  # sentence 0 and sentence 1\n",
    "      attention_mask_ids = torch.tensor([1] * (premise_len + hypothesis_len + 3))  # mask padded values\n",
    "\n",
    "      token_ids.append(torch.tensor(pair_token_ids))\n",
    "      seg_ids.append(segment_ids)\n",
    "      mask_ids.append(attention_mask_ids)\n",
    "      y.append(self.label_dict[label])\n",
    "    \n",
    "    token_ids = pad_sequence(token_ids, batch_first=True)\n",
    "    mask_ids = pad_sequence(mask_ids, batch_first=True)\n",
    "    seg_ids = pad_sequence(seg_ids, batch_first=True)\n",
    "    y = torch.tensor(y)\n",
    "    dataset = TensorDataset(token_ids, mask_ids, seg_ids, y)\n",
    "    print(len(dataset))\n",
    "    return dataset\n",
    "\n",
    "  def get_data_loaders(self, batch_size=20, shuffle=True):\n",
    "    train_loader = DataLoader(\n",
    "      self.train_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(\n",
    "      self.val_data,\n",
    "      shuffle=shuffle,\n",
    "      batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader\n",
    "  \n",
    "mnli_dataset = MNLIDataBert(train_df, val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader = mnli_dataset.get_data_loaders(batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pokreni preko grafičke kartice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Transformer + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, correct_bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 109,485,316 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_test):\n",
    "  acc = (torch.log_softmax(y_pred, dim=1).argmax(dim=1) == y_test).sum().float() / float(y_test.size(0))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Treniranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pipeline za klasifikaciju"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextClassificationPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rezultati na competition datasetu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getepochpointsandsaveresults(epoch_num):\n",
    "    \n",
    "    print(\"EPOCH = \", epoch_num)\n",
    "    \n",
    "    comp_bodies = pd.read_csv('fnc-1/competition_test_bodies.csv')\n",
    "    comp_stances = pd.read_csv('fnc-1/competition_test_stances.csv')\n",
    "\n",
    "    comp_merged = pd.merge(comp_bodies, comp_stances, left_on = 'Body ID', right_on = 'Body ID')\n",
    "    comp_merged_related = comp_merged[comp_merged.Stance != 'unrelated']\n",
    "\n",
    "    pipe = TextClassificationPipeline(model=model, \n",
    "                                  tokenizer=BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True),\n",
    "                                  device=0)\n",
    "\n",
    "    guess_df = pd.DataFrame(columns = ['Guess'])\n",
    "    comp_merged_related.reset_index()\n",
    "    guess_df.reset_index()\n",
    "\n",
    "    tok = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "    i = 0\n",
    "    for index, row in comp_merged_related.iterrows():\n",
    "\n",
    "        sentences = split_into_sentences(row['articleBody'])    \n",
    "\n",
    "        if(len(sentences) != 0):\n",
    "            sentence = sentences[0]\n",
    "            j = 1\n",
    "            while(len(sentence.split()) < 7):\n",
    "                if(j >= len(sentences)):\n",
    "                    break\n",
    "                sentence += sentences[j]\n",
    "                j += 1\n",
    "        else:\n",
    "            sentence = row['articleBody']\n",
    "\n",
    "\n",
    "        sentence_id = tok.encode(sentence, add_special_tokens = False)\n",
    "        headline_id = tok.encode(row['Headline'], add_special_tokens = False)\n",
    "        merged_sentence = \"[CLS] \" + sentence + \" [SEP] \" + row['Headline'] + \" [SEP]\"\n",
    "\n",
    "        t = (pipe(merged_sentence)[0]['label'])\n",
    "        if (t=='LABEL_0'):\n",
    "            guess_df.loc[i] = 'disagree'\n",
    "        elif (t == 'LABEL_1'):\n",
    "            guess_df.loc[i] = 'discuss'\n",
    "        else:\n",
    "            guess_df.loc[i] = 'agree'\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    comp_clean = pd.DataFrame(columns = ['Body ID', 'articleBody', 'Headline', 'Stance'])\n",
    "\n",
    "    comp_merged_related.reset_index()\n",
    "    i = 0\n",
    "    for index, row in comp_merged_related.iterrows():\n",
    "        comp_clean.loc[i] = [row['Body ID'], row['articleBody'], row['Headline'], row['Stance']]\n",
    "        i += 1\n",
    "\n",
    "    comp_clean = comp_clean.join(guess_df)\n",
    "    \n",
    "    comp_clean.reset_index()\n",
    "\n",
    "    goodagree = 0\n",
    "    badagree = 0\n",
    "    gooddiscuss = 0\n",
    "    baddiscuss = 0\n",
    "    gooddisagree = 0\n",
    "    baddisagree = 0\n",
    "\n",
    "    i = 0\n",
    "    for index, row in comp_clean.iterrows():\n",
    "        if(row['Stance'] == row['Guess']):\n",
    "            if(row['Stance'] == 'agree'):\n",
    "                goodagree += 1\n",
    "            elif(row['Stance'] == 'discuss'):\n",
    "                gooddiscuss += 1\n",
    "            elif(row['Stance'] == 'disagree'):\n",
    "                gooddisagree += 1\n",
    "        else:\n",
    "            if(row['Stance'] == 'agree'):\n",
    "                badagree += 1\n",
    "            elif(row['Stance'] == 'discuss'):\n",
    "                baddiscuss += 1\n",
    "            elif(row['Stance'] == 'disagree'):\n",
    "                baddisagree += 1\n",
    "\n",
    "    print(\"Agree: \", goodagree, \" - \", badagree)\n",
    "    print(\"Discuss: \", gooddiscuss, \" - \", baddiscuss)\n",
    "    print(\"Disagree: \", gooddisagree, \" - \", baddisagree)\n",
    "    \n",
    "    savestr = \"comp_autosave_after_epoch_\" + str(epoch_num) + \".csv\"\n",
    "    comp_clean.to_csv(savestr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Treniranje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "def train(model, train_loader, val_loader, optimizer):  \n",
    "  total_step = len(train_loader)\n",
    "\n",
    "  epoch_num = 1\n",
    "  for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_train_acc  = 0\n",
    "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(train_loader):\n",
    "      optimizer.zero_grad()\n",
    "      pair_token_ids = pair_token_ids.to(device)\n",
    "      mask_ids = mask_ids.to(device)\n",
    "      seg_ids = seg_ids.to(device)\n",
    "      labels = y.to(device)\n",
    "      # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "      loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "\n",
    "      # loss = criterion(prediction, labels)\n",
    "      acc = multi_acc(prediction, labels)\n",
    "\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      total_train_loss += loss.item()\n",
    "      total_train_acc  += acc.item()\n",
    "\n",
    "    train_acc  = total_train_acc/len(train_loader)\n",
    "    train_loss = total_train_loss/len(train_loader)\n",
    "    model.eval()\n",
    "    total_val_acc  = 0\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "      for batch_idx, (pair_token_ids, mask_ids, seg_ids, y) in enumerate(val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        pair_token_ids = pair_token_ids.to(device)\n",
    "        mask_ids = mask_ids.to(device)\n",
    "        seg_ids = seg_ids.to(device)\n",
    "        labels = y.to(device)\n",
    "\n",
    "        # prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
    "        loss, prediction = model(pair_token_ids, \n",
    "                             token_type_ids=seg_ids, \n",
    "                             attention_mask=mask_ids, \n",
    "                             labels=labels).values()\n",
    "        \n",
    "        # loss = criterion(prediction, labels)\n",
    "        acc = multi_acc(prediction, labels)\n",
    "\n",
    "        total_val_loss += loss.item()\n",
    "        total_val_acc  += acc.item()\n",
    "\n",
    "    val_acc  = total_val_acc/len(val_loader)\n",
    "    val_loss = total_val_loss/len(val_loader)\n",
    "    \n",
    "    if(epoch_num == 10):\n",
    "        getepochpointsandsaveresults(epoch_num)\n",
    "    epoch_num += 1\n",
    "    \n",
    "    end = time.time()\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: train_loss: {train_loss:.4f} train_acc: {train_acc:.4f} | val_loss: {val_loss:.4f} val_acc: {val_acc:.4f}')\n",
    "    print(\"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(model, train_loader, val_loader, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1: train_loss: 0.4410 train_acc: 0.8237 | val_loss: 0.2086 val_acc: 0.9294\n",
    "00:18:12.73\n",
    "\n",
    "Epoch 2: train_loss: 0.1123 train_acc: 0.9634 | val_loss: 0.1021 val_acc: 0.9714\n",
    "00:15:47.92\n",
    "\n",
    "Epoch 3: train_loss: 0.0461 train_acc: 0.9866 | val_loss: 0.0981 val_acc: 0.9729\n",
    "00:15:14.07\n",
    "\n",
    "Epoch 4: train_loss: 0.0291 train_acc: 0.9913 | val_loss: 0.1075 val_acc: 0.9706\n",
    "00:15:10.50\n",
    "\n",
    "Epoch 5: train_loss: 0.0224 train_acc: 0.9935 | val_loss: 0.1148 val_acc: 0.9758\n",
    "00:15:27.62\n",
    "\n",
    "Epoch 6: train_loss: 0.0116 train_acc: 0.9967 | val_loss: 0.1107 val_acc: 0.9758\n",
    "00:28:36.22\n",
    "\n",
    "Epoch 7: train_loss: 0.0135 train_acc: 0.9959 | val_loss: 0.1229 val_acc: 0.9747\n",
    "00:29:02.09\n",
    "\n",
    "Epoch 8: train_loss: 0.0189 train_acc: 0.9949 | val_loss: 0.1431 val_acc: 0.9743\n",
    "00:33:40.45\n",
    "\n",
    "Epoch 9: train_loss: 0.0121 train_acc: 0.9965 | val_loss: 0.1328 val_acc: 0.9777\n",
    "00:31:41.29\n",
    "\n",
    "\n",
    "EPOCH =  10  (tocno klasificirani - krivo klasificirani)  \n",
    "Agree:  1014  -  889  \n",
    "Discuss:  3770  -  694  \n",
    "Disagree:  271  -  426  \n",
    "Epoch 10: train_loss: 0.0104 train_acc: 0.9968 | val_loss: 0.1364 val_acc: 0.9784  \n",
    "00:19:58.83"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spremanje modela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"final_model_after_epoch_10.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "withcuda",
   "language": "python",
   "name": "withcuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
