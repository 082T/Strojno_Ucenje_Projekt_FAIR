{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://colab.research.google.com/drive/1ZQvuAVwA3IjybezQOXnrXMGAnMyZRuPU#scrollTo=diVtyCJCurxJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import re\n",
    "import math\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datoteka_za_rad.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody  \\\n",
       "0        0  A small meteorite crashed into a wooded area i...   \n",
       "1        0  A small meteorite crashed into a wooded area i...   \n",
       "2        0  A small meteorite crashed into a wooded area i...   \n",
       "3        0  A small meteorite crashed into a wooded area i...   \n",
       "4        0  A small meteorite crashed into a wooded area i...   \n",
       "\n",
       "                                            Headline     Stance  \n",
       "0  Soldier shot, Parliament locked down after gun...  unrelated  \n",
       "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
       "2  Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
       "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
       "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input Formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Sentence Input:\n",
    "\n",
    "[CLS] The man went to the store. [SEP] He bought a gallon of milk. [SEP]\n",
    "\n",
    "1 Sentence Input:\n",
    "\n",
    "[CLS] The man went to the store. [SEP]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'here', 'is', 'the', 'sentence', 'i', 'want', 'em', '##bed', '##ding', '##s', 'for', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Here is the sentence I want embeddings for.\"\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Tokenize our sentence with the BERT tokenizer.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Print out the tokens.\n",
    "print (tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['knight',\n",
       " 'lap',\n",
       " 'survey',\n",
       " 'ma',\n",
       " '##ow',\n",
       " 'noise',\n",
       " 'billy',\n",
       " '##ium',\n",
       " 'shooting',\n",
       " 'guide',\n",
       " 'bedroom',\n",
       " 'priest',\n",
       " 'resistance',\n",
       " 'motor',\n",
       " 'homes',\n",
       " 'sounded',\n",
       " 'giant',\n",
       " '##mer',\n",
       " '150',\n",
       " 'scenes']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[5000:5020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "after         2,044\n",
      "stealing     11,065\n",
      "money         2,769\n",
      "from          2,013\n",
      "the           1,996\n",
      "bank          2,924\n",
      "vault        11,632\n",
      ",             1,010\n",
      "the           1,996\n",
      "bank          2,924\n",
      "robber       27,307\n",
      "was           2,001\n",
      "seen          2,464\n",
      "fishing       5,645\n",
      "on            2,006\n",
      "the           1,996\n",
      "mississippi   5,900\n",
      "river         2,314\n",
      "bank          2,924\n",
      ".             1,012\n",
      "[SEP]           102\n"
     ]
    }
   ],
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"After stealing money from the bank vault, the bank robber was seen \" \\\n",
    "       \"fishing on the Mississippi river bank.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Mark each of the 22 tokens as belonging to sentence \"1\".\n",
    "segments_ids = [1] * len(tokenized_text)\n",
    "\n",
    "print (segments_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "\n",
    "# Load pre-trained model (weights)\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict hidden states features for each layer\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 12\n",
      "Number of batches: 1\n",
      "Number of tokens: 22\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(encoded_layers))\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers[layer_i][batch_i][token_i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For the 5th token in our sentence, select its feature values from layer 5.\n",
    "token_i = 5\n",
    "layer_i = 5\n",
    "vec = encoded_layers[layer_i][batch_i][token_i]\n",
    "\n",
    "# Plot the values as a histogram to show their distribution.\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.hist(vec, bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Type of encoded_layers:  <class 'list'>\n",
      "Tensor shape for each layer:  torch.Size([1, 22, 768])\n"
     ]
    }
   ],
   "source": [
    "# `encoded_layers` is a Python list.\n",
    "print('     Type of encoded_layers: ', type(encoded_layers))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', encoded_layers[0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 1, 22, 768])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate the tensors for all layers. We use `stack` here to\n",
    "# create a new dimension in the tensor.\n",
    "token_embeddings = torch.stack(encoded_layers, dim=0)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 22, 768])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove dimension 1, the \"batches\".\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([22, 12, 768])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap dimensions 0 and 1.\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "    \n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last \n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    \n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 22 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "    \n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `encoded_layers` has shape [12 x 1 x 22 x 768]\n",
    "\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = encoded_layers[11][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our final sentence embedding vector of shape: torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "print (\"Our final sentence embedding vector of shape:\", sentence_embedding.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [CLS]\n",
      "1 after\n",
      "2 stealing\n",
      "3 money\n",
      "4 from\n",
      "5 the\n",
      "6 bank\n",
      "7 vault\n",
      "8 ,\n",
      "9 the\n",
      "10 bank\n",
      "11 robber\n",
      "12 was\n",
      "13 seen\n",
      "14 fishing\n",
      "15 on\n",
      "16 the\n",
      "17 mississippi\n",
      "18 river\n",
      "19 bank\n",
      "20 .\n",
      "21 [SEP]\n"
     ]
    }
   ],
   "source": [
    "for i, token_str in enumerate(tokenized_text):\n",
    "  print (i, token_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"bank\".\n",
      "\n",
      "bank vault    tensor([ 2.1319, -2.1413, -1.6260,  0.8638,  3.3173])\n",
      "bank robber   tensor([ 1.1868, -1.5298, -1.3770,  1.0648,  3.1446])\n",
      "river bank    tensor([ 1.1295, -1.4724, -0.7296, -0.0901,  2.4970])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"bank\".')\n",
    "print('')\n",
    "print(\"bank vault   \", str(token_vecs_sum[6][:5]))\n",
    "print(\"bank robber  \", str(token_vecs_sum[10][:5]))\n",
    "print(\"river bank   \", str(token_vecs_sum[19][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for  *similar*  meanings:  0.95\n",
      "Vector similarity for *different* meanings:  0.68\n"
     ]
    }
   ],
   "source": [
    "#from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank \n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[19])\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"bank vault\" (same meaning).\n",
    "same_bank = 1 - cosine(token_vecs_sum[10], token_vecs_sum[6])\n",
    "\n",
    "print('Vector similarity for  *similar*  meanings:  %.2f' % same_bank)\n",
    "print('Vector similarity for *different* meanings:  %.2f' % diff_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert na našem data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Body ID</th>\n",
       "      <th>articleBody</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Stance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Soldier shot, Parliament locked down after gun...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Tourist dubbed ‘Spider Man’ after spider burro...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Luke Somers 'killed in failed rescue attempt i...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>BREAKING: Soldier shot at War Memorial in Ottawa</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>A small meteorite crashed into a wooded area i...</td>\n",
       "      <td>Giant 8ft 9in catfish weighing 19 stone caught...</td>\n",
       "      <td>unrelated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Body ID                                        articleBody  \\\n",
       "0        0  A small meteorite crashed into a wooded area i...   \n",
       "1        0  A small meteorite crashed into a wooded area i...   \n",
       "2        0  A small meteorite crashed into a wooded area i...   \n",
       "3        0  A small meteorite crashed into a wooded area i...   \n",
       "4        0  A small meteorite crashed into a wooded area i...   \n",
       "\n",
       "                                            Headline     Stance  \n",
       "0  Soldier shot, Parliament locked down after gun...  unrelated  \n",
       "1  Tourist dubbed ‘Spider Man’ after spider burro...  unrelated  \n",
       "2  Luke Somers 'killed in failed rescue attempt i...  unrelated  \n",
       "3   BREAKING: Soldier shot at War Memorial in Ottawa  unrelated  \n",
       "4  Giant 8ft 9in catfish weighing 19 stone caught...  unrelated  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): BertLayerNorm()\n",
       "    (dropout): Dropout(p=0.1)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): BertLayerNorm()\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): BertLayerNorm()\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbert(sentence):\n",
    "    marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_sentence = tokenizer.tokenize(marked_sentence)\n",
    "    indexed_tokens_sentence = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "    segments_ids_sentence = [1] * len(tokenized_sentence)\n",
    "    tokens_tensor_sentence = torch.tensor([indexed_tokens_sentence])\n",
    "    segments_tensors_sentence = torch.tensor([segments_ids_sentence])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers_sentence, _ = model(tokens_tensor_sentence, segments_tensors_sentence)\n",
    "    token_vecs_sentence = encoded_layers_sentence[11][0]\n",
    "    sentence_embedding = torch.mean(token_vecs_sentence, dim=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov)\"\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results = pd.DataFrame(columns = ['bert', 'Stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()\n",
    "\n",
    "i=0\n",
    "for index, row in df.iterrows():\n",
    "    j=0\n",
    "    #get headline_embedding\n",
    "    title = row['Headline']\n",
    "    headline_embedding = getbert(title)\n",
    "    \n",
    "    sum_sim = 0\n",
    "    \n",
    "    for sentence in split_into_sentences(row['articleBody']):\n",
    "        j += 1\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        sum_sim += cos_sim\n",
    "        \n",
    "    if(j == 0):\n",
    "        sentence_embedding = getbert(row['articleBody'])\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        j += 1\n",
    "        sum_sim += cos_sim\n",
    "    \n",
    "    avg_sim = sum_sim / j\n",
    "    \n",
    "    bert_results.loc[i] = [avg_sim, row['Stance']]\n",
    "    i += 1\n",
    "    if(i%100 == 0):\n",
    "        print(i)\n",
    "    if(i%2000 == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_related = bert_results[bert_results.Stance == 'related']\n",
    "bert_results_unrelated = bert_results[bert_results.Stance == 'unrelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.601992\n",
       "0.10    0.617005\n",
       "0.15    0.623843\n",
       "0.20    0.633216\n",
       "0.25    0.638187\n",
       "0.30    0.641688\n",
       "0.35    0.646916\n",
       "0.40    0.652417\n",
       "0.45    0.661294\n",
       "0.50    0.668464\n",
       "0.55    0.672384\n",
       "0.60    0.683943\n",
       "0.65    0.687291\n",
       "0.70    0.694414\n",
       "0.75    0.704689\n",
       "0.80    0.708743\n",
       "0.85    0.714267\n",
       "0.90    0.721413\n",
       "0.95    0.734654\n",
       "Name: bert, dtype: float64"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_related['bert'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.503861\n",
       "0.10    0.521905\n",
       "0.15    0.533803\n",
       "0.20    0.545632\n",
       "0.25    0.553062\n",
       "0.30    0.560425\n",
       "0.35    0.566717\n",
       "0.40    0.574215\n",
       "0.45    0.580840\n",
       "0.50    0.586928\n",
       "0.55    0.592725\n",
       "0.60    0.599948\n",
       "0.65    0.606374\n",
       "0.70    0.613533\n",
       "0.75    0.620599\n",
       "0.80    0.628913\n",
       "0.85    0.637321\n",
       "0.90    0.647946\n",
       "0.95    0.668741\n",
       "Name: bert, dtype: float64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_unrelated['bert'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_max = pd.DataFrame(columns = ['bert', 'Stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n",
      "450\n",
      "500\n"
     ]
    }
   ],
   "source": [
    "df.reset_index()\n",
    "\n",
    "i=0\n",
    "for index, row in df.iterrows():\n",
    "    j=0\n",
    "    #get headline_embedding\n",
    "    title = row['Headline']\n",
    "    headline_embedding = getbert(title)\n",
    "    \n",
    "    max_sim = 0\n",
    "    \n",
    "    for sentence in split_into_sentences(row['articleBody']):\n",
    "        j += 1\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        if(cos_sim > max_sim):\n",
    "            max_sim = cos_sim\n",
    "        \n",
    "    if(j == 0):\n",
    "        sentence_embedding = getbert(row['articleBody'])\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        j += 1\n",
    "        max_sim = cos_sim\n",
    "    \n",
    "    bert_results_max.loc[i] = [max_sim, row['Stance']]\n",
    "    i += 1\n",
    "    if(i%50 == 0):\n",
    "        print(i)\n",
    "    if(i%500 == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_max_related = bert_results[bert_results.Stance == 'related']\n",
    "bert_results_max_unrelated = bert_results[bert_results.Stance == 'unrelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.601992\n",
       "0.10    0.617005\n",
       "0.15    0.623843\n",
       "0.20    0.633216\n",
       "0.25    0.638187\n",
       "0.30    0.641688\n",
       "0.35    0.646916\n",
       "0.40    0.652417\n",
       "0.45    0.661294\n",
       "0.50    0.668464\n",
       "0.55    0.672384\n",
       "0.60    0.683943\n",
       "0.65    0.687291\n",
       "0.70    0.694414\n",
       "0.75    0.704689\n",
       "0.80    0.708743\n",
       "0.85    0.714267\n",
       "0.90    0.721413\n",
       "0.95    0.734654\n",
       "Name: bert, dtype: float64"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_max_related['bert'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.503861\n",
       "0.10    0.521905\n",
       "0.15    0.533803\n",
       "0.20    0.545632\n",
       "0.25    0.553062\n",
       "0.30    0.560425\n",
       "0.35    0.566717\n",
       "0.40    0.574215\n",
       "0.45    0.580840\n",
       "0.50    0.586928\n",
       "0.55    0.592725\n",
       "0.60    0.599948\n",
       "0.65    0.606374\n",
       "0.70    0.613533\n",
       "0.75    0.620599\n",
       "0.80    0.628913\n",
       "0.85    0.637321\n",
       "0.90    0.647946\n",
       "0.95    0.668741\n",
       "Name: bert, dtype: float64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_max_unrelated['bert'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############### output for first title:\n",
    "\n",
    "print (\"Number of layers:\", len(encoded_layers_title))\n",
    "\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(encoded_layers_title[layer_i]))\n",
    "\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(encoded_layers_title[layer_i][batch_i]))\n",
    "\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(encoded_layers_title[layer_i][batch_i][token_i]))\n",
    "\n",
    "Number of layers: 12\n",
    "\n",
    "Number of batches: 1\n",
    "\n",
    "Number of tokens: 16\n",
    "\n",
    "Number of hidden units: 768\n",
    "\n",
    "###############\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    #get headline\n",
    "    title = row['Headline']\n",
    "    #mark with CLS and SEP\n",
    "    marked_title = \"[CLS] \" + title + \" [SEP]\"\n",
    "    #tokenize words\n",
    "    tokenized_title = tokenizer.tokenize(marked_title)\n",
    "    #get token values\n",
    "    indexed_tokens_title = tokenizer.convert_tokens_to_ids(tokenized_title)\n",
    "    \n",
    "    #print\n",
    "    #for tup in zip(tokenized_text, indexed_tokens):\n",
    "    #    print('{:<12} {:>6,}'.format(tup[0], tup[1]))\n",
    "    \n",
    "    # 1 1 1 1 1 1..\n",
    "    segments_ids_title = [1] * len(tokenized_title)\n",
    "    \n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor_title = torch.tensor([indexed_tokens_title])\n",
    "    segments_tensors_title = torch.tensor([segments_ids_title])\n",
    "    \n",
    "    # Predict hidden states features for each layer\n",
    "    with torch.no_grad():\n",
    "        encoded_layers_title, _ = model(tokens_tensor_title, segments_tensors_title)\n",
    "        \n",
    "    ##############output from here#############\n",
    "    \n",
    "    # Combine 12 layers for one big tensor\n",
    "    ##token_embeddings_title = torch.stack(encoded_layers_title, dim=0)\n",
    "    \n",
    "    #print(token_embeddings_title.size()) # 12 1 16 768\n",
    "    \n",
    "    #remove batches dimension\n",
    "    ##token_embeddings_title = torch.squeeze(token_embeddings_title, dim=1)\n",
    "    #print(token_embeddings_title.size()) # torch.Size([12, 16, 768])\n",
    "    \n",
    "    # Swap dimensions 0 and 1.\n",
    "    ##token_embeddings_title = token_embeddings_title.permute(1,0,2)\n",
    "    #print(token_embeddings_title.size()) # torch.Size([16, 12, 768])\n",
    "    \n",
    "    # Stores the token vectors, with shape [22 x 3,072]\n",
    "    ##token_vecs_cat_title = []\n",
    "\n",
    "    # For each token in the sentence... concatenate last 4 layers\n",
    "    ##for token in token_embeddings_title:\n",
    "    ##    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "    ##    token_vecs_cat_title.append(cat_vec)\n",
    "\n",
    "    # print ('Shape is: %d x %d' % (len(token_vecs_cat_title), len(token_vecs_cat_title[0]))) # Shape is: 16 x 3072\n",
    "    \n",
    "    token_vecs_title = encoded_layers_title[11][0]\n",
    "    \n",
    "    sentence_embedding_title = torch.mean(token_vecs_title, dim=0)\n",
    "    #print(sentence_embedding_title.size()) # torch.Size([768])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=================================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bert na cijelom paragrafu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-cd109c05954d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mheadline_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0marticle_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'articleBody'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mcos_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline_embedding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marticle_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-34-239b562e6689>\u001b[0m in \u001b[0;36mgetbert\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msegments_tensors_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msegments_ids_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mencoded_layers_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_tensor_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegments_tensors_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtoken_vecs_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_layers_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msentence_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_vecs_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[0;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[0;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bert_whole_results = pd.DataFrame(columns = ['bert', 'Stance'])\n",
    "\n",
    "df.reset_index()\n",
    "\n",
    "i=0\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    title = row['Headline']\n",
    "    headline_embedding = getbert(title)\n",
    "    \n",
    "    article_embedding = getbert(row['articleBody'])\n",
    "    \n",
    "    cos_sim = 1 - cosine(headline_embedding, article_embedding)\n",
    "        \n",
    "    bert_whole_results.loc[i] = [cos_sim, row['Stance']]\n",
    "    i += 1\n",
    "    print(i)\n",
    "    \n",
    "    if(i%2000 == 0):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_results_whole_related = bert_whole_results[bert_whole_results.Stance == 'related']\n",
    "bert_results_whole_unrelated = bert_whole_results[bert_whole_results.Stance == 'unrelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.652378\n",
       "0.10    0.666382\n",
       "0.15    0.680386\n",
       "0.20    0.694390\n",
       "0.25    0.708393\n",
       "0.30    0.722397\n",
       "0.35    0.732231\n",
       "0.40    0.733726\n",
       "0.45    0.735222\n",
       "0.50    0.736717\n",
       "0.55    0.738212\n",
       "0.60    0.739707\n",
       "0.65    0.741202\n",
       "0.70    0.743244\n",
       "0.75    0.745558\n",
       "0.80    0.747873\n",
       "0.85    0.750187\n",
       "0.90    0.752501\n",
       "0.95    0.754816\n",
       "Name: bert, dtype: float64"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_whole_related['bert'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.535679\n",
       "0.10    0.554682\n",
       "0.15    0.566562\n",
       "0.20    0.580952\n",
       "0.25    0.599901\n",
       "0.30    0.604796\n",
       "0.35    0.608460\n",
       "0.40    0.617092\n",
       "0.45    0.620244\n",
       "0.50    0.630781\n",
       "0.55    0.640746\n",
       "0.60    0.650366\n",
       "0.65    0.655986\n",
       "0.70    0.659768\n",
       "0.75    0.666701\n",
       "0.80    0.671129\n",
       "0.85    0.681090\n",
       "0.90    0.695537\n",
       "0.95    0.714389\n",
       "Name: bert, dtype: float64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_results_whole_unrelated['bert'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todo: za svaku recenicu; uzmi max(bez nepoznate rijeci, sa nepoznatnom rijeci //u naslovu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getbert_without(sentence):\n",
    "    marked_sentence = \"[CLS] \" + sentence + \" [SEP]\"\n",
    "    tokenized_sentence = tokenizer.tokenize(marked_sentence)\n",
    "    ###tokenize without '##words'\n",
    "    length = len(tokenized_sentence)\n",
    "    i = 0\n",
    "    while(i<length):\n",
    "        \n",
    "        if(tokenized_sentence[i][0] == '#'):\n",
    "            \n",
    "            while(tokenized_sentence[i][0] == '#'):\n",
    "                tokenized_sentence.pop(i)\n",
    "                length -= 1\n",
    "            tokenized_sentence.pop(i-1)\n",
    "            length -= 1\n",
    "            i -= 1\n",
    "        i += 1\n",
    "        \n",
    "    ##print(tokenized_sentence) #end tokenization\n",
    "    \n",
    "    indexed_tokens_sentence = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
    "    segments_ids_sentence = [1] * len(tokenized_sentence)\n",
    "    tokens_tensor_sentence = torch.tensor([indexed_tokens_sentence])\n",
    "    segments_tensors_sentence = torch.tensor([segments_ids_sentence])\n",
    "    with torch.no_grad():\n",
    "        encoded_layers_sentence, _ = model(tokens_tensor_sentence, segments_tensors_sentence)\n",
    "    token_vecs_sentence = encoded_layers_sentence[11][0]\n",
    "    sentence_embedding = torch.mean(token_vecs_sentence, dim=0)\n",
    "    return sentence_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_napredak_results = pd.DataFrame(columns = ['sa-sa', 'sa-bez', 'bez-sa', 'bez-bez', 'Stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-820a76ec8fb5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;31m## bez sa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msentence_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetbert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[0mcos_sim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcosine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadline_embedding_without\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_embedding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0msim_bs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-239b562e6689>\u001b[0m in \u001b[0;36mgetbert\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msegments_tensors_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msegments_ids_sentence\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mencoded_layers_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens_tensor_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegments_tensors_sentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mtoken_vecs_sentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_layers_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0msentence_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken_vecs_sentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[0;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 733\u001b[1;33m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[0;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 406\u001b[1;33m             \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    407\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 393\u001b[1;33m         \u001b[0mlayer_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    394\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\pytorch_pretrained_bert\\modeling.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    476\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 477\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    478\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\modules\\linear.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\berttry\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mlinear\u001b[1;34m(input, weight, bias)\u001b[0m\n\u001b[0;32m   1024\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df.reset_index()\n",
    "\n",
    "i=0\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    j = 0\n",
    "    #get headline_embedding\n",
    "    title = row['Headline']\n",
    "    headline_embedding = getbert(title)\n",
    "    headline_embedding_without = getbert_without(title)\n",
    "    \n",
    "    sim_ss = 0\n",
    "    sim_sb = 0\n",
    "    sim_bs = 0\n",
    "    sim_bb = 0\n",
    "    \n",
    "    for sentence in split_into_sentences(row['articleBody']):\n",
    "        j += 1 \n",
    "        \n",
    "        ## zelimo average u sva 4 slucaja\n",
    "        \n",
    "        ## sa sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        sim_ss += cos_sim\n",
    "            \n",
    "        ## sa bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)    \n",
    "        sim_sb += cos_sim\n",
    "        \n",
    "        ## bez sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        sim_bs += cos_sim\n",
    "        \n",
    "        ## bez bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        sim_bb += cos_sim\n",
    "        \n",
    "    if(j == 0):\n",
    "        j = 1\n",
    "        ## sa sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        sim_ss += cos_sim\n",
    "            \n",
    "        ## sa bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)    \n",
    "        sim_sb += cos_sim\n",
    "        \n",
    "        ## bez sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        sim_bs += cos_sim\n",
    "        \n",
    "        ## bez bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        sim_bb += cos_sim\n",
    "    \n",
    "    sim_ss = sim_ss/j\n",
    "    sim_bs = sim_bs/j\n",
    "    sim_bb = sim_bb/j\n",
    "    sim_sb = sim_sb/j\n",
    "    \n",
    "    bert_napredak_results.loc[i] = [sim_ss, sim_sb, sim_bs, sim_bb, row['Stance']]\n",
    "    i += 1\n",
    "    if(i%5 == 0):\n",
    "        print(i)\n",
    "    if(i>=300):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_napredak_results.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_napredak_results_max = pd.DataFrame(columns = ['sa-sa', 'sa-bez', 'bez-sa', 'bez-bez', 'Stance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "10\n",
      "15\n",
      "20\n",
      "25\n",
      "30\n",
      "35\n",
      "40\n",
      "45\n",
      "50\n",
      "55\n",
      "60\n",
      "65\n",
      "70\n",
      "75\n",
      "80\n",
      "85\n",
      "90\n",
      "95\n",
      "100\n",
      "105\n",
      "110\n",
      "115\n",
      "120\n",
      "125\n",
      "130\n",
      "135\n",
      "140\n",
      "145\n",
      "150\n",
      "155\n",
      "160\n",
      "165\n",
      "170\n",
      "175\n",
      "180\n",
      "185\n",
      "190\n",
      "195\n",
      "200\n",
      "205\n",
      "210\n",
      "215\n",
      "220\n",
      "225\n",
      "230\n",
      "235\n",
      "240\n",
      "245\n",
      "250\n",
      "255\n",
      "260\n",
      "265\n",
      "270\n",
      "275\n",
      "280\n",
      "285\n",
      "290\n",
      "295\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "df.reset_index()\n",
    "\n",
    "i=0\n",
    "for index, row in df.iterrows():\n",
    "    \n",
    "    j = 0\n",
    "    #get headline_embedding\n",
    "    title = row['Headline']\n",
    "    headline_embedding = getbert(title)\n",
    "    headline_embedding_without = getbert_without(title)\n",
    "    \n",
    "    if(row['Stance'] == 'unrelated'):\n",
    "            continue\n",
    "            \n",
    "    max_ss = 0\n",
    "    max_sb = 0\n",
    "    max_bs = 0\n",
    "    max_bb = 0\n",
    "    \n",
    "    for sentence in split_into_sentences(row['articleBody']):\n",
    "        j += 1 \n",
    "        \n",
    "        ## zelimo average u sva 4 slucaja\n",
    "        \n",
    "        ## sa sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        if(max_ss < cos_sim):\n",
    "            max_ss = cos_sim\n",
    "            \n",
    "        ## sa bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)    \n",
    "        if(max_sb < cos_sim):\n",
    "            max_sb = cos_sim\n",
    "        \n",
    "        ## bez sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        if(max_bs < cos_sim):\n",
    "            max_bs = cos_sim\n",
    "        \n",
    "        ## bez bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        if(max_bb < cos_sim):\n",
    "            max_bb = cos_sim\n",
    "        \n",
    "    if(j == 0):\n",
    "        j = 1\n",
    "        ## sa sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        max_ss = cos_sim\n",
    "            \n",
    "        ## sa bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)    \n",
    "        max_sb = cos_sim\n",
    "        \n",
    "        ## bez sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        max_bs = cos_sim\n",
    "        \n",
    "        ## bez bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        max_bb = cos_sim\n",
    "    \n",
    "    bert_napredak_results_max.loc[i] = [max_ss, max_sb, max_bs, max_bb, row['Stance']]\n",
    "    i += 1\n",
    "    if(i%5 == 0):\n",
    "        print(i)\n",
    "    if(i>=300):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sa-sa    sa-bez    bez-sa   bez-bez     Stance\n",
      "0    0.627635  0.620608  0.578464  0.578464  unrelated\n",
      "1    0.612943  0.626350  0.612943  0.626350  unrelated\n",
      "2    0.689166  0.699177  0.659427  0.692413  unrelated\n",
      "3    0.597808  0.634351  0.597808  0.634351  unrelated\n",
      "4    0.660143  0.681400  0.670652  0.716698  unrelated\n",
      "5    0.671554  0.671554  0.671554  0.671554  unrelated\n",
      "6    0.684476  0.685156  0.687796  0.687796  unrelated\n",
      "7    0.656540  0.656540  0.656540  0.656540  unrelated\n",
      "8    0.650038  0.677106  0.659520  0.683190  unrelated\n",
      "9    0.578987  0.616212  0.578987  0.616212  unrelated\n",
      "10   0.721992  0.721992  0.721992  0.721992  unrelated\n",
      "11   0.647815  0.667102  0.647815  0.667102  unrelated\n",
      "12   0.562497  0.542274  0.558219  0.580671  unrelated\n",
      "13   0.681521  0.684489  0.681521  0.684489  unrelated\n",
      "14   0.638384  0.657983  0.638384  0.657983  unrelated\n",
      "15   0.696646  0.696646  0.691727  0.691727  unrelated\n",
      "16   0.601039  0.657830  0.601039  0.657830  unrelated\n",
      "17   0.645673  0.668799  0.645673  0.668799  unrelated\n",
      "18   0.710755  0.723399  0.700337  0.720303  unrelated\n",
      "19   0.665383  0.665383  0.665383  0.665383  unrelated\n",
      "20   0.635036  0.639836  0.635036  0.639836  unrelated\n",
      "21   0.536775  0.583193  0.536775  0.583193  unrelated\n",
      "22   0.580292  0.580292  0.580292  0.580292  unrelated\n",
      "23   0.684295  0.684295  0.672309  0.672309  unrelated\n",
      "24   0.855309  0.751238  0.763842  0.749235    related\n",
      "25   0.600654  0.626703  0.600654  0.626703  unrelated\n",
      "26   0.665299  0.665299  0.665299  0.665299  unrelated\n",
      "27   0.593561  0.613033  0.593561  0.613033  unrelated\n",
      "28   0.645278  0.680367  0.645278  0.680367  unrelated\n",
      "29   0.763693  0.763693  0.740241  0.740241  unrelated\n",
      "30   0.600717  0.604835  0.577521  0.579857  unrelated\n",
      "31   0.612137  0.616911  0.612137  0.616911  unrelated\n",
      "32   0.655110  0.650677  0.627791  0.629482  unrelated\n",
      "33   0.623014  0.632525  0.572928  0.594028  unrelated\n",
      "34   0.589642  0.589642  0.584066  0.593917  unrelated\n",
      "35   0.606199  0.606199  0.606199  0.606199  unrelated\n",
      "36   0.765006  0.749426  0.684933  0.700125    related\n",
      "37   0.726615  0.722316  0.675680  0.675509  unrelated\n",
      "38   0.632236  0.634946  0.606233  0.610797  unrelated\n",
      "39   0.664874  0.664977  0.681380  0.677214  unrelated\n",
      "40   0.662986  0.662986  0.660498  0.672157  unrelated\n",
      "41   0.684993  0.686981  0.689125  0.688421  unrelated\n",
      "42   0.630803  0.630803  0.626673  0.635285  unrelated\n",
      "43   0.683951  0.691395  0.641321  0.642576  unrelated\n",
      "44   0.623564  0.623564  0.624378  0.624378  unrelated\n",
      "45   0.625736  0.626301  0.562690  0.598746  unrelated\n",
      "46   0.572349  0.572349  0.484388  0.524447  unrelated\n",
      "47   0.642375  0.642375  0.624248  0.636340  unrelated\n",
      "48   0.748611  0.748611  0.748611  0.748611  unrelated\n",
      "49   0.643123  0.696748  0.643123  0.696748  unrelated\n",
      "50   0.627069  0.627069  0.605190  0.622886  unrelated\n",
      "51   0.615459  0.615459  0.580888  0.606098  unrelated\n",
      "52   0.640281  0.641930  0.657580  0.656231  unrelated\n",
      "53   0.693921  0.704071  0.637089  0.657565  unrelated\n",
      "54   0.642375  0.642375  0.624248  0.636340  unrelated\n",
      "55   0.677692  0.677692  0.673714  0.673714  unrelated\n",
      "56   0.658049  0.664131  0.658049  0.664131  unrelated\n",
      "57   0.671153  0.678152  0.688950  0.673355  unrelated\n",
      "58   0.722377  0.722377  0.706512  0.706512  unrelated\n",
      "59   0.589418  0.592191  0.568337  0.565291  unrelated\n",
      "60   0.672533  0.672533  0.667628  0.667628  unrelated\n",
      "61   0.564240  0.564240  0.570695  0.585740  unrelated\n",
      "62   0.653863  0.646655  0.653863  0.646655  unrelated\n",
      "63   0.583905  0.583905  0.519965  0.506514  unrelated\n",
      "64   0.677338  0.677338  0.669592  0.679043  unrelated\n",
      "65   0.679861  0.679861  0.662073  0.662073  unrelated\n",
      "66   0.608919  0.609860  0.596894  0.596041  unrelated\n",
      "67   0.638026  0.639303  0.624934  0.642430  unrelated\n",
      "68   0.668815  0.668815  0.668815  0.668815  unrelated\n",
      "69   0.715473  0.689930  0.715473  0.689930    related\n",
      "70   0.559601  0.574455  0.559601  0.574455  unrelated\n",
      "71   0.593263  0.598314  0.592926  0.605774  unrelated\n",
      "72   0.697325  0.691110  0.697325  0.691110  unrelated\n",
      "73   0.703358  0.703358  0.673026  0.673026  unrelated\n",
      "74   0.742252  0.742252  0.741509  0.741509  unrelated\n",
      "75   0.582257  0.582257  0.458734  0.495011  unrelated\n",
      "76   0.645405  0.654222  0.561128  0.588138  unrelated\n",
      "77   0.671452  0.671366  0.671452  0.671366  unrelated\n",
      "78   0.581305  0.583796  0.564468  0.607154  unrelated\n",
      "79   0.622584  0.621944  0.610847  0.605523  unrelated\n",
      "80   0.697785  0.697785  0.707608  0.707608  unrelated\n",
      "81   0.713134  0.707394  0.712381  0.708035  unrelated\n",
      "82   0.643327  0.641898  0.565492  0.589989  unrelated\n",
      "83   0.722954  0.722954  0.722954  0.722954  unrelated\n",
      "84   0.561182  0.559665  0.561182  0.559665  unrelated\n",
      "85   0.653192  0.649301  0.653192  0.649301  unrelated\n",
      "86   0.716512  0.716512  0.716512  0.716512  unrelated\n",
      "87   0.608866  0.613694  0.579219  0.591799  unrelated\n",
      "88   0.782727  0.781128  0.749278  0.749278    related\n",
      "89   0.639910  0.629883  0.613992  0.625301  unrelated\n",
      "90   0.627468  0.627468  0.590124  0.621438  unrelated\n",
      "91   0.643682  0.643682  0.630032  0.630032  unrelated\n",
      "92   0.626658  0.626658  0.597667  0.582147  unrelated\n",
      "93   0.805850  0.775058  0.723792  0.725149    related\n",
      "94   0.655953  0.645175  0.571008  0.571008  unrelated\n",
      "95   0.675071  0.686621  0.675071  0.686621  unrelated\n",
      "96   0.637326  0.637326  0.619302  0.619302  unrelated\n",
      "97   0.740977  0.756137  0.740977  0.756137  unrelated\n",
      "98   0.717242  0.717242  0.640419  0.640419  unrelated\n",
      "99   0.701463  0.700221  0.701463  0.700221  unrelated\n",
      "100  0.736869  0.724544  0.660224  0.655476  unrelated\n",
      "101  0.663014  0.675883  0.663014  0.675883  unrelated\n",
      "102  0.594601  0.610181  0.595115  0.605834  unrelated\n",
      "103  0.745804  0.752312  0.749789  0.744207  unrelated\n",
      "104  0.746786  0.746786  0.721957  0.721957  unrelated\n",
      "105  0.753466  0.743482  0.761617  0.763103  unrelated\n",
      "106  0.592566  0.573898  0.558355  0.535731  unrelated\n",
      "107  0.818662  0.799181  0.773036  0.787995    related\n",
      "108  0.730530  0.730530  0.730530  0.730530  unrelated\n",
      "109  0.625268  0.642781  0.596053  0.634234  unrelated\n",
      "110  0.672278  0.660519  0.663269  0.652998  unrelated\n",
      "111  0.664960  0.686107  0.664960  0.686107  unrelated\n",
      "112  0.569807  0.558259  0.587971  0.578023  unrelated\n",
      "113  0.606132  0.613766  0.569587  0.569587  unrelated\n",
      "114  0.689354  0.687092  0.689354  0.687092  unrelated\n",
      "115  0.795346  0.797221  0.754971  0.782410  unrelated\n",
      "116  0.602190  0.589741  0.594034  0.594034  unrelated\n",
      "117  0.796831  0.748722  0.643255  0.709587    related\n",
      "118  0.726954  0.718537  0.667354  0.691529  unrelated\n",
      "119  0.636098  0.648940  0.634973  0.637245  unrelated\n",
      "120  0.772146  0.762656  0.754417  0.761285  unrelated\n",
      "121  0.645943  0.649832  0.588726  0.588726  unrelated\n",
      "122  0.780072  0.780072  0.719540  0.742982  unrelated\n",
      "123  0.709051  0.702575  0.709051  0.702575  unrelated\n",
      "124  0.718984  0.718984  0.718984  0.718984  unrelated\n",
      "125  0.777903  0.779550  0.767798  0.771226  unrelated\n",
      "126  0.646637  0.648711  0.616364  0.665732  unrelated\n",
      "127  0.738965  0.725283  0.674277  0.670865  unrelated\n",
      "128  0.730630  0.729391  0.685717  0.694920  unrelated\n",
      "129  0.621115  0.621115  0.621115  0.621115  unrelated\n",
      "130  0.706944  0.704412  0.697248  0.716360  unrelated\n",
      "131  0.743400  0.743400  0.743400  0.743400  unrelated\n",
      "132  0.695459  0.691970  0.695459  0.691970  unrelated\n",
      "133  0.719595  0.709450  0.612699  0.620798  unrelated\n",
      "134  0.667049  0.661965  0.667240  0.659997  unrelated\n",
      "135  0.667544  0.667544  0.667544  0.667544  unrelated\n",
      "136  0.631737  0.647394  0.631737  0.647394  unrelated\n",
      "137  0.749276  0.758892  0.749276  0.758892  unrelated\n",
      "138  0.606712  0.645703  0.606712  0.645703  unrelated\n",
      "139  0.775284  0.776427  0.775284  0.776427  unrelated\n",
      "140  0.776043  0.785746  0.748035  0.753043  unrelated\n",
      "141  0.634797  0.644578  0.627512  0.627512  unrelated\n",
      "142  0.717038  0.699837  0.674458  0.665619  unrelated\n",
      "143  0.622291  0.687783  0.622291  0.687783  unrelated\n",
      "144  0.791525  0.782495  0.736540  0.756642  unrelated\n",
      "145  0.722378  0.707838  0.630863  0.654856  unrelated\n",
      "146  0.680985  0.682005  0.675452  0.696623  unrelated\n",
      "147  0.630627  0.630627  0.621449  0.621449  unrelated\n",
      "148  0.554505  0.554505  0.554505  0.554505  unrelated\n",
      "149  0.755312  0.751267  0.736849  0.743656  unrelated\n",
      "150  0.711950  0.709222  0.690736  0.698113  unrelated\n",
      "151  0.706980  0.706980  0.688798  0.688798  unrelated\n",
      "152  0.542386  0.554275  0.529717  0.548149  unrelated\n",
      "153  0.765605  0.767127  0.720217  0.736662  unrelated\n",
      "154  0.629980  0.629980  0.629980  0.629980  unrelated\n",
      "155  0.741361  0.741361  0.677043  0.677043  unrelated\n",
      "156  0.736466  0.737106  0.736466  0.737106  unrelated\n",
      "157  0.685734  0.675640  0.672355  0.677706  unrelated\n",
      "158  0.634912  0.634912  0.634912  0.634912  unrelated\n",
      "159  0.621115  0.621115  0.621115  0.621115  unrelated\n",
      "160  0.646130  0.649807  0.646130  0.649807  unrelated\n",
      "161  0.702016  0.720170  0.702016  0.720170  unrelated\n",
      "162  0.691151  0.682568  0.632478  0.647637  unrelated\n",
      "163  0.736362  0.746880  0.736362  0.746880  unrelated\n",
      "164  0.707160  0.721075  0.646229  0.688551  unrelated\n",
      "165  0.705302  0.691645  0.715406  0.709269  unrelated\n",
      "166  0.726336  0.708492  0.723069  0.714644  unrelated\n",
      "167  0.718135  0.738625  0.718135  0.738625  unrelated\n",
      "168  0.761936  0.758138  0.744011  0.744011  unrelated\n",
      "169  0.637136  0.647419  0.637136  0.647419  unrelated\n",
      "170  0.699410  0.684604  0.699410  0.684604  unrelated\n",
      "171  0.716185  0.708079  0.716185  0.708079  unrelated\n",
      "172  0.587848  0.607242  0.565717  0.594015  unrelated\n",
      "173  0.719176  0.710298  0.698461  0.704104  unrelated\n",
      "174  0.529667  0.539227  0.599997  0.652256  unrelated\n",
      "175  0.677082  0.677082  0.655158  0.655158  unrelated\n",
      "176  0.734993  0.733591  0.734993  0.733591  unrelated\n",
      "177  0.619475  0.619475  0.619475  0.619475  unrelated\n",
      "178  0.714266  0.706205  0.698997  0.709223  unrelated\n",
      "179  0.735305  0.728322  0.735305  0.728322  unrelated\n",
      "180  0.663127  0.663127  0.663127  0.663127  unrelated\n",
      "181  0.663567  0.669597  0.643546  0.651134  unrelated\n",
      "182  0.811968  0.817353  0.811968  0.817353  unrelated\n",
      "183  0.667236  0.667236  0.667236  0.667236  unrelated\n",
      "184  0.704580  0.746857  0.673219  0.720730  unrelated\n",
      "185  0.621246  0.625359  0.621246  0.625359  unrelated\n",
      "186  0.737242  0.737242  0.720658  0.720658  unrelated\n",
      "187  0.802886  0.804385  0.802886  0.804385  unrelated\n",
      "188  0.712008  0.733647  0.639677  0.650142  unrelated\n",
      "189  0.712681  0.712681  0.712681  0.712681  unrelated\n",
      "190  0.661279  0.645003  0.572525  0.587561  unrelated\n",
      "191  0.645529  0.643962  0.576985  0.597175  unrelated\n",
      "192  0.709253  0.709253  0.709253  0.709253  unrelated\n",
      "193  0.715119  0.701475  0.642421  0.642421  unrelated\n",
      "194  0.749482  0.744161  0.746671  0.743461  unrelated\n",
      "195  0.735518  0.729806  0.714999  0.718605  unrelated\n",
      "196  0.735626  0.730118  0.687323  0.689871  unrelated\n",
      "197  0.696153  0.708420  0.698491  0.720015  unrelated\n",
      "198  0.750219  0.743261  0.732365  0.741556  unrelated\n",
      "199  0.810295  0.810295  0.810295  0.810295    related\n",
      "200  0.740454  0.730902  0.717800  0.707173  unrelated\n",
      "201  0.638090  0.650554  0.761198  0.761198  unrelated\n",
      "202  0.748357  0.748357  0.686846  0.673271  unrelated\n",
      "203  0.715704  0.715704  0.732869  0.732869  unrelated\n",
      "204  0.718331  0.713182  0.689426  0.689426  unrelated\n",
      "205  0.667448  0.667448  0.696625  0.696625  unrelated\n",
      "206  0.696853  0.681679  0.702026  0.688188  unrelated\n",
      "207  0.725197  0.714594  0.717957  0.705440  unrelated\n",
      "208  0.730400  0.724474  0.732504  0.728357  unrelated\n",
      "209  0.744617  0.727654  0.735143  0.724620  unrelated\n",
      "210  0.726600  0.731074  0.726600  0.731074  unrelated\n",
      "211  0.686816  0.692720  0.686816  0.692720  unrelated\n",
      "212  0.747795  0.731205  0.688489  0.681118  unrelated\n",
      "213  0.743745  0.743745  0.708157  0.697137  unrelated\n",
      "214  0.687632  0.687632  0.687632  0.687632  unrelated\n",
      "215  0.726600  0.731074  0.726600  0.731074  unrelated\n",
      "216  0.740547  0.709872  0.719097  0.698380  unrelated\n",
      "217  0.654358  0.648069  0.653923  0.658760  unrelated\n",
      "218  0.718331  0.713182  0.689426  0.689426  unrelated\n",
      "219  0.696769  0.677497  0.682873  0.682873  unrelated\n",
      "220  0.712744  0.712744  0.712744  0.712744  unrelated\n",
      "221  0.788011  0.774394  0.788011  0.774394  unrelated\n",
      "222  0.771874  0.771874  0.771874  0.771874  unrelated\n",
      "223  0.815321  0.804177  0.811648  0.804050  unrelated\n",
      "224  0.755909  0.763118  0.754341  0.766516  unrelated\n",
      "225  0.716537  0.707894  0.658854  0.648287  unrelated\n",
      "226  0.613669  0.613669  0.617241  0.617241  unrelated\n",
      "227  0.714297  0.714297  0.706629  0.683968  unrelated\n",
      "228  0.781705  0.781705  0.781705  0.781705  unrelated\n",
      "229  0.710261  0.682631  0.662185  0.683784  unrelated\n",
      "230  0.728207  0.711747  0.742886  0.736629  unrelated\n",
      "231  0.780032  0.769839  0.714385  0.733227  unrelated\n",
      "232  0.718516  0.709316  0.673812  0.668600  unrelated\n",
      "233  0.753157  0.742096  0.740103  0.724498  unrelated\n",
      "234  0.752095  0.752095  0.749957  0.749957  unrelated\n",
      "235  0.648850  0.653736  0.665188  0.668634  unrelated\n",
      "236  0.724198  0.706422  0.688486  0.689711  unrelated\n",
      "237  0.780062  0.785761  0.780062  0.785761  unrelated\n",
      "238  0.770588  0.775105  0.748465  0.752828  unrelated\n",
      "239  0.721955  0.725439  0.721955  0.725439  unrelated\n",
      "240  0.748924  0.748924  0.751743  0.751743  unrelated\n",
      "241  0.719077  0.694791  0.682593  0.683731    related\n",
      "242  0.793313  0.736828  0.720594  0.694733    related\n",
      "243  0.719125  0.719125  0.719125  0.719125  unrelated\n",
      "244  0.720375  0.723680  0.712010  0.712010  unrelated\n",
      "245  0.578201  0.575236  0.578201  0.575236  unrelated\n",
      "246  0.700113  0.704043  0.700113  0.704043  unrelated\n",
      "247  0.640977  0.639978  0.634032  0.633393  unrelated\n",
      "248  0.645431  0.633966  0.686663  0.686663  unrelated\n",
      "249  0.664082  0.664082  0.629552  0.629552  unrelated\n",
      "250  0.599572  0.599572  0.592011  0.592011  unrelated\n",
      "251  0.648638  0.648638  0.648638  0.648638  unrelated\n",
      "252  0.547424  0.550036  0.547424  0.550036  unrelated\n",
      "253  0.626229  0.635526  0.610148  0.617415  unrelated\n",
      "254  0.632571  0.632571  0.623970  0.626756  unrelated\n",
      "255  0.738537  0.745804  0.732328  0.751831  unrelated\n",
      "256  0.724164  0.724164  0.724164  0.724164  unrelated\n",
      "257  0.639381  0.639381  0.643289  0.643289  unrelated\n",
      "258  0.594152  0.598257  0.578204  0.594234  unrelated\n",
      "259  0.692948  0.692948  0.690172  0.690172  unrelated\n",
      "260  0.642924  0.643583  0.579242  0.580334  unrelated\n",
      "261  0.547594  0.547594  0.577306  0.577306  unrelated\n",
      "262  0.744918  0.744918  0.744918  0.744918    related\n",
      "263  0.767419  0.770516  0.767419  0.770516  unrelated\n",
      "264  0.543429  0.541059  0.542147  0.542147  unrelated\n",
      "265  0.566963  0.566963  0.587174  0.587174  unrelated\n",
      "266  0.693431  0.669155  0.642658  0.644558  unrelated\n",
      "267  0.636996  0.636996  0.659461  0.659461  unrelated\n",
      "268  0.777380  0.777380  0.777380  0.777380  unrelated\n",
      "269  0.599572  0.599572  0.592011  0.592011  unrelated\n",
      "270  0.608514  0.608514  0.645139  0.626649  unrelated\n",
      "271  0.629453  0.629453  0.629453  0.629453  unrelated\n",
      "272  0.806868  0.806868  0.806868  0.806868    related\n",
      "273  0.620853  0.620853  0.601620  0.601620  unrelated\n",
      "274  0.645056  0.672158  0.645056  0.672158  unrelated\n",
      "275  0.547594  0.547594  0.577306  0.577306  unrelated\n",
      "276  0.565512  0.565512  0.597621  0.597621  unrelated\n",
      "277  0.583839  0.590720  0.610493  0.612101  unrelated\n",
      "278  0.642044  0.642615  0.628246  0.640695  unrelated\n",
      "279  0.602582  0.606173  0.595055  0.595055  unrelated\n",
      "280  0.697682  0.697682  0.647768  0.647768  unrelated\n",
      "281  0.594959  0.573259  0.568625  0.568625  unrelated\n",
      "282  0.730761  0.730761  0.730761  0.730761  unrelated\n",
      "283  0.626108  0.626108  0.619000  0.637125  unrelated\n",
      "284  0.604859  0.604859  0.610828  0.618062  unrelated\n",
      "285  0.573338  0.573338  0.543062  0.541371  unrelated\n",
      "286  0.601569  0.612697  0.620061  0.622237  unrelated\n",
      "287  0.697150  0.697150  0.698809  0.698809  unrelated\n",
      "288  0.685452  0.680450  0.659664  0.663675  unrelated\n",
      "289  0.571219  0.545104  0.539184  0.524131  unrelated\n",
      "290  0.677873  0.685721  0.677873  0.685721  unrelated\n",
      "291  0.656736  0.654090  0.656736  0.654090  unrelated\n",
      "292  0.654168  0.638662  0.632173  0.611846  unrelated\n",
      "293  0.672727  0.672727  0.667751  0.667751  unrelated\n",
      "294  0.571288  0.571288  0.539690  0.539690  unrelated\n",
      "295  0.702586  0.691289  0.682956  0.679599  unrelated\n",
      "296  0.779514  0.763882  0.779514  0.763882    related\n",
      "297  0.618810  0.620429  0.618810  0.620429  unrelated\n",
      "298  0.675738  0.690600  0.621943  0.608267  unrelated\n",
      "299  0.699629  0.699629  0.681803  0.681803  unrelated\n",
      "300  0.682036  0.682036  0.682036  0.682036  unrelated\n",
      "301  0.667219  0.666335  0.633684  0.636078  unrelated\n",
      "302  0.773869  0.754126  0.754763  0.729186  unrelated\n",
      "303  0.701865  0.701865  0.701865  0.701865  unrelated\n",
      "304  0.682769  0.681718  0.683158  0.676261  unrelated\n",
      "305  0.688758  0.680736  0.688758  0.680736  unrelated\n",
      "306  0.586317  0.586317  0.524755  0.524755  unrelated\n",
      "307  0.633709  0.633709  0.498330  0.498330  unrelated\n",
      "308  0.690360  0.690360  0.688384  0.692024  unrelated\n",
      "309  0.681424  0.678817  0.681424  0.678817  unrelated\n",
      "310  0.651700  0.631379  0.655660  0.639801  unrelated\n",
      "311  0.701335  0.701335  0.693895  0.693895  unrelated\n",
      "312  0.664805  0.665726  0.616965  0.616965  unrelated\n",
      "313  0.690128  0.690128  0.688708  0.688708  unrelated\n",
      "314  0.661204  0.661204  0.628656  0.628656  unrelated\n",
      "315  0.717653  0.717653  0.717653  0.717653  unrelated\n",
      "316  0.632142  0.627648  0.613882  0.590994  unrelated\n",
      "317  0.647909  0.647909  0.591565  0.591565  unrelated\n",
      "318  0.657292  0.657292  0.657292  0.657292  unrelated\n",
      "319  0.715606  0.700627  0.707898  0.686035  unrelated\n",
      "320  0.697198  0.697985  0.670210  0.670210  unrelated\n",
      "321  0.720531  0.687558  0.720531  0.687558  unrelated\n",
      "322  0.633953  0.627592  0.633953  0.627592  unrelated\n",
      "323  0.665972  0.664215  0.618617  0.617694  unrelated\n",
      "324  0.692376  0.693896  0.675303  0.674290  unrelated\n",
      "325  0.619591  0.619591  0.627138  0.628019  unrelated\n",
      "326  0.653813  0.653813  0.626632  0.631051  unrelated\n",
      "327  0.715118  0.714006  0.715118  0.714006  unrelated\n",
      "328  0.628416  0.628416  0.590035  0.588176  unrelated\n",
      "329  0.636901  0.635823  0.630885  0.628123  unrelated\n",
      "330  0.651159  0.652832  0.651159  0.652832  unrelated\n",
      "331  0.740801  0.699731  0.735883  0.701860  unrelated\n",
      "332  0.662743  0.662743  0.618519  0.618519  unrelated\n",
      "333  0.659364  0.659364  0.659364  0.659364  unrelated\n",
      "334  0.622499  0.622499  0.622499  0.622499  unrelated\n",
      "335  0.580242  0.580242  0.617629  0.602386  unrelated\n",
      "336  0.653643  0.661799  0.653643  0.661799  unrelated\n",
      "337  0.676960  0.676960  0.691148  0.691148  unrelated\n",
      "338  0.724252  0.726456  0.724252  0.726456  unrelated\n",
      "339  0.696397  0.671900  0.705235  0.685699  unrelated\n",
      "340  0.694246  0.694460  0.667995  0.662867  unrelated\n",
      "341  0.679705  0.640352  0.679705  0.640352  unrelated\n",
      "342  0.664583  0.664287  0.647603  0.650012  unrelated\n",
      "343  0.608451  0.608451  0.586975  0.586975  unrelated\n",
      "344  0.676436  0.676436  0.652243  0.651231  unrelated\n",
      "345  0.634157  0.634157  0.634157  0.634157  unrelated\n",
      "346  0.635801  0.638559  0.647152  0.648573  unrelated\n",
      "347  0.714575  0.714575  0.712173  0.712386  unrelated\n",
      "348  0.699241  0.688761  0.670419  0.660785  unrelated\n",
      "349  0.688811  0.682738  0.688811  0.682738  unrelated\n",
      "350  0.702538  0.702538  0.660768  0.660768  unrelated\n",
      "351  0.641232  0.649477  0.641232  0.649477  unrelated\n",
      "352  0.627174  0.627174  0.632568  0.629320  unrelated\n",
      "353  0.594735  0.594735  0.594735  0.594735  unrelated\n",
      "354  0.713641  0.711885  0.703106  0.699337  unrelated\n",
      "355  0.592246  0.592246  0.522814  0.522814  unrelated\n",
      "356  0.622255  0.622255  0.593769  0.596032  unrelated\n",
      "357  0.616653  0.616653  0.515427  0.515427  unrelated\n",
      "358  0.636645  0.634054  0.636645  0.634054  unrelated\n",
      "359  0.595461  0.595461  0.595461  0.595461  unrelated\n",
      "360  0.699851  0.689962  0.606994  0.613625  unrelated\n",
      "361  0.658081  0.659015  0.653111  0.657560  unrelated\n",
      "362  0.641277  0.639502  0.568629  0.580012  unrelated\n",
      "363  0.915655  0.920901  0.915655  0.920901    related\n",
      "364  0.746607  0.739940  0.728604  0.725624  unrelated\n",
      "365  0.663430  0.685208  0.654082  0.669905  unrelated\n",
      "366  0.754779  0.738035  0.755985  0.741640  unrelated\n",
      "367  0.622678  0.626276  0.607835  0.616681  unrelated\n",
      "368  0.723877  0.712493  0.650499  0.663135  unrelated\n",
      "369  0.744090  0.732952  0.734417  0.730627  unrelated\n",
      "370  0.692101  0.688943  0.704263  0.705298  unrelated\n",
      "371  0.717587  0.717587  0.717587  0.717587  unrelated\n",
      "372  0.643245  0.636261  0.621112  0.623536  unrelated\n",
      "373  0.627175  0.651370  0.627030  0.654189  unrelated\n",
      "374  0.716966  0.721094  0.702707  0.714220  unrelated\n",
      "375  0.851319  0.875597  0.845114  0.863322    related\n",
      "376  0.712645  0.698057  0.592461  0.592111  unrelated\n",
      "377  0.686003  0.694722  0.656830  0.706122  unrelated\n",
      "378  0.687599  0.694610  0.684962  0.703049  unrelated\n",
      "379  0.632910  0.624022  0.607446  0.614212  unrelated\n",
      "380  0.650040  0.622362  0.637954  0.621159  unrelated\n",
      "381  0.653701  0.662269  0.627566  0.651060  unrelated\n",
      "382  0.712616  0.704803  0.647377  0.665458  unrelated\n",
      "383  0.652756  0.647670  0.582966  0.607681  unrelated\n",
      "384  0.725635  0.699602  0.702162  0.695533  unrelated\n",
      "385  0.757565  0.763408  0.748498  0.759622  unrelated\n",
      "386  0.775219  0.774787  0.717746  0.737626    related\n",
      "387  0.649698  0.643267  0.599662  0.611025  unrelated\n",
      "388  0.691859  0.688202  0.642950  0.670600  unrelated\n",
      "389  0.731887  0.716951  0.742220  0.755419  unrelated\n",
      "390  0.649630  0.674645  0.649630  0.674645  unrelated\n",
      "391  0.653219  0.647442  0.609450  0.635725  unrelated\n",
      "392  0.679624  0.691381  0.600157  0.642082  unrelated\n",
      "393  0.730868  0.714753  0.711551  0.707896  unrelated\n",
      "394  0.674257  0.677159  0.640340  0.662028  unrelated\n",
      "395  0.690377  0.666817  0.643336  0.658290  unrelated\n",
      "396  0.718636  0.720599  0.696041  0.712405  unrelated\n",
      "397  0.777486  0.754847  0.777486  0.754847  unrelated\n",
      "398  0.668047  0.674656  0.668047  0.674656  unrelated\n",
      "399  0.632570  0.672570  0.632570  0.672570  unrelated\n",
      "400  0.660775  0.673770  0.589813  0.628669  unrelated\n",
      "401  0.723718  0.708431  0.668206  0.681454  unrelated\n",
      "402  0.749054  0.765888  0.761890  0.787457    related\n",
      "403  0.630442  0.630442  0.634686  0.627826  unrelated\n",
      "404  0.680629  0.655354  0.692699  0.693222  unrelated\n",
      "405  0.620613  0.615744  0.620613  0.615744  unrelated\n",
      "406  0.673406  0.666376  0.671809  0.684488  unrelated\n",
      "407  0.670196  0.660998  0.670196  0.660998  unrelated\n",
      "408  0.618922  0.618922  0.612187  0.612187  unrelated\n",
      "409  0.614894  0.603808  0.570429  0.570429  unrelated\n",
      "410  0.836785  0.836785  0.836785  0.836785    related\n",
      "411  0.599124  0.599124  0.643348  0.643348  unrelated\n",
      "412  0.638904  0.638904  0.638904  0.638904  unrelated\n",
      "413  0.647705  0.629921  0.647705  0.629921  unrelated\n",
      "414  0.630275  0.627065  0.633974  0.633974  unrelated\n",
      "415  0.685537  0.687484  0.679653  0.679653  unrelated\n",
      "416  0.709419  0.709857  0.709419  0.709857  unrelated\n",
      "417  0.617591  0.604374  0.619085  0.610759  unrelated\n",
      "418  0.712965  0.712965  0.711793  0.711793  unrelated\n",
      "419  0.686877  0.671192  0.631943  0.631943  unrelated\n",
      "420  0.683698  0.672810  0.683698  0.672810  unrelated\n",
      "421  0.637224  0.632061  0.657594  0.657594  unrelated\n",
      "422  0.629817  0.629817  0.629817  0.629817  unrelated\n",
      "423  0.600930  0.600930  0.581234  0.581234  unrelated\n",
      "424  0.673503  0.673471  0.663131  0.663936  unrelated\n",
      "425  0.626283  0.632108  0.620565  0.620565  unrelated\n",
      "426  0.680527  0.680527  0.658152  0.658152  unrelated\n",
      "427  0.684925  0.672470  0.684925  0.672470  unrelated\n",
      "428  0.639013  0.639013  0.639013  0.639013  unrelated\n",
      "429  0.661203  0.654487  0.661203  0.654487  unrelated\n",
      "430  0.667100  0.665995  0.667100  0.665995  unrelated\n",
      "431  0.710567  0.704234  0.718614  0.689841  unrelated\n",
      "432  0.604313  0.604313  0.623030  0.623030  unrelated\n",
      "433  0.636561  0.636561  0.651326  0.647054  unrelated\n",
      "434  0.606938  0.611126  0.606938  0.611126  unrelated\n",
      "435  0.649222  0.657874  0.649222  0.657874  unrelated\n",
      "436  0.595194  0.585592  0.595194  0.585592  unrelated\n",
      "437  0.605495  0.605495  0.616897  0.616897  unrelated\n",
      "438  0.572706  0.572519  0.623533  0.623533  unrelated\n",
      "439  0.643257  0.643257  0.643205  0.643205  unrelated\n",
      "440  0.699657  0.699555  0.699657  0.699555  unrelated\n",
      "441  0.628680  0.617169  0.590578  0.587404  unrelated\n",
      "442  0.719469  0.719469  0.722745  0.722745  unrelated\n",
      "443  0.687626  0.687626  0.687626  0.687626  unrelated\n",
      "444  0.609030  0.601906  0.609030  0.601906  unrelated\n",
      "445  0.639536  0.652618  0.621188  0.636944  unrelated\n",
      "446  0.596255  0.569108  0.600046  0.572675  unrelated\n",
      "447  0.515366  0.511374  0.515366  0.511374  unrelated\n",
      "448  0.674788  0.660520  0.661946  0.660188  unrelated\n",
      "449  0.611395  0.611395  0.611395  0.611395  unrelated\n",
      "450  0.599357  0.593474  0.599357  0.593474  unrelated\n",
      "451  0.662062  0.662062  0.641564  0.641564  unrelated\n",
      "452  0.644354  0.639277  0.644354  0.639277  unrelated\n",
      "453  0.615598  0.608451  0.615598  0.608451  unrelated\n",
      "454  0.678671  0.669412  0.678671  0.669412  unrelated\n",
      "455  0.795162  0.784621  0.795162  0.784621    related\n",
      "456  0.687483  0.687483  0.687483  0.687483  unrelated\n",
      "457  0.683110  0.679227  0.677264  0.674216  unrelated\n",
      "458  0.697182  0.697182  0.716568  0.716568  unrelated\n",
      "459  0.696065  0.696065  0.661356  0.661356  unrelated\n",
      "460  0.676105  0.676105  0.649129  0.654202  unrelated\n",
      "461  0.700247  0.677998  0.703493  0.697444  unrelated\n",
      "462  0.828489  0.828489  0.828489  0.828489    related\n",
      "463  0.691809  0.692384  0.692832  0.693630  unrelated\n",
      "464  0.658486  0.665371  0.636760  0.651197  unrelated\n",
      "465  0.676807  0.677998  0.676807  0.677998  unrelated\n",
      "466  0.640405  0.640405  0.624233  0.629074  unrelated\n",
      "467  0.690547  0.690547  0.720866  0.720866  unrelated\n",
      "468  0.690275  0.694143  0.696959  0.707722  unrelated\n",
      "469  0.672789  0.672789  0.649264  0.654658  unrelated\n",
      "470  0.690547  0.690547  0.720866  0.720866  unrelated\n",
      "471  0.695262  0.695262  0.695262  0.695262  unrelated\n",
      "472  0.656389  0.655327  0.610003  0.610003  unrelated\n",
      "473  0.676325  0.676325  0.676325  0.676325  unrelated\n",
      "474  0.662118  0.662118  0.662908  0.662908  unrelated\n",
      "475  0.673649  0.678375  0.673649  0.678375  unrelated\n",
      "476  0.659919  0.659919  0.634042  0.634042  unrelated\n",
      "477  0.665290  0.665290  0.665290  0.665290  unrelated\n",
      "478  0.569082  0.569956  0.546691  0.552899  unrelated\n",
      "479  0.642731  0.648748  0.642731  0.648748  unrelated\n",
      "480  0.783057  0.783057  0.783057  0.783057  unrelated\n",
      "481  0.673175  0.673175  0.673175  0.673175  unrelated\n",
      "482  0.621598  0.574581  0.633903  0.604603  unrelated\n",
      "483  0.637556  0.638667  0.618861  0.623287  unrelated\n",
      "484  0.662536  0.667267  0.643705  0.656455  unrelated\n",
      "485  0.657705  0.665863  0.657705  0.665863  unrelated\n",
      "486  0.736966  0.734623  0.726616  0.724951  unrelated\n",
      "487  0.744171  0.743545  0.692801  0.701516  unrelated\n",
      "488  0.711287  0.704484  0.711287  0.704484  unrelated\n",
      "489  0.690680  0.707038  0.690680  0.707038  unrelated\n",
      "490  0.846154  0.816261  0.788313  0.828215    related\n",
      "491  0.724499  0.705101  0.724499  0.705101  unrelated\n",
      "492  0.601469  0.586237  0.615010  0.644945  unrelated\n",
      "493  0.677662  0.691877  0.677662  0.691877  unrelated\n",
      "494  0.616222  0.614175  0.616048  0.611191  unrelated\n",
      "495  0.573239  0.573239  0.573239  0.573239  unrelated\n",
      "496  0.613089  0.614294  0.588996  0.596398  unrelated\n",
      "497  0.730885  0.735756  0.730885  0.735756  unrelated\n",
      "498  0.715178  0.717582  0.672269  0.683036  unrelated\n",
      "499  0.745290  0.745290  0.768279  0.760754    related\n",
      "500  0.862889  0.822836  0.748499  0.764214    related\n",
      "501  0.675193  0.676651  0.626539  0.635454  unrelated\n",
      "502  0.646404  0.668852  0.646404  0.668852  unrelated\n",
      "503  0.622173  0.627641  0.646696  0.652955  unrelated\n",
      "504  0.763742  0.770785  0.743170  0.775725  unrelated\n",
      "505  0.778131  0.819207  0.778131  0.819207    related\n",
      "506  0.648667  0.708530  0.648667  0.708530  unrelated\n",
      "507  0.721054  0.707537  0.693309  0.713850  unrelated\n",
      "508  0.621528  0.619759  0.631355  0.638163  unrelated\n",
      "509  0.605397  0.595872  0.605397  0.595872  unrelated\n",
      "510  0.742174  0.773663  0.742174  0.773663  unrelated\n",
      "511  0.731202  0.736842  0.695186  0.727114  unrelated\n",
      "512  0.609470  0.589782  0.616724  0.609880  unrelated\n",
      "513  0.740027  0.759244  0.695538  0.729381  unrelated\n",
      "514  0.666931  0.666931  0.666931  0.666931  unrelated\n",
      "515  0.680062  0.680767  0.656344  0.673726  unrelated\n",
      "516  0.606955  0.584894  0.591868  0.591003  unrelated\n",
      "517  0.757837  0.759871  0.714640  0.730789  unrelated\n",
      "518  0.612417  0.616540  0.612417  0.616540  unrelated\n",
      "519  0.723786  0.736667  0.723786  0.736667  unrelated\n",
      "520  0.689476  0.689476  0.685572  0.694592  unrelated\n",
      "521  0.674409  0.690263  0.674409  0.690263  unrelated\n",
      "522  0.629782  0.630872  0.611762  0.621242  unrelated\n",
      "523  0.655786  0.704997  0.655786  0.704997  unrelated\n",
      "524  0.552494  0.552494  0.596879  0.606980  unrelated\n",
      "525  0.724825  0.740073  0.724825  0.740073  unrelated\n",
      "526  0.790878  0.783715  0.790878  0.783715  unrelated\n",
      "527  0.729593  0.738878  0.657414  0.677963  unrelated\n",
      "528  0.666287  0.689534  0.668718  0.704834  unrelated\n",
      "529  0.607990  0.616267  0.607990  0.616267  unrelated\n",
      "530  0.680974  0.691280  0.680974  0.691280  unrelated\n",
      "531  0.736940  0.702726  0.736940  0.702726  unrelated\n",
      "532  0.763931  0.763931  0.774673  0.774673  unrelated\n",
      "533  0.659335  0.657433  0.616635  0.664503  unrelated\n",
      "534  0.763588  0.774371  0.723983  0.754089  unrelated\n",
      "535  0.753676  0.772428  0.753676  0.772428  unrelated\n",
      "536  0.729665  0.732185  0.675961  0.715958  unrelated\n",
      "537  0.650749  0.683527  0.650749  0.683527  unrelated\n",
      "538  0.691941  0.686457  0.620542  0.629692  unrelated\n",
      "539  0.762065  0.785518  0.747383  0.784623  unrelated\n",
      "540  0.613183  0.620272  0.613183  0.620272  unrelated\n",
      "541  0.785027  0.782650  0.769843  0.774637  unrelated\n",
      "542  0.750686  0.718484  0.684060  0.704612  unrelated\n",
      "543  0.629152  0.635488  0.629152  0.635488  unrelated\n",
      "544  0.619268  0.654340  0.610239  0.639812  unrelated\n",
      "545  0.723906  0.722934  0.723906  0.722934  unrelated\n",
      "546  0.677668  0.643663  0.677668  0.643663  unrelated\n",
      "547  0.746978  0.756402  0.695474  0.722453  unrelated\n",
      "548  0.719654  0.683596  0.654002  0.636889  unrelated\n",
      "549  0.782624  0.787121  0.781974  0.792203  unrelated\n",
      "550  0.749035  0.703771  0.736067  0.708575  unrelated\n",
      "551  0.729490  0.730503  0.729490  0.730503  unrelated\n",
      "552  0.707992  0.707992  0.696908  0.696908  unrelated\n",
      "553  0.648170  0.672248  0.648170  0.672248  unrelated\n",
      "554  0.712612  0.746516  0.659637  0.644765  unrelated\n",
      "555  0.665151  0.648083  0.659539  0.658440  unrelated\n",
      "556  0.695134  0.687077  0.666557  0.647956  unrelated\n",
      "557  0.713337  0.723110  0.633203  0.597881  unrelated\n",
      "558  0.696325  0.696325  0.671438  0.672340  unrelated\n",
      "559  0.760140  0.782526  0.747027  0.783464  unrelated\n",
      "560  0.653543  0.633108  0.653543  0.633108  unrelated\n",
      "561  0.698855  0.706082  0.698855  0.706082  unrelated\n",
      "562  0.689265  0.686647  0.663526  0.680318  unrelated\n",
      "563  0.610138  0.615550  0.594982  0.605107  unrelated\n",
      "564  0.683470  0.680533  0.646113  0.637216  unrelated\n",
      "565  0.705872  0.764510  0.668662  0.730553  unrelated\n",
      "566  0.699076  0.667885  0.691471  0.675337  unrelated\n",
      "567  0.602275  0.591877  0.572299  0.583038  unrelated\n",
      "568  0.703596  0.684333  0.630810  0.630810  unrelated\n",
      "569  0.715037  0.705033  0.749220  0.739557  unrelated\n",
      "570  0.732359  0.774085  0.732359  0.774085  unrelated\n",
      "571  0.656791  0.638100  0.640537  0.624170  unrelated\n",
      "572  0.684527  0.684527  0.696001  0.696001  unrelated\n",
      "573  0.795183  0.808245  0.744172  0.788640  unrelated\n",
      "574  0.671745  0.624255  0.652769  0.667860  unrelated\n",
      "575  0.683717  0.683717  0.691516  0.707043  unrelated\n",
      "576  0.666350  0.696568  0.666350  0.696568  unrelated\n",
      "577  0.792336  0.805177  0.763673  0.808346  unrelated\n",
      "578  0.655617  0.656461  0.655617  0.656461  unrelated\n",
      "579  0.715294  0.732057  0.715294  0.732057  unrelated\n",
      "580  0.736854  0.735648  0.690632  0.706247  unrelated\n",
      "581  0.764051  0.741209  0.714998  0.715398  unrelated\n",
      "582  0.713176  0.713176  0.656554  0.668189  unrelated\n",
      "583  0.578733  0.548137  0.578733  0.548137  unrelated\n",
      "584  0.642704  0.636187  0.631691  0.649434  unrelated\n",
      "585  0.646568  0.646568  0.646568  0.646568  unrelated\n",
      "586  0.637257  0.631568  0.666124  0.666124  unrelated\n",
      "587  0.809519  0.792444  0.766614  0.789375  unrelated\n",
      "588  0.631608  0.627863  0.621933  0.655042  unrelated\n",
      "589  0.713679  0.700409  0.658759  0.694215  unrelated\n",
      "590  0.756542  0.748810  0.714719  0.732523  unrelated\n",
      "591  0.775243  0.770432  0.757982  0.755409  unrelated\n",
      "592  0.756304  0.760768  0.721775  0.733410  unrelated\n",
      "593  0.800198  0.794425  0.800198  0.794425  unrelated\n",
      "594  0.791713  0.709971  0.770021  0.706419  unrelated\n",
      "595  0.692655  0.675959  0.670168  0.654947  unrelated\n",
      "596  0.740121  0.746345  0.740121  0.746345  unrelated\n",
      "597  0.682198  0.648305  0.677893  0.652967  unrelated\n",
      "598  0.744383  0.725048  0.750710  0.756573  unrelated\n",
      "599  0.732351  0.750338  0.681620  0.697170    related\n",
      "600  0.719315  0.755627  0.719315  0.755627  unrelated\n",
      "601  0.665081  0.672953  0.649413  0.666780  unrelated\n",
      "602  0.667490  0.667170  0.628007  0.655720  unrelated\n",
      "603  0.683532  0.656921  0.666376  0.656765  unrelated\n",
      "604  0.648245  0.651345  0.635514  0.643782  unrelated\n",
      "605  0.731503  0.718288  0.765050  0.763945  unrelated\n",
      "606  0.696839  0.727915  0.696839  0.727915  unrelated\n",
      "607  0.640229  0.640962  0.640229  0.640962  unrelated\n",
      "608  0.765583  0.765583  0.758013  0.769233  unrelated\n",
      "609  0.682626  0.696216  0.682626  0.696216  unrelated\n",
      "610  0.728458  0.734328  0.643355  0.741274  unrelated\n",
      "611  0.696325  0.696325  0.671438  0.672340  unrelated\n",
      "612  0.659535  0.649177  0.659535  0.649177  unrelated\n",
      "613  0.565115  0.566268  0.506436  0.505579  unrelated\n",
      "614  0.769514  0.769514  0.774228  0.774228    related\n",
      "615  0.612102  0.610578  0.612102  0.610578  unrelated\n",
      "616  0.746121  0.740418  0.700700  0.702160  unrelated\n",
      "617  0.590174  0.577197  0.563654  0.554732  unrelated\n",
      "618  0.793084  0.795935  0.788687  0.806020    related\n",
      "619  0.711850  0.721462  0.711018  0.719956  unrelated\n",
      "620  0.734002  0.728823  0.727969  0.724255  unrelated\n",
      "621  0.654258  0.643948  0.654258  0.643948  unrelated\n",
      "622  0.603331  0.573397  0.603331  0.573397  unrelated\n",
      "623  0.652818  0.597140  0.634439  0.590845  unrelated\n",
      "624  0.601013  0.577426  0.601013  0.577426  unrelated\n",
      "625  0.619341  0.605425  0.632851  0.619346  unrelated\n",
      "626  0.668951  0.671836  0.668951  0.671836  unrelated\n",
      "627  0.729887  0.713996  0.700107  0.701996  unrelated\n",
      "628  0.703328  0.705869  0.734235  0.744293  unrelated\n",
      "629  0.682219  0.688979  0.618853  0.650318  unrelated\n",
      "630  0.661219  0.661219  0.665344  0.667824  unrelated\n",
      "631  0.730447  0.779220  0.730447  0.779220    related\n",
      "632  0.626656  0.642257  0.621592  0.650258  unrelated\n",
      "633  0.699646  0.699646  0.686255  0.686255  unrelated\n",
      "634  0.645861  0.662438  0.583698  0.620336  unrelated\n",
      "635  0.677190  0.683504  0.663094  0.678598  unrelated\n",
      "636  0.678331  0.699689  0.685389  0.704193  unrelated\n",
      "637  0.565890  0.580909  0.565890  0.580909  unrelated\n",
      "638  0.647069  0.658980  0.647069  0.658980  unrelated\n",
      "639  0.689575  0.679139  0.702672  0.714857  unrelated\n",
      "640  0.782779  0.791118  0.766254  0.769661  unrelated\n",
      "641  0.603767  0.645573  0.603767  0.645573  unrelated\n",
      "642  0.656759  0.658382  0.621766  0.634257  unrelated\n",
      "643  0.704113  0.713120  0.679821  0.688564  unrelated\n",
      "644  0.635139  0.635139  0.636709  0.636709  unrelated\n",
      "645  0.619496  0.625579  0.618409  0.643570  unrelated\n",
      "646  0.656595  0.668293  0.535829  0.555732  unrelated\n",
      "647  0.668549  0.691469  0.664931  0.689272  unrelated\n",
      "648  0.666981  0.682380  0.566443  0.604995  unrelated\n",
      "649  0.657883  0.657883  0.663492  0.663492  unrelated\n",
      "650  0.630848  0.654916  0.594699  0.616378  unrelated\n",
      "651  0.709214  0.709214  0.719222  0.719222  unrelated\n",
      "652  0.722098  0.727346  0.685526  0.713368  unrelated\n",
      "653  0.648318  0.659400  0.648621  0.663573  unrelated\n",
      "654  0.677727  0.686194  0.653430  0.685548  unrelated\n",
      "655  0.727599  0.736905  0.682701  0.693571  unrelated\n",
      "656  0.613353  0.636107  0.617598  0.653403  unrelated\n",
      "657  0.646604  0.652948  0.643756  0.657082  unrelated\n",
      "658  0.653085  0.656300  0.664865  0.669548  unrelated\n",
      "659  0.656993  0.665434  0.649081  0.659612  unrelated\n",
      "660  0.636978  0.649765  0.649716  0.657576  unrelated\n",
      "661  0.721550  0.739787  0.722286  0.742811  unrelated\n",
      "662  0.651403  0.660650  0.671520  0.679843  unrelated\n",
      "663  0.680282  0.680282  0.668208  0.674612  unrelated\n",
      "664  0.677283  0.677283  0.701549  0.701549  unrelated\n",
      "665  0.681787  0.689868  0.587457  0.622545  unrelated\n",
      "666  0.637487  0.647245  0.654295  0.664860  unrelated\n",
      "667  0.621965  0.657500  0.621965  0.657500  unrelated\n",
      "668  0.804762  0.782333  0.806876  0.783456    related\n",
      "669  0.676241  0.695932  0.662014  0.676034  unrelated\n",
      "670  0.674291  0.697252  0.661457  0.661457  unrelated\n",
      "671  0.584926  0.579545  0.595277  0.583999  unrelated\n",
      "672  0.710553  0.699184  0.687307  0.674418  unrelated\n",
      "673  0.697251  0.719405  0.681291  0.685260  unrelated\n",
      "674  0.743260  0.735582  0.720568  0.765662  unrelated\n",
      "675  0.706696  0.706696  0.683688  0.683688  unrelated\n",
      "676  0.737769  0.740892  0.666976  0.676136  unrelated\n",
      "677  0.706416  0.723462  0.689321  0.690021  unrelated\n",
      "678  0.624354  0.624354  0.624354  0.624354  unrelated\n",
      "679  0.816090  0.816090  0.816090  0.816090    related\n",
      "680  0.791725  0.795243  0.737449  0.759442  unrelated\n",
      "681  0.787454  0.787454  0.787454  0.787454    related\n",
      "682  0.637094  0.637094  0.629459  0.629459  unrelated\n",
      "683  0.705678  0.715980  0.685915  0.707966  unrelated\n",
      "684  0.778414  0.778414  0.736416  0.754796  unrelated\n",
      "685  0.738967  0.738967  0.736320  0.736320  unrelated\n",
      "686  0.712444  0.707627  0.701594  0.703321  unrelated\n",
      "687  0.688845  0.770132  0.682249  0.745991  unrelated\n",
      "688  0.710917  0.752993  0.695893  0.743953  unrelated\n",
      "689  0.607844  0.646121  0.607844  0.646121  unrelated\n",
      "690  0.709402  0.709402  0.709402  0.709402  unrelated\n",
      "691  0.717206  0.734709  0.633458  0.694756  unrelated\n",
      "692  0.716541  0.712346  0.709816  0.745502  unrelated\n",
      "693  0.785486  0.785486  0.752113  0.752113  unrelated\n",
      "694  0.679213  0.679213  0.675980  0.692350  unrelated\n",
      "695  0.734560  0.734560  0.734560  0.734560  unrelated\n",
      "696  0.743588  0.738646  0.710886  0.715950  unrelated\n",
      "697  0.723732  0.720491  0.645504  0.645504  unrelated\n",
      "698  0.718545  0.714992  0.716797  0.714993  unrelated\n",
      "699  0.717537  0.718043  0.670152  0.670152  unrelated\n",
      "700  0.661677  0.673382  0.693564  0.693564  unrelated\n",
      "701  0.673460  0.673460  0.656077  0.656077  unrelated\n",
      "702  0.731930  0.731930  0.731930  0.731930  unrelated\n",
      "703  0.761459  0.753961  0.691703  0.706261  unrelated\n",
      "704  0.710279  0.710279  0.645199  0.686823  unrelated\n",
      "705  0.685301  0.712389  0.685301  0.712389  unrelated\n",
      "706  0.754666  0.743723  0.734744  0.737313  unrelated\n",
      "707  0.642609  0.643007  0.642609  0.643007  unrelated\n",
      "708  0.708003  0.708003  0.686680  0.686680  unrelated\n",
      "709  0.768430  0.755920  0.738757  0.742411  unrelated\n",
      "710  0.732126  0.722466  0.645558  0.675728  unrelated\n",
      "711  0.685367  0.674236  0.685367  0.674236  unrelated\n",
      "712  0.785096  0.780503  0.703183  0.716403  unrelated\n",
      "713  0.734904  0.710409  0.630846  0.649518  unrelated\n",
      "714  0.727863  0.726705  0.720136  0.745784  unrelated\n",
      "715  0.675760  0.708200  0.675760  0.708200  unrelated\n",
      "716  0.687096  0.687096  0.619443  0.687763  unrelated\n",
      "717  0.634505  0.662172  0.605452  0.643591  unrelated\n",
      "718  0.686417  0.686417  0.686417  0.686417  unrelated\n",
      "719  0.737394  0.737394  0.737394  0.737394  unrelated\n",
      "720  0.670167  0.669940  0.672952  0.672952  unrelated\n",
      "721  0.741178  0.731662  0.751271  0.735803  unrelated\n",
      "722  0.753166  0.740847  0.698673  0.718090  unrelated\n",
      "723  0.739042  0.769539  0.739042  0.769539  unrelated\n",
      "724  0.724623  0.723352  0.690951  0.712639  unrelated\n",
      "725  0.681294  0.700375  0.681294  0.700375  unrelated\n",
      "726  0.729649  0.717407  0.729649  0.717407  unrelated\n",
      "727  0.754166  0.753108  0.736712  0.727251  unrelated\n",
      "728  0.689262  0.672467  0.689262  0.672467  unrelated\n",
      "729  0.708131  0.693092  0.708131  0.693092  unrelated\n",
      "730  0.692983  0.692983  0.673900  0.673900  unrelated\n",
      "731  0.750218  0.750218  0.740377  0.740377  unrelated\n",
      "732  0.690818  0.697791  0.660135  0.685910  unrelated\n",
      "733  0.726120  0.715651  0.559185  0.559185  unrelated\n",
      "734  0.770406  0.793465  0.770406  0.793465  unrelated\n",
      "735  0.733870  0.747311  0.689938  0.748497  unrelated\n",
      "736  0.667811  0.659638  0.667811  0.659638  unrelated\n",
      "737  0.643376  0.641998  0.643025  0.643025  unrelated\n",
      "738  0.690073  0.700596  0.690073  0.700596  unrelated\n",
      "739  0.730390  0.731124  0.730390  0.731124  unrelated\n",
      "740  0.712737  0.693588  0.594995  0.622555  unrelated\n",
      "741  0.520874  0.520874  0.483257  0.483257  unrelated\n",
      "742  0.739579  0.739566  0.741536  0.747422  unrelated\n",
      "743  0.812882  0.801248  0.760562  0.774455  unrelated\n",
      "744  0.708903  0.708591  0.719600  0.714949  unrelated\n",
      "745  0.637594  0.649081  0.647831  0.674544  unrelated\n",
      "746  0.731655  0.731655  0.748620  0.750892  unrelated\n",
      "747  0.749426  0.750452  0.727406  0.745684  unrelated\n",
      "748  0.701200  0.701200  0.639032  0.639032  unrelated\n",
      "749  0.726369  0.726369  0.717435  0.717435  unrelated\n",
      "750  0.749684  0.749684  0.759439  0.759439  unrelated\n",
      "751  0.693573  0.718686  0.679180  0.714280  unrelated\n",
      "752  0.647540  0.599823  0.618112  0.680253  unrelated\n",
      "753  0.697323  0.697323  0.664124  0.664124  unrelated\n",
      "754  0.731668  0.715879  0.688094  0.713386  unrelated\n",
      "755  0.668043  0.671038  0.670770  0.670770  unrelated\n",
      "756  0.874211  0.765389  0.747428  0.782437    related\n",
      "757  0.675681  0.674287  0.670712  0.680850  unrelated\n",
      "758  0.753171  0.741188  0.638822  0.693575  unrelated\n",
      "759  0.740378  0.723509  0.697726  0.694498  unrelated\n",
      "760  0.766576  0.774886  0.766576  0.774886  unrelated\n",
      "761  0.627405  0.631332  0.605323  0.622483  unrelated\n",
      "762  0.710834  0.710834  0.710834  0.710834  unrelated\n",
      "763  0.726581  0.726581  0.711727  0.709428  unrelated\n",
      "764  0.771199  0.750356  0.752442  0.757235    related\n",
      "765  0.670467  0.673497  0.702059  0.717728  unrelated\n",
      "766  0.722486  0.713941  0.698355  0.709948  unrelated\n",
      "767  0.740497  0.743894  0.740497  0.743894  unrelated\n",
      "768  0.705107  0.705107  0.633318  0.642023  unrelated\n",
      "769  0.719896  0.719896  0.684478  0.684478  unrelated\n",
      "770  0.707406  0.705793  0.639967  0.644662  unrelated\n",
      "771  0.630370  0.642186  0.630370  0.642186  unrelated\n",
      "772  0.640628  0.620706  0.587536  0.567514  unrelated\n",
      "773  0.681006  0.693315  0.593069  0.589629  unrelated\n",
      "774  0.686041  0.672569  0.620519  0.631204  unrelated\n",
      "775  0.705381  0.704321  0.700904  0.705808  unrelated\n",
      "776  0.721632  0.720338  0.723168  0.731477  unrelated\n",
      "777  0.637199  0.655762  0.637199  0.655762  unrelated\n",
      "778  0.722518  0.721630  0.745402  0.745100  unrelated\n",
      "779  0.670120  0.658318  0.648733  0.647750  unrelated\n",
      "780  0.707770  0.715570  0.698874  0.717321  unrelated\n",
      "781  0.681062  0.679689  0.628646  0.630479  unrelated\n",
      "782  0.667631  0.656199  0.630856  0.644658  unrelated\n",
      "783  0.697697  0.698416  0.649432  0.644941  unrelated\n",
      "784  0.681062  0.679689  0.628646  0.630479  unrelated\n",
      "785  0.583789  0.579674  0.611894  0.612904  unrelated\n",
      "786  0.644385  0.619943  0.626017  0.615008  unrelated\n",
      "787  0.648974  0.658456  0.653285  0.664380  unrelated\n",
      "788  0.666192  0.660597  0.610291  0.614769  unrelated\n",
      "789  0.646608  0.678952  0.646608  0.678952  unrelated\n",
      "790  0.647808  0.640948  0.641978  0.658782  unrelated\n",
      "791  0.770507  0.742607  0.727927  0.725993  unrelated\n",
      "792  0.744004  0.743092  0.710913  0.721896  unrelated\n",
      "793  0.654615  0.633967  0.625704  0.597121  unrelated\n",
      "794  0.666160  0.668105  0.632824  0.655421  unrelated\n",
      "795  0.708868  0.701256  0.673672  0.689431  unrelated\n",
      "796  0.698090  0.713336  0.699110  0.713534  unrelated\n",
      "797  0.712322  0.696111  0.711840  0.711970  unrelated\n",
      "798  0.590192  0.598297  0.590192  0.598297  unrelated\n",
      "799  0.717641  0.714556  0.680793  0.674534  unrelated\n",
      "800  0.613512  0.595456  0.637864  0.648507  unrelated\n",
      "801  0.661875  0.657174  0.645248  0.650798  unrelated\n",
      "802  0.672415  0.671596  0.693464  0.700621  unrelated\n",
      "803  0.679900  0.682768  0.657218  0.670585  unrelated\n",
      "804  0.751144  0.745036  0.687950  0.709925  unrelated\n",
      "805  0.660312  0.652075  0.665341  0.668832  unrelated\n",
      "806  0.738212  0.717969  0.737977  0.723089  unrelated\n",
      "807  0.767943  0.778466  0.741287  0.733887  unrelated\n",
      "808  0.634323  0.614028  0.595283  0.581821  unrelated\n",
      "809  0.712391  0.729091  0.698102  0.721194  unrelated\n",
      "810  0.677936  0.642183  0.652046  0.636073  unrelated\n",
      "811  0.594932  0.587737  0.594932  0.587737  unrelated\n",
      "812  0.661861  0.682692  0.661861  0.682692  unrelated\n",
      "813  0.669706  0.659915  0.622647  0.631658  unrelated\n",
      "814  0.634982  0.616247  0.612745  0.619929  unrelated\n",
      "815  0.723112  0.719699  0.750336  0.758848  unrelated\n",
      "816  0.681006  0.693315  0.593069  0.589629  unrelated\n",
      "817  0.676015  0.697034  0.671958  0.715048  unrelated\n",
      "818  0.673309  0.680937  0.625181  0.640202  unrelated\n",
      "819  0.685677  0.676194  0.648949  0.656206  unrelated\n",
      "820  0.706837  0.715731  0.712858  0.706385  unrelated\n",
      "821  0.648090  0.651457  0.648090  0.651457  unrelated\n",
      "822  0.652784  0.627352  0.633454  0.615430  unrelated\n",
      "823  0.600434  0.585488  0.600434  0.585488  unrelated\n",
      "824  0.658399  0.656843  0.658399  0.656843  unrelated\n",
      "825  0.724214  0.738092  0.661309  0.680577  unrelated\n",
      "826  0.618231  0.640386  0.618231  0.640386  unrelated\n",
      "827  0.681008  0.701140  0.632761  0.645976  unrelated\n",
      "828  0.656575  0.658212  0.656575  0.658212  unrelated\n",
      "829  0.655641  0.683357  0.655641  0.683357  unrelated\n",
      "830  0.633094  0.618995  0.550215  0.537354  unrelated\n",
      "831  0.627192  0.620346  0.618503  0.620703  unrelated\n",
      "832  0.608108  0.597834  0.631410  0.636836  unrelated\n",
      "833  0.660415  0.666205  0.663611  0.679848  unrelated\n",
      "834  0.651746  0.639797  0.686616  0.687286  unrelated\n",
      "835  0.685353  0.684162  0.685353  0.684162  unrelated\n",
      "836  0.643739  0.633288  0.618470  0.617995  unrelated\n",
      "837  0.585809  0.567363  0.561190  0.553190  unrelated\n",
      "838  0.718389  0.713165  0.545644  0.552791  unrelated\n",
      "839  0.699772  0.717510  0.709009  0.729282  unrelated\n",
      "840  0.726090  0.737962  0.699816  0.714090  unrelated\n",
      "841  0.620994  0.602854  0.576199  0.576199  unrelated\n",
      "842  0.711764  0.699949  0.713320  0.714375  unrelated\n",
      "843  0.697261  0.701959  0.697261  0.701959  unrelated\n",
      "844  0.670155  0.662821  0.541439  0.557511  unrelated\n",
      "845  0.672017  0.705721  0.632383  0.665935  unrelated\n",
      "846  0.651351  0.617131  0.622734  0.645450  unrelated\n",
      "847  0.633680  0.633153  0.620416  0.622639  unrelated\n",
      "848  0.639850  0.604341  0.634095  0.616636  unrelated\n",
      "849  0.681160  0.689617  0.720579  0.737505  unrelated\n",
      "850  0.724839  0.729786  0.644428  0.650987  unrelated\n",
      "851  0.642079  0.642631  0.642079  0.642631  unrelated\n",
      "852  0.648875  0.643049  0.624418  0.632983  unrelated\n",
      "853  0.693922  0.688773  0.693922  0.688773  unrelated\n",
      "854  0.649807  0.624946  0.648436  0.651804  unrelated\n",
      "855  0.719703  0.720775  0.690757  0.720366  unrelated\n",
      "856  0.626175  0.646077  0.617553  0.634785  unrelated\n",
      "857  0.630066  0.605799  0.599031  0.604859  unrelated\n",
      "858  0.648668  0.670811  0.635494  0.663649  unrelated\n",
      "859  0.656402  0.738486  0.563548  0.592469  unrelated\n",
      "860  0.589157  0.606735  0.589157  0.606735  unrelated\n",
      "861  0.736358  0.729049  0.683124  0.687004  unrelated\n",
      "862  0.702233  0.687835  0.666992  0.665301  unrelated\n",
      "863  0.583789  0.579674  0.611894  0.612904  unrelated\n",
      "864  0.708588  0.692292  0.716927  0.714696  unrelated\n",
      "865  0.694338  0.681137  0.663705  0.648677  unrelated\n",
      "866  0.582357  0.624572  0.582357  0.624572  unrelated\n",
      "867  0.639784  0.621528  0.639784  0.621528  unrelated\n",
      "868  0.585674  0.620665  0.579142  0.602987  unrelated\n",
      "869  0.710169  0.702918  0.660097  0.685024  unrelated\n",
      "870  0.586480  0.581268  0.577744  0.590010  unrelated\n",
      "871  0.638577  0.621561  0.641252  0.649969  unrelated\n",
      "872  0.596382  0.609767  0.561950  0.578895  unrelated\n",
      "873  0.677748  0.658736  0.638445  0.633506  unrelated\n",
      "874  0.693904  0.695938  0.645919  0.645919  unrelated\n",
      "875  0.551852  0.578822  0.540911  0.581308  unrelated\n",
      "876  0.699526  0.699982  0.681562  0.688948  unrelated\n",
      "877  0.673835  0.676139  0.658989  0.656318  unrelated\n",
      "878  0.661387  0.649915  0.661387  0.649915  unrelated\n",
      "879  0.636579  0.618372  0.625152  0.608256  unrelated\n",
      "880  0.600492  0.588086  0.618129  0.619207  unrelated\n",
      "881  0.718856  0.713000  0.654768  0.670421  unrelated\n",
      "882  0.637703  0.639562  0.613736  0.643276  unrelated\n",
      "883  0.708828  0.714425  0.708828  0.714425  unrelated\n",
      "884  0.669592  0.660422  0.602792  0.602515  unrelated\n",
      "885  0.627193  0.594997  0.625892  0.599795  unrelated\n",
      "886  0.728214  0.739327  0.704054  0.736278  unrelated\n",
      "887  0.642069  0.617775  0.612798  0.605241  unrelated\n",
      "888  0.709331  0.684365  0.674660  0.661427  unrelated\n",
      "889  0.674838  0.674496  0.631296  0.644649  unrelated\n",
      "890  0.643438  0.625257  0.611975  0.599252  unrelated\n",
      "891  0.673052  0.674332  0.600957  0.601648  unrelated\n",
      "892  0.690728  0.658976  0.692544  0.674384  unrelated\n",
      "893  0.728866  0.729459  0.726400  0.728148  unrelated\n",
      "894  0.808785  0.833455  0.808785  0.833455    related\n",
      "895  0.740685  0.742594  0.693114  0.698618  unrelated\n",
      "896  0.694914  0.703925  0.707713  0.714569  unrelated\n",
      "897  0.631667  0.644474  0.608466  0.622039  unrelated\n",
      "898  0.692921  0.689529  0.692921  0.689529  unrelated\n",
      "899  0.634572  0.610738  0.600114  0.613307  unrelated\n",
      "900  0.665989  0.654306  0.651225  0.655045  unrelated\n",
      "901  0.623742  0.645273  0.623742  0.645273  unrelated\n",
      "902  0.657676  0.672561  0.657676  0.672561  unrelated\n",
      "903  0.678460  0.705924  0.641509  0.676246  unrelated\n",
      "904  0.678081  0.700632  0.629375  0.654828  unrelated\n",
      "905  0.708724  0.712287  0.678981  0.694474  unrelated\n",
      "906  0.645208  0.645208  0.570199  0.570199  unrelated\n",
      "907  0.686028  0.691073  0.658376  0.660609  unrelated\n",
      "908  0.651560  0.655701  0.629825  0.645743  unrelated\n",
      "909  0.683478  0.705875  0.653697  0.690009  unrelated\n",
      "910  0.677122  0.693213  0.611857  0.639276  unrelated\n",
      "911  0.633152  0.641170  0.639356  0.639356  unrelated\n",
      "912  0.636841  0.636841  0.636841  0.636841  unrelated\n",
      "913  0.709082  0.693185  0.690615  0.690615  unrelated\n",
      "914  0.674925  0.691114  0.674925  0.691114  unrelated\n",
      "915  0.635092  0.635092  0.596451  0.596451  unrelated\n",
      "916  0.582927  0.584649  0.582927  0.584649  unrelated\n",
      "917  0.689956  0.710737  0.644121  0.675040  unrelated\n",
      "918  0.630164  0.630164  0.630164  0.630164  unrelated\n",
      "919  0.719504  0.722576  0.635424  0.635424  unrelated\n",
      "920  0.576600  0.583230  0.576600  0.583230  unrelated\n",
      "921  0.736560  0.740487  0.736560  0.740487  unrelated\n",
      "922  0.658043  0.673611  0.644750  0.654775  unrelated\n",
      "923  0.742913  0.753903  0.757182  0.757182    related\n",
      "924  0.690542  0.707385  0.646115  0.646115  unrelated\n",
      "925  0.612318  0.612318  0.612318  0.612318  unrelated\n",
      "926  0.539960  0.539960  0.539960  0.539960  unrelated\n",
      "927  0.657699  0.650810  0.577764  0.582723  unrelated\n",
      "928  0.705496  0.702566  0.689698  0.692929  unrelated\n",
      "929  0.599467  0.619874  0.634111  0.670918  unrelated\n",
      "930  0.671784  0.671417  0.649709  0.652265  unrelated\n",
      "931  0.616831  0.616831  0.556281  0.590171  unrelated\n",
      "932  0.640532  0.657893  0.653793  0.670623  unrelated\n",
      "933  0.767215  0.767215  0.775729  0.775729  unrelated\n",
      "934  0.718684  0.711626  0.667461  0.682839  unrelated\n",
      "935  0.632740  0.632098  0.636440  0.636440  unrelated\n",
      "936  0.625664  0.629819  0.625664  0.629819  unrelated\n",
      "937  0.716308  0.712888  0.684218  0.684218  unrelated\n",
      "938  0.610816  0.620468  0.625111  0.635341  unrelated\n",
      "939  0.687753  0.691498  0.679777  0.709174  unrelated\n",
      "940  0.741587  0.748472  0.711745  0.716070  unrelated\n",
      "941  0.630807  0.630807  0.598022  0.602645  unrelated\n",
      "942  0.684449  0.682341  0.626491  0.645832  unrelated\n",
      "943  0.601455  0.601455  0.591125  0.594369  unrelated\n",
      "944  0.680215  0.684143  0.602938  0.602938  unrelated\n",
      "945  0.672749  0.666794  0.649154  0.649154  unrelated\n",
      "946  0.687660  0.696230  0.649124  0.662194  unrelated\n",
      "947  0.663916  0.667556  0.626328  0.648299  unrelated\n",
      "948  0.647990  0.673524  0.647990  0.673524  unrelated\n",
      "949  0.682262  0.685265  0.633213  0.650889  unrelated\n",
      "950  0.677815  0.667827  0.669206  0.676097  unrelated\n",
      "951  0.627026  0.638350  0.647546  0.647546  unrelated\n",
      "952  0.565180  0.565180  0.534656  0.534656  unrelated\n",
      "953  0.593249  0.616034  0.573699  0.573481  unrelated\n",
      "954  0.729770  0.717633  0.726513  0.731869  unrelated\n",
      "955  0.655675  0.655675  0.626551  0.642070  unrelated\n",
      "956  0.674867  0.674867  0.670071  0.670071  unrelated\n",
      "957  0.761495  0.757937  0.702404  0.707961  unrelated\n",
      "958  0.622755  0.622755  0.622755  0.622755  unrelated\n",
      "959  0.594137  0.594137  0.528101  0.528101  unrelated\n",
      "960  0.659701  0.653359  0.659701  0.653359  unrelated\n",
      "961  0.680321  0.690800  0.643405  0.655345  unrelated\n",
      "962  0.620160  0.620160  0.642439  0.658258  unrelated\n",
      "963  0.692820  0.705896  0.667748  0.682306  unrelated\n",
      "964  0.644403  0.645817  0.607744  0.622132  unrelated\n",
      "965  0.633147  0.642445  0.627847  0.643673  unrelated\n",
      "966  0.639515  0.641682  0.594616  0.619444  unrelated\n",
      "967  0.635435  0.635435  0.543442  0.543442  unrelated\n",
      "968  0.611697  0.603628  0.610662  0.610662  unrelated\n",
      "969  0.620063  0.620063  0.614593  0.614593  unrelated\n",
      "970  0.650195  0.640148  0.650195  0.640148  unrelated\n",
      "971  0.717951  0.726919  0.700148  0.700270  unrelated\n",
      "972  0.689656  0.707107  0.643527  0.650622  unrelated\n",
      "973  0.597163  0.604580  0.616482  0.616482  unrelated\n",
      "974  0.729129  0.738838  0.670286  0.691643  unrelated\n",
      "975  0.573726  0.573726  0.557037  0.557184  unrelated\n",
      "976  0.776789  0.776789  0.776789  0.776789  unrelated\n",
      "977  0.550802  0.557977  0.495786  0.503388  unrelated\n",
      "978  0.644790  0.649405  0.633853  0.641604  unrelated\n",
      "979  0.641479  0.647908  0.638676  0.661766  unrelated\n",
      "980  0.671163  0.677077  0.594337  0.615261  unrelated\n",
      "981  0.673698  0.673698  0.628423  0.628423  unrelated\n",
      "982  0.616949  0.616595  0.577499  0.617940  unrelated\n",
      "983  0.675339  0.696357  0.671729  0.703357  unrelated\n",
      "984  0.681611  0.693591  0.674402  0.684317  unrelated\n",
      "985  0.597012  0.597012  0.577892  0.585008  unrelated\n",
      "986  0.619331  0.636451  0.619635  0.616810  unrelated\n",
      "987  0.679423  0.688376  0.659778  0.659778  unrelated\n",
      "988  0.652664  0.657931  0.660493  0.668800  unrelated\n",
      "989  0.649323  0.650188  0.640156  0.644832  unrelated\n",
      "990  0.663977  0.671469  0.652209  0.663675  unrelated\n",
      "991  0.708765  0.713478  0.689123  0.698672  unrelated\n",
      "992  0.629351  0.632933  0.648064  0.655402  unrelated\n",
      "993  0.695243  0.706783  0.659792  0.672178  unrelated\n",
      "994  0.648116  0.674082  0.648116  0.674082  unrelated\n",
      "995  0.674799  0.684276  0.676127  0.717257  unrelated\n",
      "996  0.620160  0.620160  0.642439  0.658258  unrelated\n",
      "997  0.702055  0.707214  0.702055  0.707214  unrelated\n",
      "998  0.633660  0.609127  0.633660  0.609127  unrelated\n",
      "999  0.709778  0.709894  0.704875  0.704875  unrelated\n"
     ]
    }
   ],
   "source": [
    "print(bert_napredak_results_max.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_napredak_results_max_related = bert_napredak_results_max[bert_napredak_results_max.Stance == 'related']\n",
    "bert_napredak_results_max_unrelated = bert_napredak_results_max[bert_napredak_results_max.Stance == 'unrelated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01    0.688457\n",
       "0.05    0.709983\n",
       "0.10    0.732371\n",
       "0.15    0.743061\n",
       "0.20    0.752685\n",
       "0.25    0.764993\n",
       "0.30    0.774630\n",
       "0.35    0.782564\n",
       "0.40    0.792877\n",
       "0.45    0.798543\n",
       "0.50    0.806359\n",
       "0.55    0.814102\n",
       "0.60    0.822401\n",
       "0.65    0.828489\n",
       "0.70    0.835244\n",
       "0.75    0.840653\n",
       "0.80    0.851319\n",
       "0.85    0.862414\n",
       "0.90    0.871631\n",
       "0.95    0.887814\n",
       "0.99    0.949210\n",
       "Name: sa-sa, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_napredak_results_max_related['sa-sa'].quantile([.01, .05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05    0.585911\n",
       "0.10    0.605074\n",
       "0.15    0.620121\n",
       "0.20    0.630049\n",
       "0.25    0.637224\n",
       "0.30    0.644843\n",
       "0.35    0.652113\n",
       "0.40    0.660374\n",
       "0.45    0.667997\n",
       "0.50    0.676015\n",
       "0.55    0.681837\n",
       "0.60    0.689607\n",
       "0.65    0.697230\n",
       "0.70    0.707948\n",
       "0.75    0.715037\n",
       "0.80    0.722377\n",
       "0.85    0.730875\n",
       "0.90    0.742857\n",
       "0.95    0.763672\n",
       "Name: sa-sa, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_napredak_results_max_unrelated['sa-sa'].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_col = bert_napredak_results_max_related['sa-sa'] - bert_napredak_results_max_related['bez-bez']\n",
    "\n",
    "bert_napredak_results_max_related_sum = bert_napredak_results_max_related\n",
    "\n",
    "bert_napredak_results_max_related_sum[\"sum\"] = sum_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_col = bert_napredak_results_max_unrelated['sa-sa'] - bert_napredak_results_max_unrelated['bez-bez']\n",
    "\n",
    "bert_napredak_results_max_unrelated_sum = bert_napredak_results_max_unrelated\n",
    "\n",
    "bert_napredak_results_max_unrelated_sum[\"sum\"] = sum_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05   -0.024745\n",
       "0.10   -0.015455\n",
       "0.15   -0.011208\n",
       "0.20   -0.006624\n",
       "0.25   -0.001845\n",
       "0.30    0.000000\n",
       "0.35    0.000000\n",
       "0.40    0.000000\n",
       "0.45    0.004226\n",
       "0.50    0.007936\n",
       "0.55    0.014579\n",
       "0.60    0.019687\n",
       "0.65    0.025352\n",
       "0.70    0.032065\n",
       "0.75    0.039082\n",
       "0.80    0.047595\n",
       "0.85    0.059732\n",
       "0.90    0.080574\n",
       "0.95    0.098675\n",
       "Name: sum, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_napredak_results_max_related_sum[\"sum\"].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05   -0.031808\n",
       "0.10   -0.023099\n",
       "0.15   -0.016923\n",
       "0.20   -0.011030\n",
       "0.25   -0.006605\n",
       "0.30   -0.003422\n",
       "0.35   -0.000506\n",
       "0.40    0.000000\n",
       "0.45    0.001232\n",
       "0.50    0.004976\n",
       "0.55    0.007224\n",
       "0.60    0.010911\n",
       "0.65    0.015047\n",
       "0.70    0.019620\n",
       "0.75    0.024019\n",
       "0.80    0.030714\n",
       "0.85    0.038439\n",
       "0.90    0.047302\n",
       "0.95    0.062713\n",
       "Name: sum, dtype: float64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_napredak_results_max_unrelated_sum[\"sum\"].quantile([.05, .1, .15, .2, .25, .3, .35, .4, .45, .5, .55, .6, .65, .7, .75, .8, .85, .9, .95])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sa-sa    sa-bez    bez-sa   bez-bez   Stance       sum\n",
      "0    0.855309  0.751238  0.763842  0.749235  related  0.106074\n",
      "1    0.765006  0.749426  0.684933  0.700125  related  0.064882\n",
      "2    0.715473  0.689930  0.715473  0.689930  related  0.025542\n",
      "3    0.782727  0.781128  0.749278  0.749278  related  0.033449\n",
      "4    0.805850  0.775058  0.723792  0.725149  related  0.080701\n",
      "5    0.818662  0.799181  0.773036  0.787995  related  0.030667\n",
      "6    0.796831  0.748722  0.643255  0.709587  related  0.087244\n",
      "7    0.810295  0.810295  0.810295  0.810295  related  0.000000\n",
      "8    0.719077  0.694791  0.682593  0.683731  related  0.035346\n",
      "9    0.793313  0.736828  0.720594  0.694733  related  0.098580\n",
      "10   0.744918  0.744918  0.744918  0.744918  related  0.000000\n",
      "11   0.806868  0.806868  0.806868  0.806868  related  0.000000\n",
      "12   0.779514  0.763882  0.779514  0.763882  related  0.015632\n",
      "13   0.915655  0.920901  0.915655  0.920901  related -0.005246\n",
      "14   0.851319  0.875597  0.845114  0.863322  related -0.012003\n",
      "15   0.775219  0.774787  0.717746  0.737626  related  0.037593\n",
      "16   0.749054  0.765888  0.761890  0.787457  related -0.038403\n",
      "17   0.836785  0.836785  0.836785  0.836785  related  0.000000\n",
      "18   0.795162  0.784621  0.795162  0.784621  related  0.010541\n",
      "19   0.828489  0.828489  0.828489  0.828489  related  0.000000\n",
      "20   0.846154  0.816261  0.788313  0.828215  related  0.017939\n",
      "21   0.745290  0.745290  0.768279  0.760754  related -0.015463\n",
      "22   0.862889  0.822836  0.748499  0.764214  related  0.098675\n",
      "23   0.778131  0.819207  0.778131  0.819207  related -0.041076\n",
      "24   0.732351  0.750338  0.681620  0.697170  related  0.035181\n",
      "25   0.769514  0.769514  0.774228  0.774228  related -0.004715\n",
      "26   0.793084  0.795935  0.788687  0.806020  related -0.012936\n",
      "27   0.730447  0.779220  0.730447  0.779220  related -0.048772\n",
      "28   0.804762  0.782333  0.806876  0.783456  related  0.021306\n",
      "29   0.816090  0.816090  0.816090  0.816090  related  0.000000\n",
      "30   0.787454  0.787454  0.787454  0.787454  related  0.000000\n",
      "31   0.874211  0.765389  0.747428  0.782437  related  0.091774\n",
      "32   0.771199  0.750356  0.752442  0.757235  related  0.013964\n",
      "33   0.808785  0.833455  0.808785  0.833455  related -0.024670\n",
      "34   0.742913  0.753903  0.757182  0.757182  related -0.014268\n",
      "35   0.857981  0.843948  0.857981  0.843948  related  0.014033\n",
      "36   0.853628  0.853628  0.835628  0.835628  related  0.018000\n",
      "37   0.887556  0.804351  0.778532  0.894192  related -0.006635\n",
      "38   0.857878  0.857878  0.857878  0.857878  related  0.000000\n",
      "39   0.910406  0.908728  0.893377  0.910949  related -0.000543\n",
      "40   0.755067  0.757609  0.755067  0.757609  related -0.002542\n",
      "41   0.753198  0.722984  0.663162  0.689280  related  0.063918\n",
      "42   0.727218  0.731981  0.658776  0.683247  related  0.043971\n",
      "43   0.881416  0.838638  0.734192  0.778981  related  0.102435\n",
      "44   0.795722  0.795722  0.795722  0.795722  related  0.000000\n",
      "45   0.772420  0.772420  0.772420  0.772420  related  0.000000\n",
      "46   0.849148  0.849148  0.840604  0.840604  related  0.008544\n",
      "47   0.774396  0.756914  0.658169  0.689726  related  0.084671\n",
      "48   0.865557  0.862416  0.826183  0.835787  related  0.029770\n",
      "49   0.876534  0.882460  0.811916  0.848670  related  0.027864\n",
      "50   0.688457  0.724429  0.688457  0.724429  related -0.035973\n",
      "51   0.875592  0.838137  0.808751  0.828058  related  0.047534\n",
      "52   0.821224  0.807336  0.739140  0.789982  related  0.031242\n",
      "53   0.822223  0.841527  0.822223  0.841527  related -0.019304\n",
      "54   0.982270  0.939142  0.949493  0.978101  related  0.004169\n",
      "55   0.848264  0.814403  0.827495  0.841558  related  0.006706\n",
      "56   0.826747  0.825905  0.782364  0.782364  related  0.044383\n",
      "57   0.811009  0.814818  0.772160  0.783381  related  0.027628\n",
      "58   0.733407  0.733407  0.683879  0.685797  related  0.047610\n",
      "59   0.855439  0.864161  0.855439  0.864161  related -0.008722\n",
      "60   0.809702  0.811553  0.809702  0.811553  related -0.001851\n",
      "61   0.834191  0.834191  0.834191  0.834191  related  0.000000\n",
      "62   0.866610  0.865170  0.848469  0.874277  related -0.007667\n",
      "63   0.836843  0.830244  0.795948  0.802258  related  0.034585\n",
      "64   0.849448  0.839389  0.842893  0.848970  related  0.000477\n",
      "65   0.742524  0.695155  0.659049  0.659049  related  0.083475\n",
      "66   0.801793  0.808583  0.801793  0.808583  related -0.006790\n",
      "67   0.809827  0.807328  0.809827  0.807328  related  0.002499\n",
      "68   0.773103  0.779684  0.773103  0.779684  related -0.006581\n",
      "69   0.837458  0.837458  0.786539  0.786539  related  0.050920\n",
      "70   0.789172  0.789172  0.789172  0.789172  related  0.000000\n",
      "71   0.743901  0.734605  0.700180  0.718902  related  0.024999\n",
      "72   0.703681  0.692024  0.642391  0.647388  related  0.056293\n",
      "73   0.764989  0.722156  0.705585  0.686807  related  0.078182\n",
      "74   0.735038  0.741577  0.735038  0.741577  related -0.006539\n",
      "75   0.838891  0.867511  0.799222  0.852840  related -0.013949\n",
      "76   0.836316  0.791509  0.743418  0.823976  related  0.012340\n",
      "77   0.858903  0.850065  0.805119  0.819764  related  0.039139\n",
      "78   0.834501  0.828643  0.809419  0.803149  related  0.031352\n",
      "79   0.836133  0.818647  0.836133  0.818647  related  0.017486\n",
      "80   0.832501  0.837621  0.832501  0.837621  related -0.005120\n",
      "81   0.794049  0.786379  0.776226  0.776226  related  0.017823\n",
      "82   0.826917  0.843176  0.826917  0.843176  related -0.016259\n",
      "83   0.799600  0.814315  0.734297  0.771075  related  0.028526\n",
      "84   0.686359  0.676111  0.644353  0.678934  related  0.007425\n",
      "85   0.732780  0.711060  0.657808  0.678181  related  0.054598\n",
      "86   0.832688  0.817832  0.832688  0.817832  related  0.014856\n",
      "87   0.813101  0.791502  0.803150  0.802625  related  0.010476\n",
      "88   0.858416  0.839614  0.787947  0.807672  related  0.050744\n",
      "89   0.887828  0.884019  0.852558  0.863019  related  0.024809\n",
      "90   0.766789  0.766789  0.816765  0.816765  related -0.049977\n",
      "91   0.851922  0.814086  0.833371  0.834186  related  0.017736\n",
      "92   0.826403  0.814679  0.786975  0.793800  related  0.032603\n",
      "93   0.867985  0.854020  0.867985  0.854020  related  0.013965\n",
      "94   0.866315  0.843208  0.822403  0.823443  related  0.042872\n",
      "95   0.740243  0.761624  0.740243  0.761624  related -0.021382\n",
      "96   0.856839  0.869067  0.850966  0.865650  related -0.008811\n",
      "97   0.749153  0.755144  0.748173  0.764536  related -0.015383\n",
      "98   0.688457  0.696367  0.688457  0.696367  related -0.007910\n",
      "99   0.815103  0.795870  0.835881  0.845146  related -0.030043\n",
      "100  0.817125  0.817850  0.771553  0.795511  related  0.021614\n",
      "101  0.734878  0.743344  0.734878  0.743344  related -0.008466\n",
      "102  0.737913  0.766310  0.737913  0.766310  related -0.028396\n",
      "103  0.794307  0.810264  0.794307  0.810264  related -0.015957\n",
      "104  0.697433  0.691216  0.697433  0.691216  related  0.006217\n",
      "105  0.830086  0.832913  0.830086  0.832913  related -0.002828\n",
      "106  0.823780  0.824131  0.823780  0.824131  related -0.000351\n",
      "107  0.753724  0.799186  0.666613  0.706737  related  0.046987\n",
      "108  0.869627  0.884715  0.852531  0.877814  related -0.008186\n",
      "109  0.839786  0.790040  0.780457  0.825545  related  0.014241\n",
      "110  0.775789  0.763433  0.756686  0.755460  related  0.020329\n",
      "111  0.845772  0.835836  0.804078  0.791296  related  0.054475\n",
      "112  0.709784  0.701654  0.695682  0.711610  related -0.001826\n",
      "113  0.752556  0.758137  0.763808  0.773819  related -0.021263\n",
      "114  0.804104  0.804104  0.804104  0.804104  related  0.000000\n",
      "115  0.779102  0.779102  0.768220  0.768220  related  0.010882\n",
      "116  0.873277  0.874510  0.873277  0.874510  related -0.001234\n",
      "117  0.807545  0.798803  0.804852  0.800535  related  0.007010\n",
      "118  0.802909  0.775757  0.727925  0.719803  related  0.083106\n",
      "119  1.000000  0.894412  0.894412  1.000000  related  0.000000\n",
      "120  0.838403  0.838403  0.838403  0.838403  related  0.000000\n",
      "121  0.854686  0.886307  0.854686  0.886307  related -0.031621\n",
      "122  0.862893  0.776334  0.838425  0.808027  related  0.054866\n",
      "123  0.879625  0.886285  0.879625  0.886285  related -0.006660\n",
      "124  0.948488  0.952541  0.948488  0.952541  related -0.004052\n",
      "125  0.774548  0.712525  0.655051  0.713992  related  0.060557\n",
      "126  0.768672  0.768672  0.768672  0.768672  related  0.000000\n",
      "127  0.787253  0.787253  0.787253  0.787253  related  0.000000\n",
      "128  0.921365  0.921365  0.921365  0.921365  related  0.000000\n",
      "129  0.733566  0.739683  0.688562  0.688562  related  0.045003\n",
      "130  0.809211  0.723711  0.762071  0.757080  related  0.052131\n",
      "131  0.705119  0.751959  0.698177  0.733654  related -0.028535\n",
      "132  0.917538  0.917538  0.917538  0.917538  related  0.000000\n",
      "133  0.817263  0.811881  0.817263  0.811881  related  0.005381\n",
      "134  0.814509  0.806924  0.688911  0.746713  related  0.067796\n",
      "135  0.808532  0.769580  0.790497  0.824085  related -0.015553\n",
      "136  0.854229  0.854229  0.854229  0.854229  related  0.000000\n",
      "137  0.765052  0.762596  0.719824  0.762813  related  0.002239\n",
      "138  0.872219  0.873046  0.812436  0.839224  related  0.032994\n",
      "139  0.803933  0.761401  0.527738  0.685693  related  0.118240\n",
      "140  0.867477  0.867477  0.867477  0.867477  related  0.000000\n",
      "141  0.837255  0.837255  0.837255  0.837255  related  0.000000\n",
      "142  0.840941  0.772754  0.717700  0.717700  related  0.123241\n",
      "143  0.828173  0.846821  0.828173  0.846821  related -0.018648\n",
      "144  0.849010  0.844713  0.849010  0.844713  related  0.004297\n",
      "145  0.870583  0.890595  0.870583  0.890595  related -0.020012\n",
      "146  0.906021  0.917466  0.906021  0.917466  related -0.011445\n",
      "147  0.864936  0.856744  0.858240  0.860222  related  0.004714\n",
      "148  0.834625  0.814317  0.784976  0.779298  related  0.055327\n",
      "149  0.718075  0.715895  0.670253  0.681678  related  0.036396\n",
      "150  0.792713  0.797088  0.771452  0.785365  related  0.007348\n",
      "151  0.830757  0.664240  0.753124  0.727966  related  0.102791\n",
      "152  0.713758  0.694765  0.659668  0.653550  related  0.060207\n",
      "153  0.717679  0.696387  0.564373  0.585936  related  0.131743\n",
      "154  0.774114  0.784725  0.752748  0.763421  related  0.010694\n",
      "155  0.800155  0.763667  0.681203  0.712985  related  0.087170\n",
      "156  0.828573  0.790068  0.783605  0.782345  related  0.046228\n",
      "157  0.835509  0.844354  0.847175  0.847175  related -0.011666\n",
      "158  0.833212  0.833212  0.833212  0.833212  related  0.000000\n",
      "159  0.847331  0.847331  0.842459  0.842459  related  0.004871\n",
      "160  0.809364  0.809364  0.785914  0.785914  related  0.023450\n",
      "161  0.801857  0.801857  0.801857  0.801857  related  0.000000\n",
      "162  0.823715  0.751911  0.783499  0.764655  related  0.059060\n",
      "163  0.737208  0.739138  0.688702  0.712340  related  0.024868\n",
      "164  0.820238  0.736341  0.708424  0.690675  related  0.129563\n",
      "165  0.755524  0.769005  0.586508  0.633196  related  0.122328\n",
      "166  0.844826  0.844826  0.844826  0.844826  related  0.000000\n",
      "167  0.754972  0.754972  0.754972  0.754972  related  0.000000\n",
      "168  0.800856  0.800856  0.800856  0.800856  related  0.000000\n",
      "169  0.757930  0.757930  0.757930  0.757930  related  0.000000\n",
      "170  0.849237  0.852606  0.788579  0.789431  related  0.059805\n",
      "171  0.795522  0.778931  0.788609  0.785828  related  0.009694\n",
      "172  0.796457  0.798917  0.796457  0.798917  related -0.002460\n",
      "173  0.833012  0.859590  0.786274  0.801076  related  0.031936\n",
      "174  0.892075  0.880807  0.849088  0.859955  related  0.032120\n",
      "175  0.837228  0.869826  0.818349  0.847809  related -0.010580\n",
      "176  0.786219  0.702527  0.630650  0.655165  related  0.131054\n",
      "177  0.702740  0.652539  0.721840  0.721840  related -0.019100\n",
      "178  0.859725  0.861242  0.859725  0.861242  related -0.001517\n",
      "179  0.827281  0.829019  0.765038  0.824977  related  0.002304\n",
      "180  0.722676  0.756885  0.722676  0.756885  related -0.034209\n",
      "181  0.875772  0.831771  0.821168  0.836863  related  0.038909\n",
      "182  0.748395  0.737876  0.748395  0.737876  related  0.010519\n",
      "183  0.835880  0.789821  0.618648  0.641145  related  0.194736\n",
      "184  0.816532  0.778508  0.798957  0.782895  related  0.033637\n",
      "185  0.782476  0.728389  0.712482  0.754694  related  0.027783\n",
      "186  0.796275  0.749052  0.694955  0.734187  related  0.062089\n",
      "187  0.900329  0.763152  0.827544  0.766116  related  0.134213\n",
      "188  0.830503  0.805639  0.768496  0.775567  related  0.054935\n",
      "189  0.745421  0.872668  0.745421  0.872668  related -0.127247\n",
      "190  0.768703  0.776621  0.768703  0.776621  related -0.007918\n",
      "191  0.783307  0.783307  0.783307  0.783307  related  0.000000\n",
      "192  0.820672  0.820672  0.820672  0.820672  related  0.000000\n",
      "193  0.753862  0.753862  0.753862  0.753862  related  0.000000\n",
      "194  0.817292  0.784811  0.665593  0.664197  related  0.153095\n",
      "195  0.825581  0.804776  0.783707  0.806304  related  0.019277\n",
      "196  0.781447  0.770664  0.781447  0.770664  related  0.010783\n",
      "197  0.828230  0.805512  0.729914  0.748851  related  0.079379\n",
      "198  0.759620  0.745343  0.706139  0.716369  related  0.043251\n",
      "199  0.730918  0.735744  0.730918  0.735744  related -0.004827\n",
      "200  0.781495  0.781003  0.781495  0.781003  related  0.000493\n",
      "201  0.737551  0.745635  0.576517  0.589319  related  0.148232\n",
      "202  0.879840  0.872835  0.808901  0.814799  related  0.065042\n",
      "203  0.656797  0.656797  0.612179  0.612179  related  0.044618\n",
      "204  0.979161  0.967397  0.968322  0.978897  related  0.000264\n",
      "205  0.781267  0.787094  0.744059  0.775325  related  0.005942\n",
      "206  0.758568  0.769386  0.710338  0.724494  related  0.034074\n",
      "207  0.732937  0.733648  0.664784  0.682167  related  0.050770\n",
      "208  0.732553  0.735853  0.732553  0.735853  related -0.003300\n",
      "209  0.774822  0.774822  0.774822  0.774822  related  0.000000\n",
      "210  0.753324  0.753324  0.753324  0.753324  related  0.000000\n",
      "211  0.792019  0.771936  0.764104  0.792401  related -0.000382\n",
      "212  0.754740  0.756823  0.754740  0.756823  related -0.002083\n",
      "213  0.745380  0.738380  0.693067  0.686062  related  0.059318\n",
      "214  0.720127  0.720127  0.662252  0.664213  related  0.055914\n",
      "215  0.705266  0.683856  0.660539  0.685548  related  0.019718\n",
      "216  0.730444  0.761637  0.616670  0.647979  related  0.082464\n",
      "217  0.703479  0.696816  0.568801  0.621780  related  0.081699\n",
      "218  0.830249  0.706994  0.699198  0.670837  related  0.159412\n",
      "219  0.762114  0.754105  0.741785  0.760423  related  0.001690\n",
      "220  0.804359  0.679558  0.765518  0.721685  related  0.082673\n",
      "221  0.805512  0.806285  0.805512  0.806285  related -0.000773\n",
      "222  0.823239  0.823239  0.807560  0.807560  related  0.015679\n",
      "223  0.876433  0.873676  0.864707  0.870480  related  0.005953\n",
      "224  0.787643  0.781032  0.808448  0.811458  related -0.023815\n",
      "225  0.742461  0.756052  0.742461  0.756052  related -0.013591\n",
      "226  0.729412  0.690499  0.667609  0.667609  related  0.061803\n",
      "227  0.695349  0.676573  0.605341  0.605341  related  0.090008\n",
      "228  0.810317  0.784370  0.713672  0.730889  related  0.079428\n",
      "229  0.762211  0.733107  0.748999  0.723477  related  0.038734\n",
      "230  0.700647  0.672447  0.679684  0.679684  related  0.020963\n",
      "231  0.780638  0.737921  0.672125  0.656507  related  0.124131\n",
      "232  0.679570  0.686450  0.693741  0.705112  related -0.025542\n",
      "233  0.796910  0.800250  0.706641  0.713744  related  0.083166\n",
      "234  0.813659  0.806786  0.780097  0.786867  related  0.026793\n",
      "235  0.785423  0.785423  0.778371  0.778371  related  0.007052\n",
      "236  0.749204  0.749204  0.749204  0.749204  related  0.000000\n",
      "237  0.814465  0.791717  0.805314  0.807880  related  0.006585\n",
      "238  0.821925  0.829670  0.821925  0.829670  related -0.007745\n",
      "239  0.825297  0.769928  0.771233  0.771233  related  0.054064\n",
      "240  0.820271  0.779177  0.783920  0.802950  related  0.017321\n",
      "241  0.771650  0.735604  0.683497  0.683497  related  0.088153\n",
      "242  0.779872  0.787542  0.786969  0.809937  related -0.030065\n",
      "243  0.868716  0.840348  0.817846  0.822032  related  0.046684\n",
      "244  0.779568  0.783408  0.773553  0.771229  related  0.008339\n",
      "245  0.747210  0.726475  0.669844  0.670334  related  0.076876\n",
      "246  0.791111  0.755466  0.762191  0.751483  related  0.039628\n",
      "247  0.830749  0.840460  0.830749  0.840460  related -0.009711\n",
      "248  0.865467  0.842942  0.807162  0.821093  related  0.044374\n",
      "249  0.855338  0.813772  0.801708  0.815885  related  0.039453\n",
      "250  0.805571  0.790937  0.765256  0.782131  related  0.023440\n",
      "251  0.866320  0.866320  0.866320  0.866320  related  0.000000\n",
      "252  0.789420  0.770336  0.789420  0.770336  related  0.019084\n",
      "253  0.774247  0.791196  0.771094  0.787350  related -0.013103\n",
      "254  0.776027  0.789628  0.745847  0.756386  related  0.019641\n",
      "255  0.735246  0.710113  0.700069  0.705116  related  0.030129\n",
      "256  0.949402  0.806231  0.830228  0.926686  related  0.022716\n",
      "257  0.822520  0.835936  0.822520  0.835936  related -0.013416\n",
      "258  0.810799  0.820932  0.810799  0.820932  related -0.010133\n",
      "259  0.836272  0.817041  0.802200  0.805284  related  0.030988\n",
      "260  0.864725  0.864725  0.833369  0.833369  related  0.031356\n",
      "261  0.838627  0.827179  0.791618  0.803275  related  0.035352\n",
      "262  0.785163  0.785163  0.785163  0.785163  related  0.000000\n",
      "263  0.745000  0.752681  0.738201  0.742858  related  0.002142\n",
      "264  0.749123  0.741561  0.739305  0.741591  related  0.007532\n",
      "265  0.763384  0.751075  0.711306  0.736858  related  0.026526\n",
      "266  0.798026  0.833404  0.771865  0.809345  related -0.011319\n",
      "267  0.792739  0.814888  0.776009  0.776009  related  0.016730\n",
      "268  0.843707  0.841711  0.833018  0.837477  related  0.006230\n",
      "269  0.844114  0.844114  0.821533  0.821533  related  0.022582\n",
      "270  0.880439  0.880439  0.890412  0.890412  related -0.009973\n",
      "271  0.786614  0.786614  0.786614  0.786614  related  0.000000\n",
      "272  0.849519  0.866782  0.849519  0.866782  related -0.017263\n",
      "273  0.841017  0.842919  0.813528  0.820875  related  0.020141\n",
      "274  0.704618  0.701306  0.510401  0.503063  related  0.201555\n",
      "275  0.898319  0.851213  0.792280  0.866430  related  0.031889\n",
      "276  0.896458  0.902649  0.893407  0.903751  related -0.007293\n",
      "277  0.805223  0.806631  0.696130  0.756050  related  0.049173\n",
      "278  0.758288  0.792977  0.716353  0.783036  related -0.024749\n",
      "279  0.810799  0.806825  0.784793  0.788518  related  0.022281\n",
      "280  0.893817  0.894265  0.884340  0.893523  related  0.000294\n",
      "281  0.863304  0.850131  0.852402  0.855831  related  0.007473\n",
      "282  0.752252  0.737582  0.735606  0.735606  related  0.016646\n",
      "283  0.828672  0.826759  0.777351  0.788103  related  0.040570\n",
      "284  0.838375  0.821350  0.780184  0.791009  related  0.047367\n",
      "285  0.830797  0.849542  0.807681  0.845307  related -0.014510\n",
      "286  0.871748  0.871748  0.871748  0.871748  related  0.000000\n",
      "287  0.695160  0.695160  0.628002  0.616910  related  0.078251\n",
      "288  0.822740  0.785750  0.798869  0.798869  related  0.023871\n",
      "289  0.825170  0.845769  0.825170  0.845769  related -0.020599\n",
      "290  0.720330  0.720330  0.720330  0.720330  related  0.000000\n",
      "291  0.796154  0.797231  0.765032  0.778562  related  0.017591\n",
      "292  0.695945  0.698312  0.611498  0.627823  related  0.068122\n",
      "293  0.799175  0.799618  0.799175  0.799618  related -0.000443\n",
      "294  0.857324  0.857324  0.857324  0.857324  related  0.000000\n",
      "295  0.853488  0.853488  0.858128  0.858128  related -0.004640\n",
      "296  0.886341  0.901035  0.886341  0.901035  related -0.014694\n",
      "297  0.883537  0.883671  0.883537  0.883671  related -0.000134\n",
      "298  0.828090  0.826629  0.828090  0.826629  related  0.001461\n",
      "299  0.795423  0.788032  0.728231  0.730955  related  0.064468\n",
      "363  0.915655  0.920901  0.915655  0.920901  related -0.005246\n",
      "375  0.851319  0.875597  0.845114  0.863322  related -0.012003\n",
      "386  0.775219  0.774787  0.717746  0.737626  related  0.037593\n",
      "402  0.749054  0.765888  0.761890  0.787457  related -0.038403\n",
      "410  0.836785  0.836785  0.836785  0.836785  related  0.000000\n",
      "455  0.795162  0.784621  0.795162  0.784621  related  0.010541\n",
      "462  0.828489  0.828489  0.828489  0.828489  related  0.000000\n",
      "490  0.846154  0.816261  0.788313  0.828215  related  0.017939\n",
      "499  0.745290  0.745290  0.768279  0.760754  related -0.015463\n",
      "500  0.862889  0.822836  0.748499  0.764214  related  0.098675\n",
      "505  0.778131  0.819207  0.778131  0.819207  related -0.041076\n",
      "599  0.732351  0.750338  0.681620  0.697170  related  0.035181\n",
      "614  0.769514  0.769514  0.774228  0.774228  related -0.004715\n",
      "618  0.793084  0.795935  0.788687  0.806020  related -0.012936\n",
      "631  0.730447  0.779220  0.730447  0.779220  related -0.048772\n",
      "668  0.804762  0.782333  0.806876  0.783456  related  0.021306\n",
      "679  0.816090  0.816090  0.816090  0.816090  related  0.000000\n",
      "681  0.787454  0.787454  0.787454  0.787454  related  0.000000\n",
      "756  0.874211  0.765389  0.747428  0.782437  related  0.091774\n",
      "764  0.771199  0.750356  0.752442  0.757235  related  0.013964\n",
      "894  0.808785  0.833455  0.808785  0.833455  related -0.024670\n",
      "923  0.742913  0.753903  0.757182  0.757182  related -0.014268\n"
     ]
    }
   ],
   "source": [
    "print(bert_napredak_results_max_related_sum.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sa-sa    sa-bez    bez-sa   bez-bez     Stance       sum\n",
      "0    0.627635  0.620608  0.578464  0.578464  unrelated  0.049171\n",
      "1    0.612943  0.626350  0.612943  0.626350  unrelated -0.013407\n",
      "2    0.689166  0.699177  0.659427  0.692413  unrelated -0.003247\n",
      "3    0.597808  0.634351  0.597808  0.634351  unrelated -0.036543\n",
      "4    0.660143  0.681400  0.670652  0.716698  unrelated -0.056555\n",
      "5    0.671554  0.671554  0.671554  0.671554  unrelated  0.000000\n",
      "6    0.684476  0.685156  0.687796  0.687796  unrelated -0.003320\n",
      "7    0.656540  0.656540  0.656540  0.656540  unrelated  0.000000\n",
      "8    0.650038  0.677106  0.659520  0.683190  unrelated -0.033152\n",
      "9    0.578987  0.616212  0.578987  0.616212  unrelated -0.037224\n",
      "10   0.721992  0.721992  0.721992  0.721992  unrelated  0.000000\n",
      "11   0.647815  0.667102  0.647815  0.667102  unrelated -0.019287\n",
      "12   0.562497  0.542274  0.558219  0.580671  unrelated -0.018174\n",
      "13   0.681521  0.684489  0.681521  0.684489  unrelated -0.002969\n",
      "14   0.638384  0.657983  0.638384  0.657983  unrelated -0.019599\n",
      "15   0.696646  0.696646  0.691727  0.691727  unrelated  0.004919\n",
      "16   0.601039  0.657830  0.601039  0.657830  unrelated -0.056790\n",
      "17   0.645673  0.668799  0.645673  0.668799  unrelated -0.023126\n",
      "18   0.710755  0.723399  0.700337  0.720303  unrelated -0.009547\n",
      "19   0.665383  0.665383  0.665383  0.665383  unrelated  0.000000\n",
      "20   0.635036  0.639836  0.635036  0.639836  unrelated -0.004799\n",
      "21   0.536775  0.583193  0.536775  0.583193  unrelated -0.046417\n",
      "22   0.580292  0.580292  0.580292  0.580292  unrelated  0.000000\n",
      "23   0.684295  0.684295  0.672309  0.672309  unrelated  0.011986\n",
      "25   0.600654  0.626703  0.600654  0.626703  unrelated -0.026050\n",
      "26   0.665299  0.665299  0.665299  0.665299  unrelated  0.000000\n",
      "27   0.593561  0.613033  0.593561  0.613033  unrelated -0.019471\n",
      "28   0.645278  0.680367  0.645278  0.680367  unrelated -0.035090\n",
      "29   0.763693  0.763693  0.740241  0.740241  unrelated  0.023453\n",
      "30   0.600717  0.604835  0.577521  0.579857  unrelated  0.020859\n",
      "31   0.612137  0.616911  0.612137  0.616911  unrelated -0.004774\n",
      "32   0.655110  0.650677  0.627791  0.629482  unrelated  0.025628\n",
      "33   0.623014  0.632525  0.572928  0.594028  unrelated  0.028987\n",
      "34   0.589642  0.589642  0.584066  0.593917  unrelated -0.004275\n",
      "35   0.606199  0.606199  0.606199  0.606199  unrelated  0.000000\n",
      "37   0.726615  0.722316  0.675680  0.675509  unrelated  0.051106\n",
      "38   0.632236  0.634946  0.606233  0.610797  unrelated  0.021439\n",
      "39   0.664874  0.664977  0.681380  0.677214  unrelated -0.012340\n",
      "40   0.662986  0.662986  0.660498  0.672157  unrelated -0.009171\n",
      "41   0.684993  0.686981  0.689125  0.688421  unrelated -0.003428\n",
      "42   0.630803  0.630803  0.626673  0.635285  unrelated -0.004482\n",
      "43   0.683951  0.691395  0.641321  0.642576  unrelated  0.041375\n",
      "44   0.623564  0.623564  0.624378  0.624378  unrelated -0.000814\n",
      "45   0.625736  0.626301  0.562690  0.598746  unrelated  0.026990\n",
      "46   0.572349  0.572349  0.484388  0.524447  unrelated  0.047902\n",
      "47   0.642375  0.642375  0.624248  0.636340  unrelated  0.006035\n",
      "48   0.748611  0.748611  0.748611  0.748611  unrelated  0.000000\n",
      "49   0.643123  0.696748  0.643123  0.696748  unrelated -0.053625\n",
      "50   0.627069  0.627069  0.605190  0.622886  unrelated  0.004183\n",
      "51   0.615459  0.615459  0.580888  0.606098  unrelated  0.009362\n",
      "52   0.640281  0.641930  0.657580  0.656231  unrelated -0.015950\n",
      "53   0.693921  0.704071  0.637089  0.657565  unrelated  0.036356\n",
      "54   0.642375  0.642375  0.624248  0.636340  unrelated  0.006035\n",
      "55   0.677692  0.677692  0.673714  0.673714  unrelated  0.003977\n",
      "56   0.658049  0.664131  0.658049  0.664131  unrelated -0.006082\n",
      "57   0.671153  0.678152  0.688950  0.673355  unrelated -0.002202\n",
      "58   0.722377  0.722377  0.706512  0.706512  unrelated  0.015865\n",
      "59   0.589418  0.592191  0.568337  0.565291  unrelated  0.024127\n",
      "60   0.672533  0.672533  0.667628  0.667628  unrelated  0.004905\n",
      "61   0.564240  0.564240  0.570695  0.585740  unrelated -0.021500\n",
      "62   0.653863  0.646655  0.653863  0.646655  unrelated  0.007208\n",
      "63   0.583905  0.583905  0.519965  0.506514  unrelated  0.077390\n",
      "64   0.677338  0.677338  0.669592  0.679043  unrelated -0.001704\n",
      "65   0.679861  0.679861  0.662073  0.662073  unrelated  0.017788\n",
      "66   0.608919  0.609860  0.596894  0.596041  unrelated  0.012878\n",
      "67   0.638026  0.639303  0.624934  0.642430  unrelated -0.004404\n",
      "68   0.668815  0.668815  0.668815  0.668815  unrelated  0.000000\n",
      "70   0.559601  0.574455  0.559601  0.574455  unrelated -0.014855\n",
      "71   0.593263  0.598314  0.592926  0.605774  unrelated -0.012512\n",
      "72   0.697325  0.691110  0.697325  0.691110  unrelated  0.006215\n",
      "73   0.703358  0.703358  0.673026  0.673026  unrelated  0.030332\n",
      "74   0.742252  0.742252  0.741509  0.741509  unrelated  0.000743\n",
      "75   0.582257  0.582257  0.458734  0.495011  unrelated  0.087246\n",
      "76   0.645405  0.654222  0.561128  0.588138  unrelated  0.057267\n",
      "77   0.671452  0.671366  0.671452  0.671366  unrelated  0.000086\n",
      "78   0.581305  0.583796  0.564468  0.607154  unrelated -0.025849\n",
      "79   0.622584  0.621944  0.610847  0.605523  unrelated  0.017061\n",
      "80   0.697785  0.697785  0.707608  0.707608  unrelated -0.009823\n",
      "81   0.713134  0.707394  0.712381  0.708035  unrelated  0.005100\n",
      "82   0.643327  0.641898  0.565492  0.589989  unrelated  0.053338\n",
      "83   0.722954  0.722954  0.722954  0.722954  unrelated  0.000000\n",
      "84   0.561182  0.559665  0.561182  0.559665  unrelated  0.001518\n",
      "85   0.653192  0.649301  0.653192  0.649301  unrelated  0.003891\n",
      "86   0.716512  0.716512  0.716512  0.716512  unrelated  0.000000\n",
      "87   0.608866  0.613694  0.579219  0.591799  unrelated  0.017068\n",
      "89   0.639910  0.629883  0.613992  0.625301  unrelated  0.014609\n",
      "90   0.627468  0.627468  0.590124  0.621438  unrelated  0.006030\n",
      "91   0.643682  0.643682  0.630032  0.630032  unrelated  0.013649\n",
      "92   0.626658  0.626658  0.597667  0.582147  unrelated  0.044511\n",
      "94   0.655953  0.645175  0.571008  0.571008  unrelated  0.084945\n",
      "95   0.675071  0.686621  0.675071  0.686621  unrelated -0.011550\n",
      "96   0.637326  0.637326  0.619302  0.619302  unrelated  0.018024\n",
      "97   0.740977  0.756137  0.740977  0.756137  unrelated -0.015159\n",
      "98   0.717242  0.717242  0.640419  0.640419  unrelated  0.076822\n",
      "99   0.701463  0.700221  0.701463  0.700221  unrelated  0.001242\n",
      "100  0.736869  0.724544  0.660224  0.655476  unrelated  0.081393\n",
      "101  0.663014  0.675883  0.663014  0.675883  unrelated -0.012869\n",
      "102  0.594601  0.610181  0.595115  0.605834  unrelated -0.011234\n",
      "103  0.745804  0.752312  0.749789  0.744207  unrelated  0.001597\n",
      "104  0.746786  0.746786  0.721957  0.721957  unrelated  0.024829\n",
      "105  0.753466  0.743482  0.761617  0.763103  unrelated -0.009637\n",
      "106  0.592566  0.573898  0.558355  0.535731  unrelated  0.056835\n",
      "108  0.730530  0.730530  0.730530  0.730530  unrelated  0.000000\n",
      "109  0.625268  0.642781  0.596053  0.634234  unrelated -0.008967\n",
      "110  0.672278  0.660519  0.663269  0.652998  unrelated  0.019280\n",
      "111  0.664960  0.686107  0.664960  0.686107  unrelated -0.021147\n",
      "112  0.569807  0.558259  0.587971  0.578023  unrelated -0.008217\n",
      "113  0.606132  0.613766  0.569587  0.569587  unrelated  0.036545\n",
      "114  0.689354  0.687092  0.689354  0.687092  unrelated  0.002262\n",
      "115  0.795346  0.797221  0.754971  0.782410  unrelated  0.012937\n",
      "116  0.602190  0.589741  0.594034  0.594034  unrelated  0.008156\n",
      "118  0.726954  0.718537  0.667354  0.691529  unrelated  0.035425\n",
      "119  0.636098  0.648940  0.634973  0.637245  unrelated -0.001147\n",
      "120  0.772146  0.762656  0.754417  0.761285  unrelated  0.010861\n",
      "121  0.645943  0.649832  0.588726  0.588726  unrelated  0.057217\n",
      "122  0.780072  0.780072  0.719540  0.742982  unrelated  0.037090\n",
      "123  0.709051  0.702575  0.709051  0.702575  unrelated  0.006477\n",
      "124  0.718984  0.718984  0.718984  0.718984  unrelated  0.000000\n",
      "125  0.777903  0.779550  0.767798  0.771226  unrelated  0.006678\n",
      "126  0.646637  0.648711  0.616364  0.665732  unrelated -0.019095\n",
      "127  0.738965  0.725283  0.674277  0.670865  unrelated  0.068100\n",
      "128  0.730630  0.729391  0.685717  0.694920  unrelated  0.035710\n",
      "129  0.621115  0.621115  0.621115  0.621115  unrelated  0.000000\n",
      "130  0.706944  0.704412  0.697248  0.716360  unrelated -0.009417\n",
      "131  0.743400  0.743400  0.743400  0.743400  unrelated  0.000000\n",
      "132  0.695459  0.691970  0.695459  0.691970  unrelated  0.003489\n",
      "133  0.719595  0.709450  0.612699  0.620798  unrelated  0.098797\n",
      "134  0.667049  0.661965  0.667240  0.659997  unrelated  0.007052\n",
      "135  0.667544  0.667544  0.667544  0.667544  unrelated  0.000000\n",
      "136  0.631737  0.647394  0.631737  0.647394  unrelated -0.015657\n",
      "137  0.749276  0.758892  0.749276  0.758892  unrelated -0.009616\n",
      "138  0.606712  0.645703  0.606712  0.645703  unrelated -0.038991\n",
      "139  0.775284  0.776427  0.775284  0.776427  unrelated -0.001143\n",
      "140  0.776043  0.785746  0.748035  0.753043  unrelated  0.023001\n",
      "141  0.634797  0.644578  0.627512  0.627512  unrelated  0.007285\n",
      "142  0.717038  0.699837  0.674458  0.665619  unrelated  0.051419\n",
      "143  0.622291  0.687783  0.622291  0.687783  unrelated -0.065492\n",
      "144  0.791525  0.782495  0.736540  0.756642  unrelated  0.034883\n",
      "145  0.722378  0.707838  0.630863  0.654856  unrelated  0.067521\n",
      "146  0.680985  0.682005  0.675452  0.696623  unrelated -0.015638\n",
      "147  0.630627  0.630627  0.621449  0.621449  unrelated  0.009178\n",
      "148  0.554505  0.554505  0.554505  0.554505  unrelated  0.000000\n",
      "149  0.755312  0.751267  0.736849  0.743656  unrelated  0.011656\n",
      "150  0.711950  0.709222  0.690736  0.698113  unrelated  0.013837\n",
      "151  0.706980  0.706980  0.688798  0.688798  unrelated  0.018182\n",
      "152  0.542386  0.554275  0.529717  0.548149  unrelated -0.005763\n",
      "153  0.765605  0.767127  0.720217  0.736662  unrelated  0.028943\n",
      "154  0.629980  0.629980  0.629980  0.629980  unrelated  0.000000\n",
      "155  0.741361  0.741361  0.677043  0.677043  unrelated  0.064318\n",
      "156  0.736466  0.737106  0.736466  0.737106  unrelated -0.000640\n",
      "157  0.685734  0.675640  0.672355  0.677706  unrelated  0.008028\n",
      "158  0.634912  0.634912  0.634912  0.634912  unrelated  0.000000\n",
      "159  0.621115  0.621115  0.621115  0.621115  unrelated  0.000000\n",
      "160  0.646130  0.649807  0.646130  0.649807  unrelated -0.003677\n",
      "161  0.702016  0.720170  0.702016  0.720170  unrelated -0.018154\n",
      "162  0.691151  0.682568  0.632478  0.647637  unrelated  0.043513\n",
      "163  0.736362  0.746880  0.736362  0.746880  unrelated -0.010519\n",
      "164  0.707160  0.721075  0.646229  0.688551  unrelated  0.018609\n",
      "165  0.705302  0.691645  0.715406  0.709269  unrelated -0.003967\n",
      "166  0.726336  0.708492  0.723069  0.714644  unrelated  0.011692\n",
      "167  0.718135  0.738625  0.718135  0.738625  unrelated -0.020490\n",
      "168  0.761936  0.758138  0.744011  0.744011  unrelated  0.017925\n",
      "169  0.637136  0.647419  0.637136  0.647419  unrelated -0.010283\n",
      "170  0.699410  0.684604  0.699410  0.684604  unrelated  0.014807\n",
      "171  0.716185  0.708079  0.716185  0.708079  unrelated  0.008106\n",
      "172  0.587848  0.607242  0.565717  0.594015  unrelated -0.006166\n",
      "173  0.719176  0.710298  0.698461  0.704104  unrelated  0.015071\n",
      "174  0.529667  0.539227  0.599997  0.652256  unrelated -0.122589\n",
      "175  0.677082  0.677082  0.655158  0.655158  unrelated  0.021923\n",
      "176  0.734993  0.733591  0.734993  0.733591  unrelated  0.001402\n",
      "177  0.619475  0.619475  0.619475  0.619475  unrelated  0.000000\n",
      "178  0.714266  0.706205  0.698997  0.709223  unrelated  0.005043\n",
      "179  0.735305  0.728322  0.735305  0.728322  unrelated  0.006983\n",
      "180  0.663127  0.663127  0.663127  0.663127  unrelated  0.000000\n",
      "181  0.663567  0.669597  0.643546  0.651134  unrelated  0.012434\n",
      "182  0.811968  0.817353  0.811968  0.817353  unrelated -0.005385\n",
      "183  0.667236  0.667236  0.667236  0.667236  unrelated  0.000000\n",
      "184  0.704580  0.746857  0.673219  0.720730  unrelated -0.016151\n",
      "185  0.621246  0.625359  0.621246  0.625359  unrelated -0.004113\n",
      "186  0.737242  0.737242  0.720658  0.720658  unrelated  0.016585\n",
      "187  0.802886  0.804385  0.802886  0.804385  unrelated -0.001499\n",
      "188  0.712008  0.733647  0.639677  0.650142  unrelated  0.061866\n",
      "189  0.712681  0.712681  0.712681  0.712681  unrelated  0.000000\n",
      "190  0.661279  0.645003  0.572525  0.587561  unrelated  0.073718\n",
      "191  0.645529  0.643962  0.576985  0.597175  unrelated  0.048354\n",
      "192  0.709253  0.709253  0.709253  0.709253  unrelated  0.000000\n",
      "193  0.715119  0.701475  0.642421  0.642421  unrelated  0.072697\n",
      "194  0.749482  0.744161  0.746671  0.743461  unrelated  0.006021\n",
      "195  0.735518  0.729806  0.714999  0.718605  unrelated  0.016913\n",
      "196  0.735626  0.730118  0.687323  0.689871  unrelated  0.045756\n",
      "197  0.696153  0.708420  0.698491  0.720015  unrelated -0.023862\n",
      "198  0.750219  0.743261  0.732365  0.741556  unrelated  0.008663\n",
      "200  0.740454  0.730902  0.717800  0.707173  unrelated  0.033281\n",
      "201  0.638090  0.650554  0.761198  0.761198  unrelated -0.123108\n",
      "202  0.748357  0.748357  0.686846  0.673271  unrelated  0.075086\n",
      "203  0.715704  0.715704  0.732869  0.732869  unrelated -0.017164\n",
      "204  0.718331  0.713182  0.689426  0.689426  unrelated  0.028905\n",
      "205  0.667448  0.667448  0.696625  0.696625  unrelated -0.029177\n",
      "206  0.696853  0.681679  0.702026  0.688188  unrelated  0.008664\n",
      "207  0.725197  0.714594  0.717957  0.705440  unrelated  0.019758\n",
      "208  0.730400  0.724474  0.732504  0.728357  unrelated  0.002043\n",
      "209  0.744617  0.727654  0.735143  0.724620  unrelated  0.019997\n",
      "210  0.726600  0.731074  0.726600  0.731074  unrelated -0.004474\n",
      "211  0.686816  0.692720  0.686816  0.692720  unrelated -0.005903\n",
      "212  0.747795  0.731205  0.688489  0.681118  unrelated  0.066677\n",
      "213  0.743745  0.743745  0.708157  0.697137  unrelated  0.046608\n",
      "214  0.687632  0.687632  0.687632  0.687632  unrelated  0.000000\n",
      "215  0.726600  0.731074  0.726600  0.731074  unrelated -0.004474\n",
      "216  0.740547  0.709872  0.719097  0.698380  unrelated  0.042167\n",
      "217  0.654358  0.648069  0.653923  0.658760  unrelated -0.004402\n",
      "218  0.718331  0.713182  0.689426  0.689426  unrelated  0.028905\n",
      "219  0.696769  0.677497  0.682873  0.682873  unrelated  0.013896\n",
      "220  0.712744  0.712744  0.712744  0.712744  unrelated  0.000000\n",
      "221  0.788011  0.774394  0.788011  0.774394  unrelated  0.013617\n",
      "222  0.771874  0.771874  0.771874  0.771874  unrelated  0.000000\n",
      "223  0.815321  0.804177  0.811648  0.804050  unrelated  0.011271\n",
      "224  0.755909  0.763118  0.754341  0.766516  unrelated -0.010607\n",
      "225  0.716537  0.707894  0.658854  0.648287  unrelated  0.068249\n",
      "226  0.613669  0.613669  0.617241  0.617241  unrelated -0.003572\n",
      "227  0.714297  0.714297  0.706629  0.683968  unrelated  0.030330\n",
      "228  0.781705  0.781705  0.781705  0.781705  unrelated  0.000000\n",
      "229  0.710261  0.682631  0.662185  0.683784  unrelated  0.026477\n",
      "230  0.728207  0.711747  0.742886  0.736629  unrelated -0.008422\n",
      "231  0.780032  0.769839  0.714385  0.733227  unrelated  0.046804\n",
      "232  0.718516  0.709316  0.673812  0.668600  unrelated  0.049916\n",
      "233  0.753157  0.742096  0.740103  0.724498  unrelated  0.028659\n",
      "234  0.752095  0.752095  0.749957  0.749957  unrelated  0.002137\n",
      "235  0.648850  0.653736  0.665188  0.668634  unrelated -0.019783\n",
      "236  0.724198  0.706422  0.688486  0.689711  unrelated  0.034488\n",
      "237  0.780062  0.785761  0.780062  0.785761  unrelated -0.005699\n",
      "238  0.770588  0.775105  0.748465  0.752828  unrelated  0.017759\n",
      "239  0.721955  0.725439  0.721955  0.725439  unrelated -0.003483\n",
      "240  0.748924  0.748924  0.751743  0.751743  unrelated -0.002819\n",
      "243  0.719125  0.719125  0.719125  0.719125  unrelated  0.000000\n",
      "244  0.720375  0.723680  0.712010  0.712010  unrelated  0.008365\n",
      "245  0.578201  0.575236  0.578201  0.575236  unrelated  0.002965\n",
      "246  0.700113  0.704043  0.700113  0.704043  unrelated -0.003929\n",
      "247  0.640977  0.639978  0.634032  0.633393  unrelated  0.007584\n",
      "248  0.645431  0.633966  0.686663  0.686663  unrelated -0.041232\n",
      "249  0.664082  0.664082  0.629552  0.629552  unrelated  0.034531\n",
      "250  0.599572  0.599572  0.592011  0.592011  unrelated  0.007561\n",
      "251  0.648638  0.648638  0.648638  0.648638  unrelated  0.000000\n",
      "252  0.547424  0.550036  0.547424  0.550036  unrelated -0.002612\n",
      "253  0.626229  0.635526  0.610148  0.617415  unrelated  0.008814\n",
      "254  0.632571  0.632571  0.623970  0.626756  unrelated  0.005815\n",
      "255  0.738537  0.745804  0.732328  0.751831  unrelated -0.013294\n",
      "256  0.724164  0.724164  0.724164  0.724164  unrelated  0.000000\n",
      "257  0.639381  0.639381  0.643289  0.643289  unrelated -0.003907\n",
      "258  0.594152  0.598257  0.578204  0.594234  unrelated -0.000081\n",
      "259  0.692948  0.692948  0.690172  0.690172  unrelated  0.002776\n",
      "260  0.642924  0.643583  0.579242  0.580334  unrelated  0.062590\n",
      "261  0.547594  0.547594  0.577306  0.577306  unrelated -0.029712\n",
      "263  0.767419  0.770516  0.767419  0.770516  unrelated -0.003097\n",
      "264  0.543429  0.541059  0.542147  0.542147  unrelated  0.001282\n",
      "265  0.566963  0.566963  0.587174  0.587174  unrelated -0.020211\n",
      "266  0.693431  0.669155  0.642658  0.644558  unrelated  0.048874\n",
      "267  0.636996  0.636996  0.659461  0.659461  unrelated -0.022465\n",
      "268  0.777380  0.777380  0.777380  0.777380  unrelated  0.000000\n",
      "269  0.599572  0.599572  0.592011  0.592011  unrelated  0.007561\n",
      "270  0.608514  0.608514  0.645139  0.626649  unrelated -0.018136\n",
      "271  0.629453  0.629453  0.629453  0.629453  unrelated  0.000000\n",
      "273  0.620853  0.620853  0.601620  0.601620  unrelated  0.019233\n",
      "274  0.645056  0.672158  0.645056  0.672158  unrelated -0.027102\n",
      "275  0.547594  0.547594  0.577306  0.577306  unrelated -0.029712\n",
      "276  0.565512  0.565512  0.597621  0.597621  unrelated -0.032109\n",
      "277  0.583839  0.590720  0.610493  0.612101  unrelated -0.028262\n",
      "278  0.642044  0.642615  0.628246  0.640695  unrelated  0.001349\n",
      "279  0.602582  0.606173  0.595055  0.595055  unrelated  0.007527\n",
      "280  0.697682  0.697682  0.647768  0.647768  unrelated  0.049915\n",
      "281  0.594959  0.573259  0.568625  0.568625  unrelated  0.026335\n",
      "282  0.730761  0.730761  0.730761  0.730761  unrelated  0.000000\n",
      "283  0.626108  0.626108  0.619000  0.637125  unrelated -0.011017\n",
      "284  0.604859  0.604859  0.610828  0.618062  unrelated -0.013204\n",
      "285  0.573338  0.573338  0.543062  0.541371  unrelated  0.031967\n",
      "286  0.601569  0.612697  0.620061  0.622237  unrelated -0.020668\n",
      "287  0.697150  0.697150  0.698809  0.698809  unrelated -0.001659\n",
      "288  0.685452  0.680450  0.659664  0.663675  unrelated  0.021777\n",
      "289  0.571219  0.545104  0.539184  0.524131  unrelated  0.047088\n",
      "290  0.677873  0.685721  0.677873  0.685721  unrelated -0.007848\n",
      "291  0.656736  0.654090  0.656736  0.654090  unrelated  0.002646\n",
      "292  0.654168  0.638662  0.632173  0.611846  unrelated  0.042322\n",
      "293  0.672727  0.672727  0.667751  0.667751  unrelated  0.004976\n",
      "294  0.571288  0.571288  0.539690  0.539690  unrelated  0.031598\n",
      "295  0.702586  0.691289  0.682956  0.679599  unrelated  0.022986\n",
      "297  0.618810  0.620429  0.618810  0.620429  unrelated -0.001620\n",
      "298  0.675738  0.690600  0.621943  0.608267  unrelated  0.067471\n",
      "299  0.699629  0.699629  0.681803  0.681803  unrelated  0.017826\n",
      "300  0.682036  0.682036  0.682036  0.682036  unrelated  0.000000\n",
      "301  0.667219  0.666335  0.633684  0.636078  unrelated  0.031141\n",
      "302  0.773869  0.754126  0.754763  0.729186  unrelated  0.044683\n",
      "303  0.701865  0.701865  0.701865  0.701865  unrelated  0.000000\n",
      "304  0.682769  0.681718  0.683158  0.676261  unrelated  0.006508\n",
      "305  0.688758  0.680736  0.688758  0.680736  unrelated  0.008021\n",
      "306  0.586317  0.586317  0.524755  0.524755  unrelated  0.061562\n",
      "307  0.633709  0.633709  0.498330  0.498330  unrelated  0.135379\n",
      "308  0.690360  0.690360  0.688384  0.692024  unrelated -0.001664\n",
      "309  0.681424  0.678817  0.681424  0.678817  unrelated  0.002607\n",
      "310  0.651700  0.631379  0.655660  0.639801  unrelated  0.011899\n",
      "311  0.701335  0.701335  0.693895  0.693895  unrelated  0.007440\n",
      "312  0.664805  0.665726  0.616965  0.616965  unrelated  0.047840\n",
      "313  0.690128  0.690128  0.688708  0.688708  unrelated  0.001420\n",
      "314  0.661204  0.661204  0.628656  0.628656  unrelated  0.032548\n",
      "315  0.717653  0.717653  0.717653  0.717653  unrelated  0.000000\n",
      "316  0.632142  0.627648  0.613882  0.590994  unrelated  0.041148\n",
      "317  0.647909  0.647909  0.591565  0.591565  unrelated  0.056345\n",
      "318  0.657292  0.657292  0.657292  0.657292  unrelated  0.000000\n",
      "319  0.715606  0.700627  0.707898  0.686035  unrelated  0.029571\n",
      "320  0.697198  0.697985  0.670210  0.670210  unrelated  0.026988\n",
      "321  0.720531  0.687558  0.720531  0.687558  unrelated  0.032972\n",
      "322  0.633953  0.627592  0.633953  0.627592  unrelated  0.006361\n",
      "323  0.665972  0.664215  0.618617  0.617694  unrelated  0.048278\n",
      "324  0.692376  0.693896  0.675303  0.674290  unrelated  0.018086\n",
      "325  0.619591  0.619591  0.627138  0.628019  unrelated -0.008428\n",
      "326  0.653813  0.653813  0.626632  0.631051  unrelated  0.022762\n",
      "327  0.715118  0.714006  0.715118  0.714006  unrelated  0.001112\n",
      "328  0.628416  0.628416  0.590035  0.588176  unrelated  0.040240\n",
      "329  0.636901  0.635823  0.630885  0.628123  unrelated  0.008778\n",
      "330  0.651159  0.652832  0.651159  0.652832  unrelated -0.001673\n",
      "331  0.740801  0.699731  0.735883  0.701860  unrelated  0.038940\n",
      "332  0.662743  0.662743  0.618519  0.618519  unrelated  0.044223\n",
      "333  0.659364  0.659364  0.659364  0.659364  unrelated  0.000000\n",
      "334  0.622499  0.622499  0.622499  0.622499  unrelated  0.000000\n",
      "335  0.580242  0.580242  0.617629  0.602386  unrelated -0.022144\n",
      "336  0.653643  0.661799  0.653643  0.661799  unrelated -0.008155\n",
      "337  0.676960  0.676960  0.691148  0.691148  unrelated -0.014188\n",
      "338  0.724252  0.726456  0.724252  0.726456  unrelated -0.002203\n",
      "339  0.696397  0.671900  0.705235  0.685699  unrelated  0.010699\n",
      "340  0.694246  0.694460  0.667995  0.662867  unrelated  0.031379\n",
      "341  0.679705  0.640352  0.679705  0.640352  unrelated  0.039353\n",
      "342  0.664583  0.664287  0.647603  0.650012  unrelated  0.014570\n",
      "343  0.608451  0.608451  0.586975  0.586975  unrelated  0.021477\n",
      "344  0.676436  0.676436  0.652243  0.651231  unrelated  0.025206\n",
      "345  0.634157  0.634157  0.634157  0.634157  unrelated  0.000000\n",
      "346  0.635801  0.638559  0.647152  0.648573  unrelated -0.012772\n",
      "347  0.714575  0.714575  0.712173  0.712386  unrelated  0.002189\n",
      "348  0.699241  0.688761  0.670419  0.660785  unrelated  0.038457\n",
      "349  0.688811  0.682738  0.688811  0.682738  unrelated  0.006072\n",
      "350  0.702538  0.702538  0.660768  0.660768  unrelated  0.041770\n",
      "351  0.641232  0.649477  0.641232  0.649477  unrelated -0.008244\n",
      "352  0.627174  0.627174  0.632568  0.629320  unrelated -0.002146\n",
      "353  0.594735  0.594735  0.594735  0.594735  unrelated  0.000000\n",
      "354  0.713641  0.711885  0.703106  0.699337  unrelated  0.014304\n",
      "355  0.592246  0.592246  0.522814  0.522814  unrelated  0.069432\n",
      "356  0.622255  0.622255  0.593769  0.596032  unrelated  0.026222\n",
      "357  0.616653  0.616653  0.515427  0.515427  unrelated  0.101226\n",
      "358  0.636645  0.634054  0.636645  0.634054  unrelated  0.002591\n",
      "359  0.595461  0.595461  0.595461  0.595461  unrelated  0.000000\n",
      "360  0.699851  0.689962  0.606994  0.613625  unrelated  0.086227\n",
      "361  0.658081  0.659015  0.653111  0.657560  unrelated  0.000521\n",
      "362  0.641277  0.639502  0.568629  0.580012  unrelated  0.061265\n",
      "364  0.746607  0.739940  0.728604  0.725624  unrelated  0.020983\n",
      "365  0.663430  0.685208  0.654082  0.669905  unrelated -0.006475\n",
      "366  0.754779  0.738035  0.755985  0.741640  unrelated  0.013138\n",
      "367  0.622678  0.626276  0.607835  0.616681  unrelated  0.005997\n",
      "368  0.723877  0.712493  0.650499  0.663135  unrelated  0.060742\n",
      "369  0.744090  0.732952  0.734417  0.730627  unrelated  0.013463\n",
      "370  0.692101  0.688943  0.704263  0.705298  unrelated -0.013197\n",
      "371  0.717587  0.717587  0.717587  0.717587  unrelated  0.000000\n",
      "372  0.643245  0.636261  0.621112  0.623536  unrelated  0.019709\n",
      "373  0.627175  0.651370  0.627030  0.654189  unrelated -0.027015\n",
      "374  0.716966  0.721094  0.702707  0.714220  unrelated  0.002747\n",
      "376  0.712645  0.698057  0.592461  0.592111  unrelated  0.120534\n",
      "377  0.686003  0.694722  0.656830  0.706122  unrelated -0.020118\n",
      "378  0.687599  0.694610  0.684962  0.703049  unrelated -0.015450\n",
      "379  0.632910  0.624022  0.607446  0.614212  unrelated  0.018698\n",
      "380  0.650040  0.622362  0.637954  0.621159  unrelated  0.028881\n",
      "381  0.653701  0.662269  0.627566  0.651060  unrelated  0.002641\n",
      "382  0.712616  0.704803  0.647377  0.665458  unrelated  0.047158\n",
      "383  0.652756  0.647670  0.582966  0.607681  unrelated  0.045075\n",
      "384  0.725635  0.699602  0.702162  0.695533  unrelated  0.030102\n",
      "385  0.757565  0.763408  0.748498  0.759622  unrelated -0.002057\n",
      "387  0.649698  0.643267  0.599662  0.611025  unrelated  0.038673\n",
      "388  0.691859  0.688202  0.642950  0.670600  unrelated  0.021259\n",
      "389  0.731887  0.716951  0.742220  0.755419  unrelated -0.023532\n",
      "390  0.649630  0.674645  0.649630  0.674645  unrelated -0.025015\n",
      "391  0.653219  0.647442  0.609450  0.635725  unrelated  0.017495\n",
      "392  0.679624  0.691381  0.600157  0.642082  unrelated  0.037542\n",
      "393  0.730868  0.714753  0.711551  0.707896  unrelated  0.022972\n",
      "394  0.674257  0.677159  0.640340  0.662028  unrelated  0.012229\n",
      "395  0.690377  0.666817  0.643336  0.658290  unrelated  0.032087\n",
      "396  0.718636  0.720599  0.696041  0.712405  unrelated  0.006231\n",
      "397  0.777486  0.754847  0.777486  0.754847  unrelated  0.022639\n",
      "398  0.668047  0.674656  0.668047  0.674656  unrelated -0.006608\n",
      "399  0.632570  0.672570  0.632570  0.672570  unrelated -0.040000\n",
      "400  0.660775  0.673770  0.589813  0.628669  unrelated  0.032106\n",
      "401  0.723718  0.708431  0.668206  0.681454  unrelated  0.042264\n",
      "403  0.630442  0.630442  0.634686  0.627826  unrelated  0.002616\n",
      "404  0.680629  0.655354  0.692699  0.693222  unrelated -0.012593\n",
      "405  0.620613  0.615744  0.620613  0.615744  unrelated  0.004869\n",
      "406  0.673406  0.666376  0.671809  0.684488  unrelated -0.011082\n",
      "407  0.670196  0.660998  0.670196  0.660998  unrelated  0.009198\n",
      "408  0.618922  0.618922  0.612187  0.612187  unrelated  0.006736\n",
      "409  0.614894  0.603808  0.570429  0.570429  unrelated  0.044465\n",
      "411  0.599124  0.599124  0.643348  0.643348  unrelated -0.044224\n",
      "412  0.638904  0.638904  0.638904  0.638904  unrelated  0.000000\n",
      "413  0.647705  0.629921  0.647705  0.629921  unrelated  0.017783\n",
      "414  0.630275  0.627065  0.633974  0.633974  unrelated -0.003699\n",
      "415  0.685537  0.687484  0.679653  0.679653  unrelated  0.005884\n",
      "416  0.709419  0.709857  0.709419  0.709857  unrelated -0.000438\n",
      "417  0.617591  0.604374  0.619085  0.610759  unrelated  0.006831\n",
      "418  0.712965  0.712965  0.711793  0.711793  unrelated  0.001172\n",
      "419  0.686877  0.671192  0.631943  0.631943  unrelated  0.054935\n",
      "420  0.683698  0.672810  0.683698  0.672810  unrelated  0.010888\n",
      "421  0.637224  0.632061  0.657594  0.657594  unrelated -0.020370\n",
      "422  0.629817  0.629817  0.629817  0.629817  unrelated  0.000000\n",
      "423  0.600930  0.600930  0.581234  0.581234  unrelated  0.019696\n",
      "424  0.673503  0.673471  0.663131  0.663936  unrelated  0.009566\n",
      "425  0.626283  0.632108  0.620565  0.620565  unrelated  0.005718\n",
      "426  0.680527  0.680527  0.658152  0.658152  unrelated  0.022375\n",
      "427  0.684925  0.672470  0.684925  0.672470  unrelated  0.012455\n",
      "428  0.639013  0.639013  0.639013  0.639013  unrelated  0.000000\n",
      "429  0.661203  0.654487  0.661203  0.654487  unrelated  0.006716\n",
      "430  0.667100  0.665995  0.667100  0.665995  unrelated  0.001105\n",
      "431  0.710567  0.704234  0.718614  0.689841  unrelated  0.020727\n",
      "432  0.604313  0.604313  0.623030  0.623030  unrelated -0.018717\n",
      "433  0.636561  0.636561  0.651326  0.647054  unrelated -0.010493\n",
      "434  0.606938  0.611126  0.606938  0.611126  unrelated -0.004188\n",
      "435  0.649222  0.657874  0.649222  0.657874  unrelated -0.008652\n",
      "436  0.595194  0.585592  0.595194  0.585592  unrelated  0.009602\n",
      "437  0.605495  0.605495  0.616897  0.616897  unrelated -0.011402\n",
      "438  0.572706  0.572519  0.623533  0.623533  unrelated -0.050827\n",
      "439  0.643257  0.643257  0.643205  0.643205  unrelated  0.000052\n",
      "440  0.699657  0.699555  0.699657  0.699555  unrelated  0.000102\n",
      "441  0.628680  0.617169  0.590578  0.587404  unrelated  0.041276\n",
      "442  0.719469  0.719469  0.722745  0.722745  unrelated -0.003275\n",
      "443  0.687626  0.687626  0.687626  0.687626  unrelated  0.000000\n",
      "444  0.609030  0.601906  0.609030  0.601906  unrelated  0.007124\n",
      "445  0.639536  0.652618  0.621188  0.636944  unrelated  0.002592\n",
      "446  0.596255  0.569108  0.600046  0.572675  unrelated  0.023581\n",
      "447  0.515366  0.511374  0.515366  0.511374  unrelated  0.003992\n",
      "448  0.674788  0.660520  0.661946  0.660188  unrelated  0.014600\n",
      "449  0.611395  0.611395  0.611395  0.611395  unrelated  0.000000\n",
      "450  0.599357  0.593474  0.599357  0.593474  unrelated  0.005883\n",
      "451  0.662062  0.662062  0.641564  0.641564  unrelated  0.020498\n",
      "452  0.644354  0.639277  0.644354  0.639277  unrelated  0.005078\n",
      "453  0.615598  0.608451  0.615598  0.608451  unrelated  0.007147\n",
      "454  0.678671  0.669412  0.678671  0.669412  unrelated  0.009259\n",
      "456  0.687483  0.687483  0.687483  0.687483  unrelated  0.000000\n",
      "457  0.683110  0.679227  0.677264  0.674216  unrelated  0.008894\n",
      "458  0.697182  0.697182  0.716568  0.716568  unrelated -0.019386\n",
      "459  0.696065  0.696065  0.661356  0.661356  unrelated  0.034708\n",
      "460  0.676105  0.676105  0.649129  0.654202  unrelated  0.021903\n",
      "461  0.700247  0.677998  0.703493  0.697444  unrelated  0.002803\n",
      "463  0.691809  0.692384  0.692832  0.693630  unrelated -0.001821\n",
      "464  0.658486  0.665371  0.636760  0.651197  unrelated  0.007290\n",
      "465  0.676807  0.677998  0.676807  0.677998  unrelated -0.001191\n",
      "466  0.640405  0.640405  0.624233  0.629074  unrelated  0.011331\n",
      "467  0.690547  0.690547  0.720866  0.720866  unrelated -0.030320\n",
      "468  0.690275  0.694143  0.696959  0.707722  unrelated -0.017447\n",
      "469  0.672789  0.672789  0.649264  0.654658  unrelated  0.018130\n",
      "470  0.690547  0.690547  0.720866  0.720866  unrelated -0.030320\n",
      "471  0.695262  0.695262  0.695262  0.695262  unrelated  0.000000\n",
      "472  0.656389  0.655327  0.610003  0.610003  unrelated  0.046387\n",
      "473  0.676325  0.676325  0.676325  0.676325  unrelated  0.000000\n",
      "474  0.662118  0.662118  0.662908  0.662908  unrelated -0.000791\n",
      "475  0.673649  0.678375  0.673649  0.678375  unrelated -0.004725\n",
      "476  0.659919  0.659919  0.634042  0.634042  unrelated  0.025876\n",
      "477  0.665290  0.665290  0.665290  0.665290  unrelated  0.000000\n",
      "478  0.569082  0.569956  0.546691  0.552899  unrelated  0.016183\n",
      "479  0.642731  0.648748  0.642731  0.648748  unrelated -0.006017\n",
      "480  0.783057  0.783057  0.783057  0.783057  unrelated  0.000000\n",
      "481  0.673175  0.673175  0.673175  0.673175  unrelated  0.000000\n",
      "482  0.621598  0.574581  0.633903  0.604603  unrelated  0.016994\n",
      "483  0.637556  0.638667  0.618861  0.623287  unrelated  0.014269\n",
      "484  0.662536  0.667267  0.643705  0.656455  unrelated  0.006081\n",
      "485  0.657705  0.665863  0.657705  0.665863  unrelated -0.008158\n",
      "486  0.736966  0.734623  0.726616  0.724951  unrelated  0.012015\n",
      "487  0.744171  0.743545  0.692801  0.701516  unrelated  0.042654\n",
      "488  0.711287  0.704484  0.711287  0.704484  unrelated  0.006803\n",
      "489  0.690680  0.707038  0.690680  0.707038  unrelated -0.016357\n",
      "491  0.724499  0.705101  0.724499  0.705101  unrelated  0.019398\n",
      "492  0.601469  0.586237  0.615010  0.644945  unrelated -0.043476\n",
      "493  0.677662  0.691877  0.677662  0.691877  unrelated -0.014214\n",
      "494  0.616222  0.614175  0.616048  0.611191  unrelated  0.005031\n",
      "495  0.573239  0.573239  0.573239  0.573239  unrelated  0.000000\n",
      "496  0.613089  0.614294  0.588996  0.596398  unrelated  0.016691\n",
      "497  0.730885  0.735756  0.730885  0.735756  unrelated -0.004872\n",
      "498  0.715178  0.717582  0.672269  0.683036  unrelated  0.032142\n",
      "501  0.675193  0.676651  0.626539  0.635454  unrelated  0.039739\n",
      "502  0.646404  0.668852  0.646404  0.668852  unrelated -0.022448\n",
      "503  0.622173  0.627641  0.646696  0.652955  unrelated -0.030782\n",
      "504  0.763742  0.770785  0.743170  0.775725  unrelated -0.011983\n",
      "506  0.648667  0.708530  0.648667  0.708530  unrelated -0.059862\n",
      "507  0.721054  0.707537  0.693309  0.713850  unrelated  0.007204\n",
      "508  0.621528  0.619759  0.631355  0.638163  unrelated -0.016635\n",
      "509  0.605397  0.595872  0.605397  0.595872  unrelated  0.009525\n",
      "510  0.742174  0.773663  0.742174  0.773663  unrelated -0.031489\n",
      "511  0.731202  0.736842  0.695186  0.727114  unrelated  0.004088\n",
      "512  0.609470  0.589782  0.616724  0.609880  unrelated -0.000410\n",
      "513  0.740027  0.759244  0.695538  0.729381  unrelated  0.010646\n",
      "514  0.666931  0.666931  0.666931  0.666931  unrelated  0.000000\n",
      "515  0.680062  0.680767  0.656344  0.673726  unrelated  0.006336\n",
      "516  0.606955  0.584894  0.591868  0.591003  unrelated  0.015952\n",
      "517  0.757837  0.759871  0.714640  0.730789  unrelated  0.027048\n",
      "518  0.612417  0.616540  0.612417  0.616540  unrelated -0.004122\n",
      "519  0.723786  0.736667  0.723786  0.736667  unrelated -0.012882\n",
      "520  0.689476  0.689476  0.685572  0.694592  unrelated -0.005116\n",
      "521  0.674409  0.690263  0.674409  0.690263  unrelated -0.015855\n",
      "522  0.629782  0.630872  0.611762  0.621242  unrelated  0.008540\n",
      "523  0.655786  0.704997  0.655786  0.704997  unrelated -0.049212\n",
      "524  0.552494  0.552494  0.596879  0.606980  unrelated -0.054486\n",
      "525  0.724825  0.740073  0.724825  0.740073  unrelated -0.015247\n",
      "526  0.790878  0.783715  0.790878  0.783715  unrelated  0.007162\n",
      "527  0.729593  0.738878  0.657414  0.677963  unrelated  0.051630\n",
      "528  0.666287  0.689534  0.668718  0.704834  unrelated -0.038547\n",
      "529  0.607990  0.616267  0.607990  0.616267  unrelated -0.008276\n",
      "530  0.680974  0.691280  0.680974  0.691280  unrelated -0.010306\n",
      "531  0.736940  0.702726  0.736940  0.702726  unrelated  0.034214\n",
      "532  0.763931  0.763931  0.774673  0.774673  unrelated -0.010742\n",
      "533  0.659335  0.657433  0.616635  0.664503  unrelated -0.005168\n",
      "534  0.763588  0.774371  0.723983  0.754089  unrelated  0.009499\n",
      "535  0.753676  0.772428  0.753676  0.772428  unrelated -0.018752\n",
      "536  0.729665  0.732185  0.675961  0.715958  unrelated  0.013707\n",
      "537  0.650749  0.683527  0.650749  0.683527  unrelated -0.032778\n",
      "538  0.691941  0.686457  0.620542  0.629692  unrelated  0.062249\n",
      "539  0.762065  0.785518  0.747383  0.784623  unrelated -0.022557\n",
      "540  0.613183  0.620272  0.613183  0.620272  unrelated -0.007090\n",
      "541  0.785027  0.782650  0.769843  0.774637  unrelated  0.010390\n",
      "542  0.750686  0.718484  0.684060  0.704612  unrelated  0.046074\n",
      "543  0.629152  0.635488  0.629152  0.635488  unrelated -0.006336\n",
      "544  0.619268  0.654340  0.610239  0.639812  unrelated -0.020544\n",
      "545  0.723906  0.722934  0.723906  0.722934  unrelated  0.000972\n",
      "546  0.677668  0.643663  0.677668  0.643663  unrelated  0.034004\n",
      "547  0.746978  0.756402  0.695474  0.722453  unrelated  0.024525\n",
      "548  0.719654  0.683596  0.654002  0.636889  unrelated  0.082765\n",
      "549  0.782624  0.787121  0.781974  0.792203  unrelated -0.009579\n",
      "550  0.749035  0.703771  0.736067  0.708575  unrelated  0.040460\n",
      "551  0.729490  0.730503  0.729490  0.730503  unrelated -0.001012\n",
      "552  0.707992  0.707992  0.696908  0.696908  unrelated  0.011084\n",
      "553  0.648170  0.672248  0.648170  0.672248  unrelated -0.024078\n",
      "554  0.712612  0.746516  0.659637  0.644765  unrelated  0.067846\n",
      "555  0.665151  0.648083  0.659539  0.658440  unrelated  0.006710\n",
      "556  0.695134  0.687077  0.666557  0.647956  unrelated  0.047178\n",
      "557  0.713337  0.723110  0.633203  0.597881  unrelated  0.115456\n",
      "558  0.696325  0.696325  0.671438  0.672340  unrelated  0.023986\n",
      "559  0.760140  0.782526  0.747027  0.783464  unrelated -0.023324\n",
      "560  0.653543  0.633108  0.653543  0.633108  unrelated  0.020435\n",
      "561  0.698855  0.706082  0.698855  0.706082  unrelated -0.007227\n",
      "562  0.689265  0.686647  0.663526  0.680318  unrelated  0.008946\n",
      "563  0.610138  0.615550  0.594982  0.605107  unrelated  0.005031\n",
      "564  0.683470  0.680533  0.646113  0.637216  unrelated  0.046254\n",
      "565  0.705872  0.764510  0.668662  0.730553  unrelated -0.024681\n",
      "566  0.699076  0.667885  0.691471  0.675337  unrelated  0.023739\n",
      "567  0.602275  0.591877  0.572299  0.583038  unrelated  0.019237\n",
      "568  0.703596  0.684333  0.630810  0.630810  unrelated  0.072786\n",
      "569  0.715037  0.705033  0.749220  0.739557  unrelated -0.024520\n",
      "570  0.732359  0.774085  0.732359  0.774085  unrelated -0.041726\n",
      "571  0.656791  0.638100  0.640537  0.624170  unrelated  0.032622\n",
      "572  0.684527  0.684527  0.696001  0.696001  unrelated -0.011474\n",
      "573  0.795183  0.808245  0.744172  0.788640  unrelated  0.006544\n",
      "574  0.671745  0.624255  0.652769  0.667860  unrelated  0.003885\n",
      "575  0.683717  0.683717  0.691516  0.707043  unrelated -0.023326\n",
      "576  0.666350  0.696568  0.666350  0.696568  unrelated -0.030218\n",
      "577  0.792336  0.805177  0.763673  0.808346  unrelated -0.016010\n",
      "578  0.655617  0.656461  0.655617  0.656461  unrelated -0.000844\n",
      "579  0.715294  0.732057  0.715294  0.732057  unrelated -0.016762\n",
      "580  0.736854  0.735648  0.690632  0.706247  unrelated  0.030607\n",
      "581  0.764051  0.741209  0.714998  0.715398  unrelated  0.048653\n",
      "582  0.713176  0.713176  0.656554  0.668189  unrelated  0.044987\n",
      "583  0.578733  0.548137  0.578733  0.548137  unrelated  0.030596\n",
      "584  0.642704  0.636187  0.631691  0.649434  unrelated -0.006730\n",
      "585  0.646568  0.646568  0.646568  0.646568  unrelated  0.000000\n",
      "586  0.637257  0.631568  0.666124  0.666124  unrelated -0.028866\n",
      "587  0.809519  0.792444  0.766614  0.789375  unrelated  0.020144\n",
      "588  0.631608  0.627863  0.621933  0.655042  unrelated -0.023434\n",
      "589  0.713679  0.700409  0.658759  0.694215  unrelated  0.019464\n",
      "590  0.756542  0.748810  0.714719  0.732523  unrelated  0.024019\n",
      "591  0.775243  0.770432  0.757982  0.755409  unrelated  0.019834\n",
      "592  0.756304  0.760768  0.721775  0.733410  unrelated  0.022894\n",
      "593  0.800198  0.794425  0.800198  0.794425  unrelated  0.005774\n",
      "594  0.791713  0.709971  0.770021  0.706419  unrelated  0.085294\n",
      "595  0.692655  0.675959  0.670168  0.654947  unrelated  0.037708\n",
      "596  0.740121  0.746345  0.740121  0.746345  unrelated -0.006224\n",
      "597  0.682198  0.648305  0.677893  0.652967  unrelated  0.029231\n",
      "598  0.744383  0.725048  0.750710  0.756573  unrelated -0.012190\n",
      "600  0.719315  0.755627  0.719315  0.755627  unrelated -0.036312\n",
      "601  0.665081  0.672953  0.649413  0.666780  unrelated -0.001700\n",
      "602  0.667490  0.667170  0.628007  0.655720  unrelated  0.011770\n",
      "603  0.683532  0.656921  0.666376  0.656765  unrelated  0.026767\n",
      "604  0.648245  0.651345  0.635514  0.643782  unrelated  0.004463\n",
      "605  0.731503  0.718288  0.765050  0.763945  unrelated -0.032441\n",
      "606  0.696839  0.727915  0.696839  0.727915  unrelated -0.031076\n",
      "607  0.640229  0.640962  0.640229  0.640962  unrelated -0.000733\n",
      "608  0.765583  0.765583  0.758013  0.769233  unrelated -0.003651\n",
      "609  0.682626  0.696216  0.682626  0.696216  unrelated -0.013590\n",
      "610  0.728458  0.734328  0.643355  0.741274  unrelated -0.012816\n",
      "611  0.696325  0.696325  0.671438  0.672340  unrelated  0.023986\n",
      "612  0.659535  0.649177  0.659535  0.649177  unrelated  0.010358\n",
      "613  0.565115  0.566268  0.506436  0.505579  unrelated  0.059536\n",
      "615  0.612102  0.610578  0.612102  0.610578  unrelated  0.001524\n",
      "616  0.746121  0.740418  0.700700  0.702160  unrelated  0.043961\n",
      "617  0.590174  0.577197  0.563654  0.554732  unrelated  0.035443\n",
      "619  0.711850  0.721462  0.711018  0.719956  unrelated -0.008106\n",
      "620  0.734002  0.728823  0.727969  0.724255  unrelated  0.009747\n",
      "621  0.654258  0.643948  0.654258  0.643948  unrelated  0.010309\n",
      "622  0.603331  0.573397  0.603331  0.573397  unrelated  0.029934\n",
      "623  0.652818  0.597140  0.634439  0.590845  unrelated  0.061973\n",
      "624  0.601013  0.577426  0.601013  0.577426  unrelated  0.023587\n",
      "625  0.619341  0.605425  0.632851  0.619346  unrelated -0.000005\n",
      "626  0.668951  0.671836  0.668951  0.671836  unrelated -0.002885\n",
      "627  0.729887  0.713996  0.700107  0.701996  unrelated  0.027891\n",
      "628  0.703328  0.705869  0.734235  0.744293  unrelated -0.040965\n",
      "629  0.682219  0.688979  0.618853  0.650318  unrelated  0.031901\n",
      "630  0.661219  0.661219  0.665344  0.667824  unrelated -0.006605\n",
      "632  0.626656  0.642257  0.621592  0.650258  unrelated -0.023602\n",
      "633  0.699646  0.699646  0.686255  0.686255  unrelated  0.013390\n",
      "634  0.645861  0.662438  0.583698  0.620336  unrelated  0.025525\n",
      "635  0.677190  0.683504  0.663094  0.678598  unrelated -0.001408\n",
      "636  0.678331  0.699689  0.685389  0.704193  unrelated -0.025862\n",
      "637  0.565890  0.580909  0.565890  0.580909  unrelated -0.015019\n",
      "638  0.647069  0.658980  0.647069  0.658980  unrelated -0.011912\n",
      "639  0.689575  0.679139  0.702672  0.714857  unrelated -0.025282\n",
      "640  0.782779  0.791118  0.766254  0.769661  unrelated  0.013118\n",
      "641  0.603767  0.645573  0.603767  0.645573  unrelated -0.041806\n",
      "642  0.656759  0.658382  0.621766  0.634257  unrelated  0.022502\n",
      "643  0.704113  0.713120  0.679821  0.688564  unrelated  0.015549\n",
      "644  0.635139  0.635139  0.636709  0.636709  unrelated -0.001570\n",
      "645  0.619496  0.625579  0.618409  0.643570  unrelated -0.024074\n",
      "646  0.656595  0.668293  0.535829  0.555732  unrelated  0.100862\n",
      "647  0.668549  0.691469  0.664931  0.689272  unrelated -0.020724\n",
      "648  0.666981  0.682380  0.566443  0.604995  unrelated  0.061986\n",
      "649  0.657883  0.657883  0.663492  0.663492  unrelated -0.005610\n",
      "650  0.630848  0.654916  0.594699  0.616378  unrelated  0.014470\n",
      "651  0.709214  0.709214  0.719222  0.719222  unrelated -0.010007\n",
      "652  0.722098  0.727346  0.685526  0.713368  unrelated  0.008730\n",
      "653  0.648318  0.659400  0.648621  0.663573  unrelated -0.015255\n",
      "654  0.677727  0.686194  0.653430  0.685548  unrelated -0.007821\n",
      "655  0.727599  0.736905  0.682701  0.693571  unrelated  0.034027\n",
      "656  0.613353  0.636107  0.617598  0.653403  unrelated -0.040050\n",
      "657  0.646604  0.652948  0.643756  0.657082  unrelated -0.010478\n",
      "658  0.653085  0.656300  0.664865  0.669548  unrelated -0.016463\n",
      "659  0.656993  0.665434  0.649081  0.659612  unrelated -0.002619\n",
      "660  0.636978  0.649765  0.649716  0.657576  unrelated -0.020598\n",
      "661  0.721550  0.739787  0.722286  0.742811  unrelated -0.021261\n",
      "662  0.651403  0.660650  0.671520  0.679843  unrelated -0.028439\n",
      "663  0.680282  0.680282  0.668208  0.674612  unrelated  0.005670\n",
      "664  0.677283  0.677283  0.701549  0.701549  unrelated -0.024266\n",
      "665  0.681787  0.689868  0.587457  0.622545  unrelated  0.059242\n",
      "666  0.637487  0.647245  0.654295  0.664860  unrelated -0.027373\n",
      "667  0.621965  0.657500  0.621965  0.657500  unrelated -0.035535\n",
      "669  0.676241  0.695932  0.662014  0.676034  unrelated  0.000208\n",
      "670  0.674291  0.697252  0.661457  0.661457  unrelated  0.012834\n",
      "671  0.584926  0.579545  0.595277  0.583999  unrelated  0.000927\n",
      "672  0.710553  0.699184  0.687307  0.674418  unrelated  0.036135\n",
      "673  0.697251  0.719405  0.681291  0.685260  unrelated  0.011991\n",
      "674  0.743260  0.735582  0.720568  0.765662  unrelated -0.022403\n",
      "675  0.706696  0.706696  0.683688  0.683688  unrelated  0.023008\n",
      "676  0.737769  0.740892  0.666976  0.676136  unrelated  0.061634\n",
      "677  0.706416  0.723462  0.689321  0.690021  unrelated  0.016394\n",
      "678  0.624354  0.624354  0.624354  0.624354  unrelated  0.000000\n",
      "680  0.791725  0.795243  0.737449  0.759442  unrelated  0.032283\n",
      "682  0.637094  0.637094  0.629459  0.629459  unrelated  0.007635\n",
      "683  0.705678  0.715980  0.685915  0.707966  unrelated -0.002288\n",
      "684  0.778414  0.778414  0.736416  0.754796  unrelated  0.023618\n",
      "685  0.738967  0.738967  0.736320  0.736320  unrelated  0.002648\n",
      "686  0.712444  0.707627  0.701594  0.703321  unrelated  0.009122\n",
      "687  0.688845  0.770132  0.682249  0.745991  unrelated -0.057145\n",
      "688  0.710917  0.752993  0.695893  0.743953  unrelated -0.033036\n",
      "689  0.607844  0.646121  0.607844  0.646121  unrelated -0.038276\n",
      "690  0.709402  0.709402  0.709402  0.709402  unrelated  0.000000\n",
      "691  0.717206  0.734709  0.633458  0.694756  unrelated  0.022450\n",
      "692  0.716541  0.712346  0.709816  0.745502  unrelated -0.028961\n",
      "693  0.785486  0.785486  0.752113  0.752113  unrelated  0.033373\n",
      "694  0.679213  0.679213  0.675980  0.692350  unrelated -0.013137\n",
      "695  0.734560  0.734560  0.734560  0.734560  unrelated  0.000000\n",
      "696  0.743588  0.738646  0.710886  0.715950  unrelated  0.027638\n",
      "697  0.723732  0.720491  0.645504  0.645504  unrelated  0.078228\n",
      "698  0.718545  0.714992  0.716797  0.714993  unrelated  0.003552\n",
      "699  0.717537  0.718043  0.670152  0.670152  unrelated  0.047385\n",
      "700  0.661677  0.673382  0.693564  0.693564  unrelated -0.031887\n",
      "701  0.673460  0.673460  0.656077  0.656077  unrelated  0.017382\n",
      "702  0.731930  0.731930  0.731930  0.731930  unrelated  0.000000\n",
      "703  0.761459  0.753961  0.691703  0.706261  unrelated  0.055198\n",
      "704  0.710279  0.710279  0.645199  0.686823  unrelated  0.023456\n",
      "705  0.685301  0.712389  0.685301  0.712389  unrelated -0.027088\n",
      "706  0.754666  0.743723  0.734744  0.737313  unrelated  0.017354\n",
      "707  0.642609  0.643007  0.642609  0.643007  unrelated -0.000398\n",
      "708  0.708003  0.708003  0.686680  0.686680  unrelated  0.021322\n",
      "709  0.768430  0.755920  0.738757  0.742411  unrelated  0.026018\n",
      "710  0.732126  0.722466  0.645558  0.675728  unrelated  0.056398\n",
      "711  0.685367  0.674236  0.685367  0.674236  unrelated  0.011131\n",
      "712  0.785096  0.780503  0.703183  0.716403  unrelated  0.068693\n",
      "713  0.734904  0.710409  0.630846  0.649518  unrelated  0.085386\n",
      "714  0.727863  0.726705  0.720136  0.745784  unrelated -0.017921\n",
      "715  0.675760  0.708200  0.675760  0.708200  unrelated -0.032440\n",
      "716  0.687096  0.687096  0.619443  0.687763  unrelated -0.000667\n",
      "717  0.634505  0.662172  0.605452  0.643591  unrelated -0.009086\n",
      "718  0.686417  0.686417  0.686417  0.686417  unrelated  0.000000\n",
      "719  0.737394  0.737394  0.737394  0.737394  unrelated  0.000000\n",
      "720  0.670167  0.669940  0.672952  0.672952  unrelated -0.002785\n",
      "721  0.741178  0.731662  0.751271  0.735803  unrelated  0.005375\n",
      "722  0.753166  0.740847  0.698673  0.718090  unrelated  0.035076\n",
      "723  0.739042  0.769539  0.739042  0.769539  unrelated -0.030497\n",
      "724  0.724623  0.723352  0.690951  0.712639  unrelated  0.011984\n",
      "725  0.681294  0.700375  0.681294  0.700375  unrelated -0.019082\n",
      "726  0.729649  0.717407  0.729649  0.717407  unrelated  0.012242\n",
      "727  0.754166  0.753108  0.736712  0.727251  unrelated  0.026914\n",
      "728  0.689262  0.672467  0.689262  0.672467  unrelated  0.016795\n",
      "729  0.708131  0.693092  0.708131  0.693092  unrelated  0.015039\n",
      "730  0.692983  0.692983  0.673900  0.673900  unrelated  0.019083\n",
      "731  0.750218  0.750218  0.740377  0.740377  unrelated  0.009842\n",
      "732  0.690818  0.697791  0.660135  0.685910  unrelated  0.004908\n",
      "733  0.726120  0.715651  0.559185  0.559185  unrelated  0.166934\n",
      "734  0.770406  0.793465  0.770406  0.793465  unrelated -0.023058\n",
      "735  0.733870  0.747311  0.689938  0.748497  unrelated -0.014627\n",
      "736  0.667811  0.659638  0.667811  0.659638  unrelated  0.008173\n",
      "737  0.643376  0.641998  0.643025  0.643025  unrelated  0.000351\n",
      "738  0.690073  0.700596  0.690073  0.700596  unrelated -0.010523\n",
      "739  0.730390  0.731124  0.730390  0.731124  unrelated -0.000735\n",
      "740  0.712737  0.693588  0.594995  0.622555  unrelated  0.090182\n",
      "741  0.520874  0.520874  0.483257  0.483257  unrelated  0.037618\n",
      "742  0.739579  0.739566  0.741536  0.747422  unrelated -0.007844\n",
      "743  0.812882  0.801248  0.760562  0.774455  unrelated  0.038427\n",
      "744  0.708903  0.708591  0.719600  0.714949  unrelated -0.006046\n",
      "745  0.637594  0.649081  0.647831  0.674544  unrelated -0.036950\n",
      "746  0.731655  0.731655  0.748620  0.750892  unrelated -0.019237\n",
      "747  0.749426  0.750452  0.727406  0.745684  unrelated  0.003742\n",
      "748  0.701200  0.701200  0.639032  0.639032  unrelated  0.062168\n",
      "749  0.726369  0.726369  0.717435  0.717435  unrelated  0.008934\n",
      "750  0.749684  0.749684  0.759439  0.759439  unrelated -0.009754\n",
      "751  0.693573  0.718686  0.679180  0.714280  unrelated -0.020707\n",
      "752  0.647540  0.599823  0.618112  0.680253  unrelated -0.032713\n",
      "753  0.697323  0.697323  0.664124  0.664124  unrelated  0.033198\n",
      "754  0.731668  0.715879  0.688094  0.713386  unrelated  0.018283\n",
      "755  0.668043  0.671038  0.670770  0.670770  unrelated -0.002727\n",
      "757  0.675681  0.674287  0.670712  0.680850  unrelated -0.005170\n",
      "758  0.753171  0.741188  0.638822  0.693575  unrelated  0.059596\n",
      "759  0.740378  0.723509  0.697726  0.694498  unrelated  0.045880\n",
      "760  0.766576  0.774886  0.766576  0.774886  unrelated -0.008310\n",
      "761  0.627405  0.631332  0.605323  0.622483  unrelated  0.004923\n",
      "762  0.710834  0.710834  0.710834  0.710834  unrelated  0.000000\n",
      "763  0.726581  0.726581  0.711727  0.709428  unrelated  0.017153\n",
      "765  0.670467  0.673497  0.702059  0.717728  unrelated -0.047261\n",
      "766  0.722486  0.713941  0.698355  0.709948  unrelated  0.012539\n",
      "767  0.740497  0.743894  0.740497  0.743894  unrelated -0.003397\n",
      "768  0.705107  0.705107  0.633318  0.642023  unrelated  0.063085\n",
      "769  0.719896  0.719896  0.684478  0.684478  unrelated  0.035419\n",
      "770  0.707406  0.705793  0.639967  0.644662  unrelated  0.062744\n",
      "771  0.630370  0.642186  0.630370  0.642186  unrelated -0.011817\n",
      "772  0.640628  0.620706  0.587536  0.567514  unrelated  0.073114\n",
      "773  0.681006  0.693315  0.593069  0.589629  unrelated  0.091377\n",
      "774  0.686041  0.672569  0.620519  0.631204  unrelated  0.054836\n",
      "775  0.705381  0.704321  0.700904  0.705808  unrelated -0.000426\n",
      "776  0.721632  0.720338  0.723168  0.731477  unrelated -0.009845\n",
      "777  0.637199  0.655762  0.637199  0.655762  unrelated -0.018563\n",
      "778  0.722518  0.721630  0.745402  0.745100  unrelated -0.022582\n",
      "779  0.670120  0.658318  0.648733  0.647750  unrelated  0.022371\n",
      "780  0.707770  0.715570  0.698874  0.717321  unrelated -0.009551\n",
      "781  0.681062  0.679689  0.628646  0.630479  unrelated  0.050582\n",
      "782  0.667631  0.656199  0.630856  0.644658  unrelated  0.022973\n",
      "783  0.697697  0.698416  0.649432  0.644941  unrelated  0.052756\n",
      "784  0.681062  0.679689  0.628646  0.630479  unrelated  0.050582\n",
      "785  0.583789  0.579674  0.611894  0.612904  unrelated -0.029115\n",
      "786  0.644385  0.619943  0.626017  0.615008  unrelated  0.029377\n",
      "787  0.648974  0.658456  0.653285  0.664380  unrelated -0.015406\n",
      "788  0.666192  0.660597  0.610291  0.614769  unrelated  0.051422\n",
      "789  0.646608  0.678952  0.646608  0.678952  unrelated -0.032344\n",
      "790  0.647808  0.640948  0.641978  0.658782  unrelated -0.010974\n",
      "791  0.770507  0.742607  0.727927  0.725993  unrelated  0.044513\n",
      "792  0.744004  0.743092  0.710913  0.721896  unrelated  0.022108\n",
      "793  0.654615  0.633967  0.625704  0.597121  unrelated  0.057494\n",
      "794  0.666160  0.668105  0.632824  0.655421  unrelated  0.010738\n",
      "795  0.708868  0.701256  0.673672  0.689431  unrelated  0.019437\n",
      "796  0.698090  0.713336  0.699110  0.713534  unrelated -0.015444\n",
      "797  0.712322  0.696111  0.711840  0.711970  unrelated  0.000352\n",
      "798  0.590192  0.598297  0.590192  0.598297  unrelated -0.008106\n",
      "799  0.717641  0.714556  0.680793  0.674534  unrelated  0.043107\n",
      "800  0.613512  0.595456  0.637864  0.648507  unrelated -0.034995\n",
      "801  0.661875  0.657174  0.645248  0.650798  unrelated  0.011077\n",
      "802  0.672415  0.671596  0.693464  0.700621  unrelated -0.028206\n",
      "803  0.679900  0.682768  0.657218  0.670585  unrelated  0.009316\n",
      "804  0.751144  0.745036  0.687950  0.709925  unrelated  0.041219\n",
      "805  0.660312  0.652075  0.665341  0.668832  unrelated -0.008520\n",
      "806  0.738212  0.717969  0.737977  0.723089  unrelated  0.015124\n",
      "807  0.767943  0.778466  0.741287  0.733887  unrelated  0.034057\n",
      "808  0.634323  0.614028  0.595283  0.581821  unrelated  0.052501\n",
      "809  0.712391  0.729091  0.698102  0.721194  unrelated -0.008803\n",
      "810  0.677936  0.642183  0.652046  0.636073  unrelated  0.041863\n",
      "811  0.594932  0.587737  0.594932  0.587737  unrelated  0.007195\n",
      "812  0.661861  0.682692  0.661861  0.682692  unrelated -0.020830\n",
      "813  0.669706  0.659915  0.622647  0.631658  unrelated  0.038048\n",
      "814  0.634982  0.616247  0.612745  0.619929  unrelated  0.015052\n",
      "815  0.723112  0.719699  0.750336  0.758848  unrelated -0.035736\n",
      "816  0.681006  0.693315  0.593069  0.589629  unrelated  0.091377\n",
      "817  0.676015  0.697034  0.671958  0.715048  unrelated -0.039033\n",
      "818  0.673309  0.680937  0.625181  0.640202  unrelated  0.033107\n",
      "819  0.685677  0.676194  0.648949  0.656206  unrelated  0.029470\n",
      "820  0.706837  0.715731  0.712858  0.706385  unrelated  0.000453\n",
      "821  0.648090  0.651457  0.648090  0.651457  unrelated -0.003367\n",
      "822  0.652784  0.627352  0.633454  0.615430  unrelated  0.037354\n",
      "823  0.600434  0.585488  0.600434  0.585488  unrelated  0.014946\n",
      "824  0.658399  0.656843  0.658399  0.656843  unrelated  0.001556\n",
      "825  0.724214  0.738092  0.661309  0.680577  unrelated  0.043637\n",
      "826  0.618231  0.640386  0.618231  0.640386  unrelated -0.022155\n",
      "827  0.681008  0.701140  0.632761  0.645976  unrelated  0.035031\n",
      "828  0.656575  0.658212  0.656575  0.658212  unrelated -0.001637\n",
      "829  0.655641  0.683357  0.655641  0.683357  unrelated -0.027716\n",
      "830  0.633094  0.618995  0.550215  0.537354  unrelated  0.095741\n",
      "831  0.627192  0.620346  0.618503  0.620703  unrelated  0.006489\n",
      "832  0.608108  0.597834  0.631410  0.636836  unrelated -0.028728\n",
      "833  0.660415  0.666205  0.663611  0.679848  unrelated -0.019434\n",
      "834  0.651746  0.639797  0.686616  0.687286  unrelated -0.035540\n",
      "835  0.685353  0.684162  0.685353  0.684162  unrelated  0.001191\n",
      "836  0.643739  0.633288  0.618470  0.617995  unrelated  0.025744\n",
      "837  0.585809  0.567363  0.561190  0.553190  unrelated  0.032619\n",
      "838  0.718389  0.713165  0.545644  0.552791  unrelated  0.165597\n",
      "839  0.699772  0.717510  0.709009  0.729282  unrelated -0.029510\n",
      "840  0.726090  0.737962  0.699816  0.714090  unrelated  0.012000\n",
      "841  0.620994  0.602854  0.576199  0.576199  unrelated  0.044795\n",
      "842  0.711764  0.699949  0.713320  0.714375  unrelated -0.002611\n",
      "843  0.697261  0.701959  0.697261  0.701959  unrelated -0.004698\n",
      "844  0.670155  0.662821  0.541439  0.557511  unrelated  0.112643\n",
      "845  0.672017  0.705721  0.632383  0.665935  unrelated  0.006081\n",
      "846  0.651351  0.617131  0.622734  0.645450  unrelated  0.005901\n",
      "847  0.633680  0.633153  0.620416  0.622639  unrelated  0.011041\n",
      "848  0.639850  0.604341  0.634095  0.616636  unrelated  0.023213\n",
      "849  0.681160  0.689617  0.720579  0.737505  unrelated -0.056345\n",
      "850  0.724839  0.729786  0.644428  0.650987  unrelated  0.073852\n",
      "851  0.642079  0.642631  0.642079  0.642631  unrelated -0.000551\n",
      "852  0.648875  0.643049  0.624418  0.632983  unrelated  0.015892\n",
      "853  0.693922  0.688773  0.693922  0.688773  unrelated  0.005149\n",
      "854  0.649807  0.624946  0.648436  0.651804  unrelated -0.001997\n",
      "855  0.719703  0.720775  0.690757  0.720366  unrelated -0.000662\n",
      "856  0.626175  0.646077  0.617553  0.634785  unrelated -0.008610\n",
      "857  0.630066  0.605799  0.599031  0.604859  unrelated  0.025206\n",
      "858  0.648668  0.670811  0.635494  0.663649  unrelated -0.014981\n",
      "859  0.656402  0.738486  0.563548  0.592469  unrelated  0.063934\n",
      "860  0.589157  0.606735  0.589157  0.606735  unrelated -0.017578\n",
      "861  0.736358  0.729049  0.683124  0.687004  unrelated  0.049354\n",
      "862  0.702233  0.687835  0.666992  0.665301  unrelated  0.036932\n",
      "863  0.583789  0.579674  0.611894  0.612904  unrelated -0.029115\n",
      "864  0.708588  0.692292  0.716927  0.714696  unrelated -0.006108\n",
      "865  0.694338  0.681137  0.663705  0.648677  unrelated  0.045662\n",
      "866  0.582357  0.624572  0.582357  0.624572  unrelated -0.042215\n",
      "867  0.639784  0.621528  0.639784  0.621528  unrelated  0.018256\n",
      "868  0.585674  0.620665  0.579142  0.602987  unrelated -0.017313\n",
      "869  0.710169  0.702918  0.660097  0.685024  unrelated  0.025144\n",
      "870  0.586480  0.581268  0.577744  0.590010  unrelated -0.003531\n",
      "871  0.638577  0.621561  0.641252  0.649969  unrelated -0.011392\n",
      "872  0.596382  0.609767  0.561950  0.578895  unrelated  0.017487\n",
      "873  0.677748  0.658736  0.638445  0.633506  unrelated  0.044243\n",
      "874  0.693904  0.695938  0.645919  0.645919  unrelated  0.047985\n",
      "875  0.551852  0.578822  0.540911  0.581308  unrelated -0.029456\n",
      "876  0.699526  0.699982  0.681562  0.688948  unrelated  0.010579\n",
      "877  0.673835  0.676139  0.658989  0.656318  unrelated  0.017517\n",
      "878  0.661387  0.649915  0.661387  0.649915  unrelated  0.011473\n",
      "879  0.636579  0.618372  0.625152  0.608256  unrelated  0.028323\n",
      "880  0.600492  0.588086  0.618129  0.619207  unrelated -0.018714\n",
      "881  0.718856  0.713000  0.654768  0.670421  unrelated  0.048434\n",
      "882  0.637703  0.639562  0.613736  0.643276  unrelated -0.005572\n",
      "883  0.708828  0.714425  0.708828  0.714425  unrelated -0.005597\n",
      "884  0.669592  0.660422  0.602792  0.602515  unrelated  0.067077\n",
      "885  0.627193  0.594997  0.625892  0.599795  unrelated  0.027398\n",
      "886  0.728214  0.739327  0.704054  0.736278  unrelated -0.008064\n",
      "887  0.642069  0.617775  0.612798  0.605241  unrelated  0.036828\n",
      "888  0.709331  0.684365  0.674660  0.661427  unrelated  0.047904\n",
      "889  0.674838  0.674496  0.631296  0.644649  unrelated  0.030189\n",
      "890  0.643438  0.625257  0.611975  0.599252  unrelated  0.044186\n",
      "891  0.673052  0.674332  0.600957  0.601648  unrelated  0.071404\n",
      "892  0.690728  0.658976  0.692544  0.674384  unrelated  0.016344\n",
      "893  0.728866  0.729459  0.726400  0.728148  unrelated  0.000718\n",
      "895  0.740685  0.742594  0.693114  0.698618  unrelated  0.042067\n",
      "896  0.694914  0.703925  0.707713  0.714569  unrelated -0.019655\n",
      "897  0.631667  0.644474  0.608466  0.622039  unrelated  0.009628\n",
      "898  0.692921  0.689529  0.692921  0.689529  unrelated  0.003391\n",
      "899  0.634572  0.610738  0.600114  0.613307  unrelated  0.021265\n",
      "900  0.665989  0.654306  0.651225  0.655045  unrelated  0.010944\n",
      "901  0.623742  0.645273  0.623742  0.645273  unrelated -0.021531\n",
      "902  0.657676  0.672561  0.657676  0.672561  unrelated -0.014884\n",
      "903  0.678460  0.705924  0.641509  0.676246  unrelated  0.002215\n",
      "904  0.678081  0.700632  0.629375  0.654828  unrelated  0.023252\n",
      "905  0.708724  0.712287  0.678981  0.694474  unrelated  0.014250\n",
      "906  0.645208  0.645208  0.570199  0.570199  unrelated  0.075009\n",
      "907  0.686028  0.691073  0.658376  0.660609  unrelated  0.025419\n",
      "908  0.651560  0.655701  0.629825  0.645743  unrelated  0.005818\n",
      "909  0.683478  0.705875  0.653697  0.690009  unrelated -0.006531\n",
      "910  0.677122  0.693213  0.611857  0.639276  unrelated  0.037846\n",
      "911  0.633152  0.641170  0.639356  0.639356  unrelated -0.006204\n",
      "912  0.636841  0.636841  0.636841  0.636841  unrelated  0.000000\n",
      "913  0.709082  0.693185  0.690615  0.690615  unrelated  0.018467\n",
      "914  0.674925  0.691114  0.674925  0.691114  unrelated -0.016188\n",
      "915  0.635092  0.635092  0.596451  0.596451  unrelated  0.038641\n",
      "916  0.582927  0.584649  0.582927  0.584649  unrelated -0.001722\n",
      "917  0.689956  0.710737  0.644121  0.675040  unrelated  0.014916\n",
      "918  0.630164  0.630164  0.630164  0.630164  unrelated  0.000000\n",
      "919  0.719504  0.722576  0.635424  0.635424  unrelated  0.084080\n",
      "920  0.576600  0.583230  0.576600  0.583230  unrelated -0.006630\n",
      "921  0.736560  0.740487  0.736560  0.740487  unrelated -0.003927\n",
      "922  0.658043  0.673611  0.644750  0.654775  unrelated  0.003268\n",
      "924  0.690542  0.707385  0.646115  0.646115  unrelated  0.044427\n",
      "925  0.612318  0.612318  0.612318  0.612318  unrelated  0.000000\n",
      "926  0.539960  0.539960  0.539960  0.539960  unrelated  0.000000\n",
      "927  0.657699  0.650810  0.577764  0.582723  unrelated  0.074976\n",
      "928  0.705496  0.702566  0.689698  0.692929  unrelated  0.012567\n",
      "929  0.599467  0.619874  0.634111  0.670918  unrelated -0.071452\n",
      "930  0.671784  0.671417  0.649709  0.652265  unrelated  0.019520\n",
      "931  0.616831  0.616831  0.556281  0.590171  unrelated  0.026660\n",
      "932  0.640532  0.657893  0.653793  0.670623  unrelated -0.030091\n",
      "933  0.767215  0.767215  0.775729  0.775729  unrelated -0.008514\n",
      "934  0.718684  0.711626  0.667461  0.682839  unrelated  0.035845\n",
      "935  0.632740  0.632098  0.636440  0.636440  unrelated -0.003700\n",
      "936  0.625664  0.629819  0.625664  0.629819  unrelated -0.004155\n",
      "937  0.716308  0.712888  0.684218  0.684218  unrelated  0.032090\n",
      "938  0.610816  0.620468  0.625111  0.635341  unrelated -0.024524\n",
      "939  0.687753  0.691498  0.679777  0.709174  unrelated -0.021420\n",
      "940  0.741587  0.748472  0.711745  0.716070  unrelated  0.025517\n",
      "941  0.630807  0.630807  0.598022  0.602645  unrelated  0.028162\n",
      "942  0.684449  0.682341  0.626491  0.645832  unrelated  0.038617\n",
      "943  0.601455  0.601455  0.591125  0.594369  unrelated  0.007086\n",
      "944  0.680215  0.684143  0.602938  0.602938  unrelated  0.077277\n",
      "945  0.672749  0.666794  0.649154  0.649154  unrelated  0.023595\n",
      "946  0.687660  0.696230  0.649124  0.662194  unrelated  0.025466\n",
      "947  0.663916  0.667556  0.626328  0.648299  unrelated  0.015617\n",
      "948  0.647990  0.673524  0.647990  0.673524  unrelated -0.025534\n",
      "949  0.682262  0.685265  0.633213  0.650889  unrelated  0.031373\n",
      "950  0.677815  0.667827  0.669206  0.676097  unrelated  0.001719\n",
      "951  0.627026  0.638350  0.647546  0.647546  unrelated -0.020520\n",
      "952  0.565180  0.565180  0.534656  0.534656  unrelated  0.030523\n",
      "953  0.593249  0.616034  0.573699  0.573481  unrelated  0.019768\n",
      "954  0.729770  0.717633  0.726513  0.731869  unrelated -0.002100\n",
      "955  0.655675  0.655675  0.626551  0.642070  unrelated  0.013606\n",
      "956  0.674867  0.674867  0.670071  0.670071  unrelated  0.004797\n",
      "957  0.761495  0.757937  0.702404  0.707961  unrelated  0.053533\n",
      "958  0.622755  0.622755  0.622755  0.622755  unrelated  0.000000\n",
      "959  0.594137  0.594137  0.528101  0.528101  unrelated  0.066037\n",
      "960  0.659701  0.653359  0.659701  0.653359  unrelated  0.006342\n",
      "961  0.680321  0.690800  0.643405  0.655345  unrelated  0.024976\n",
      "962  0.620160  0.620160  0.642439  0.658258  unrelated -0.038098\n",
      "963  0.692820  0.705896  0.667748  0.682306  unrelated  0.010513\n",
      "964  0.644403  0.645817  0.607744  0.622132  unrelated  0.022271\n",
      "965  0.633147  0.642445  0.627847  0.643673  unrelated -0.010526\n",
      "966  0.639515  0.641682  0.594616  0.619444  unrelated  0.020071\n",
      "967  0.635435  0.635435  0.543442  0.543442  unrelated  0.091993\n",
      "968  0.611697  0.603628  0.610662  0.610662  unrelated  0.001036\n",
      "969  0.620063  0.620063  0.614593  0.614593  unrelated  0.005470\n",
      "970  0.650195  0.640148  0.650195  0.640148  unrelated  0.010047\n",
      "971  0.717951  0.726919  0.700148  0.700270  unrelated  0.017681\n",
      "972  0.689656  0.707107  0.643527  0.650622  unrelated  0.039034\n",
      "973  0.597163  0.604580  0.616482  0.616482  unrelated -0.019319\n",
      "974  0.729129  0.738838  0.670286  0.691643  unrelated  0.037486\n",
      "975  0.573726  0.573726  0.557037  0.557184  unrelated  0.016542\n",
      "976  0.776789  0.776789  0.776789  0.776789  unrelated  0.000000\n",
      "977  0.550802  0.557977  0.495786  0.503388  unrelated  0.047414\n",
      "978  0.644790  0.649405  0.633853  0.641604  unrelated  0.003186\n",
      "979  0.641479  0.647908  0.638676  0.661766  unrelated -0.020287\n",
      "980  0.671163  0.677077  0.594337  0.615261  unrelated  0.055902\n",
      "981  0.673698  0.673698  0.628423  0.628423  unrelated  0.045274\n",
      "982  0.616949  0.616595  0.577499  0.617940  unrelated -0.000990\n",
      "983  0.675339  0.696357  0.671729  0.703357  unrelated -0.028018\n",
      "984  0.681611  0.693591  0.674402  0.684317  unrelated -0.002706\n",
      "985  0.597012  0.597012  0.577892  0.585008  unrelated  0.012004\n",
      "986  0.619331  0.636451  0.619635  0.616810  unrelated  0.002521\n",
      "987  0.679423  0.688376  0.659778  0.659778  unrelated  0.019645\n",
      "988  0.652664  0.657931  0.660493  0.668800  unrelated -0.016136\n",
      "989  0.649323  0.650188  0.640156  0.644832  unrelated  0.004490\n",
      "990  0.663977  0.671469  0.652209  0.663675  unrelated  0.000302\n",
      "991  0.708765  0.713478  0.689123  0.698672  unrelated  0.010093\n",
      "992  0.629351  0.632933  0.648064  0.655402  unrelated -0.026052\n",
      "993  0.695243  0.706783  0.659792  0.672178  unrelated  0.023066\n",
      "994  0.648116  0.674082  0.648116  0.674082  unrelated -0.025966\n",
      "995  0.674799  0.684276  0.676127  0.717257  unrelated -0.042458\n",
      "996  0.620160  0.620160  0.642439  0.658258  unrelated -0.038098\n",
      "997  0.702055  0.707214  0.702055  0.707214  unrelated -0.005159\n",
      "998  0.633660  0.609127  0.633660  0.609127  unrelated  0.024532\n",
      "999  0.709778  0.709894  0.704875  0.704875  unrelated  0.004902\n"
     ]
    }
   ],
   "source": [
    "print(bert_napredak_results_max_unrelated_sum.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stavi dodatno random naslove rank od 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_napredak_results_max_rank = pd.DataFrame(columns = ['sa-sa', 'bez-bez', 'Stance', 'rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reset_index()\n",
    "\n",
    "i=0\n",
    "for index, row in df.iterrows():\n",
    "\n",
    "    j = 0\n",
    "    #get headline_embedding\n",
    "    title = row['Headline']\n",
    "    headline_embedding = getbert(title)\n",
    "    headline_embedding_without = getbert_without(title)\n",
    "            \n",
    "    max_ss = 0\n",
    "    max_bb = 0\n",
    "    \n",
    "    rank = 1\n",
    "    \n",
    "    ############################################################################\n",
    "    #real\n",
    "    \n",
    "    for sentence in split_into_sentences(row['articleBody']):\n",
    "        j += 1 \n",
    "        \n",
    "        ## zelimo average u oba 2 slucaja\n",
    "        \n",
    "        ## sa sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        if(max_ss < cos_sim):\n",
    "            max_ss = cos_sim\n",
    "        \n",
    "        ## bez bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        if(max_bb < cos_sim):\n",
    "            max_bb = cos_sim\n",
    "        \n",
    "    if(j == 0):\n",
    "        j = 1\n",
    "        ## sa sa\n",
    "        sentence_embedding = getbert(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "        max_ss = cos_sim\n",
    "        \n",
    "        ## bez bez\n",
    "        sentence_embedding = getbert_without(sentence)\n",
    "        cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "        max_bb = cos_sim\n",
    "        \n",
    "    #max_ss, max_bb dobiveni\n",
    "    #######################################################################################\n",
    "    #fake\n",
    "    \n",
    "    for t in range (1,11):\n",
    "        \n",
    "        ran_max_bb = 0\n",
    "        ran_max_ss = 0\n",
    "    \n",
    "        randomrow = df.sample()\n",
    "        for sentence in split_into_sentences(randomrow['articleBody'].to_string()):\n",
    "            j += 1 \n",
    "\n",
    "            ## zelimo average u oba 2 slucaja\n",
    "\n",
    "            ## sa sa\n",
    "            sentence_embedding = getbert(sentence)\n",
    "            cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "            if(ran_max_ss < cos_sim):\n",
    "                ran_max_ss = cos_sim\n",
    "\n",
    "            ## bez bez\n",
    "            sentence_embedding = getbert_without(sentence)\n",
    "            cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "            if(ran_max_bb < cos_sim):\n",
    "                ran_max_bb = cos_sim\n",
    "\n",
    "        if(j == 0):\n",
    "            j = 1\n",
    "            ## sa sa\n",
    "            sentence_embedding = getbert(sentence)\n",
    "            cos_sim = 1 - cosine(headline_embedding, sentence_embedding)\n",
    "            ran_max_ss = cos_sim\n",
    "\n",
    "            ## bez bez\n",
    "            sentence_embedding = getbert_without(sentence)\n",
    "            cos_sim = 1 - cosine(headline_embedding_without, sentence_embedding)\n",
    "            ran_max_bb = cos_sim\n",
    "\n",
    "        if(ran_max_ss > max_ss):\n",
    "            rank += 1\n",
    "    #######################################################################################\n",
    "    \n",
    "    \n",
    "    bert_napredak_results_max_rank.loc[i] = [max_ss, max_bb, row['Stance'], rank]\n",
    "    i += 1\n",
    "    print(i)\n",
    "    if(i>=2000):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        sa-sa   bez-bez     Stance rank\n",
      "0    0.627635  0.578464  unrelated    1\n",
      "1    0.612943  0.626350  unrelated    1\n",
      "2    0.689166  0.692413  unrelated    2\n",
      "3    0.597808  0.634351  unrelated    4\n",
      "4    0.660143  0.716698  unrelated    1\n",
      "5    0.671554  0.671554  unrelated    1\n",
      "6    0.684476  0.687796  unrelated    5\n",
      "7    0.656540  0.656540  unrelated    3\n",
      "8    0.650038  0.683190  unrelated    2\n",
      "9    0.578987  0.616212  unrelated    4\n",
      "10   0.721992  0.721992  unrelated    1\n",
      "11   0.647815  0.667102  unrelated    2\n",
      "12   0.562497  0.580671  unrelated    1\n",
      "13   0.681521  0.684489  unrelated    2\n",
      "14   0.638384  0.657983  unrelated    2\n",
      "15   0.696646  0.691727  unrelated    2\n",
      "16   0.601039  0.657830  unrelated    3\n",
      "17   0.645673  0.668799  unrelated    2\n",
      "18   0.710755  0.720303  unrelated    1\n",
      "19   0.665383  0.665383  unrelated    4\n",
      "20   0.635036  0.639836  unrelated    2\n",
      "21   0.536775  0.583193  unrelated    6\n",
      "22   0.580292  0.580292  unrelated    5\n",
      "23   0.684295  0.672309  unrelated    3\n",
      "24   0.855309  0.749235    related    1\n",
      "25   0.600654  0.626703  unrelated    5\n",
      "26   0.665299  0.665299  unrelated    1\n",
      "27   0.593561  0.613033  unrelated    3\n",
      "28   0.645278  0.680367  unrelated    2\n",
      "29   0.763693  0.740241  unrelated    1\n",
      "30   0.600717  0.579857  unrelated    2\n",
      "31   0.612137  0.616911  unrelated    6\n",
      "32   0.655110  0.629482  unrelated    2\n",
      "33   0.623014  0.594028  unrelated    4\n",
      "34   0.589642  0.593917  unrelated    2\n",
      "35   0.606199  0.606199  unrelated    1\n",
      "36   0.765006  0.700125    related    1\n",
      "37   0.726615  0.675509  unrelated    1\n",
      "38   0.632236  0.610797  unrelated    3\n",
      "39   0.664874  0.677214  unrelated    4\n",
      "40   0.662986  0.672157  unrelated    1\n",
      "41   0.684993  0.688421  unrelated    4\n",
      "42   0.630803  0.635285  unrelated    2\n",
      "43   0.683951  0.642576  unrelated    4\n",
      "44   0.623564  0.624378  unrelated    3\n",
      "45   0.625736  0.598746  unrelated    2\n",
      "46   0.572349  0.524447  unrelated    3\n",
      "47   0.642375  0.636340  unrelated    2\n",
      "48   0.748611  0.748611  unrelated    1\n",
      "49   0.643123  0.696748  unrelated    2\n",
      "50   0.627069  0.622886  unrelated    6\n",
      "51   0.615459  0.606098  unrelated    3\n",
      "52   0.640281  0.656231  unrelated    4\n",
      "53   0.693921  0.657565  unrelated    1\n",
      "54   0.642375  0.636340  unrelated    2\n",
      "55   0.677692  0.673714  unrelated    1\n",
      "56   0.658049  0.664131  unrelated    1\n",
      "57   0.671153  0.673355  unrelated    2\n",
      "58   0.722377  0.706512  unrelated    1\n",
      "59   0.589418  0.565291  unrelated    5\n",
      "60   0.672533  0.667628  unrelated    1\n",
      "61   0.564240  0.585740  unrelated    2\n",
      "62   0.653863  0.646655  unrelated    1\n",
      "63   0.583905  0.506514  unrelated    6\n",
      "64   0.677338  0.679043  unrelated    1\n",
      "65   0.679861  0.662073  unrelated    3\n",
      "66   0.608919  0.596041  unrelated    5\n",
      "67   0.638026  0.642430  unrelated    2\n",
      "68   0.668815  0.668815  unrelated    2\n",
      "69   0.715473  0.689930    related    1\n",
      "70   0.559601  0.574455  unrelated    1\n",
      "71   0.593263  0.605774  unrelated    5\n",
      "72   0.697325  0.691110  unrelated    2\n",
      "73   0.703358  0.673026  unrelated    2\n",
      "74   0.742252  0.741509  unrelated    1\n",
      "75   0.582257  0.495011  unrelated    3\n",
      "76   0.645405  0.588138  unrelated    4\n",
      "77   0.671452  0.671366  unrelated    2\n",
      "78   0.581305  0.607154  unrelated    3\n",
      "79   0.622584  0.605523  unrelated    5\n",
      "80   0.697785  0.707608  unrelated    1\n",
      "81   0.713134  0.708035  unrelated    2\n",
      "82   0.643327  0.589989  unrelated    1\n",
      "83   0.722954  0.722954  unrelated    1\n",
      "84   0.561182  0.559665  unrelated    3\n",
      "85   0.653192  0.649301  unrelated    2\n",
      "86   0.716512  0.716512  unrelated    1\n",
      "87   0.608866  0.591799  unrelated    5\n",
      "88   0.782727  0.749278    related    1\n",
      "89   0.639910  0.625301  unrelated    2\n",
      "90   0.627468  0.621438  unrelated    3\n",
      "91   0.643682  0.630032  unrelated    3\n",
      "92   0.626658  0.582147  unrelated    3\n",
      "93   0.805850  0.725149    related    1\n",
      "94   0.655953  0.571008  unrelated    1\n",
      "95   0.675071  0.686621  unrelated    2\n",
      "96   0.637326  0.619302  unrelated    1\n",
      "97   0.740977  0.756137  unrelated    1\n",
      "98   0.717242  0.640419  unrelated    1\n",
      "99   0.701463  0.700221  unrelated    1\n",
      "100  0.736869  0.655476  unrelated    4\n",
      "101  0.663014  0.675883  unrelated    1\n",
      "102  0.594601  0.605834  unrelated    7\n",
      "103  0.745804  0.744207  unrelated    1\n",
      "104  0.746786  0.721957  unrelated    1\n",
      "105  0.753466  0.763103  unrelated    2\n",
      "106  0.592566  0.535731  unrelated    3\n",
      "107  0.818662  0.787995    related    1\n",
      "108  0.730530  0.730530  unrelated    1\n",
      "109  0.625268  0.634234  unrelated    1\n",
      "110  0.672278  0.652998  unrelated    2\n",
      "111  0.664960  0.686107  unrelated    1\n",
      "112  0.569807  0.578023  unrelated    5\n",
      "113  0.606132  0.569587  unrelated    6\n",
      "114  0.689354  0.687092  unrelated    1\n",
      "115  0.795346  0.782410  unrelated    1\n",
      "116  0.602190  0.594034  unrelated    1\n",
      "117  0.796831  0.709587    related    1\n",
      "118  0.726954  0.691529  unrelated    1\n",
      "119  0.636098  0.637245  unrelated    6\n",
      "120  0.772146  0.761285  unrelated    1\n",
      "121  0.645943  0.588726  unrelated    2\n",
      "122  0.780072  0.742982  unrelated    1\n",
      "123  0.709051  0.702575  unrelated    2\n",
      "124  0.718984  0.718984  unrelated    1\n",
      "125  0.777903  0.771226  unrelated    1\n",
      "126  0.646637  0.665732  unrelated    2\n",
      "127  0.738965  0.670865  unrelated    1\n",
      "128  0.730630  0.694920  unrelated    1\n",
      "129  0.621115  0.621115  unrelated    1\n",
      "130  0.706944  0.716360  unrelated    1\n",
      "131  0.743400  0.743400  unrelated    2\n",
      "132  0.695459  0.691970  unrelated    3\n",
      "133  0.719595  0.620798  unrelated    1\n",
      "134  0.667049  0.659997  unrelated    4\n",
      "135  0.667544  0.667544  unrelated    3\n",
      "136  0.631737  0.647394  unrelated    7\n",
      "137  0.749276  0.758892  unrelated    1\n",
      "138  0.606712  0.645703  unrelated    1\n",
      "139  0.775284  0.776427  unrelated    1\n",
      "140  0.776043  0.753043  unrelated    1\n",
      "141  0.634797  0.627512  unrelated    2\n",
      "142  0.717038  0.665619  unrelated    1\n",
      "143  0.622291  0.687783  unrelated    1\n",
      "144  0.791525  0.756642  unrelated    1\n",
      "145  0.722378  0.654856  unrelated    1\n",
      "146  0.680985  0.696623  unrelated    1\n",
      "147  0.630627  0.621449  unrelated    3\n",
      "148  0.554505  0.554505  unrelated    8\n",
      "149  0.755312  0.743656  unrelated    1\n",
      "150  0.711950  0.698113  unrelated    1\n",
      "151  0.706980  0.688798  unrelated    1\n",
      "152  0.542386  0.548149  unrelated    9\n",
      "153  0.765605  0.736662  unrelated    1\n",
      "154  0.629980  0.629980  unrelated    2\n",
      "155  0.741361  0.677043  unrelated    1\n",
      "156  0.736466  0.737106  unrelated    1\n",
      "157  0.685734  0.677706  unrelated    2\n",
      "158  0.634912  0.634912  unrelated    2\n",
      "159  0.621115  0.621115  unrelated    2\n",
      "160  0.646130  0.649807  unrelated    1\n",
      "161  0.702016  0.720170  unrelated    1\n",
      "162  0.691151  0.647637  unrelated    1\n",
      "163  0.736362  0.746880  unrelated    1\n",
      "164  0.707160  0.688551  unrelated    1\n",
      "165  0.705302  0.709269  unrelated    1\n",
      "166  0.726336  0.714644  unrelated    1\n",
      "167  0.718135  0.738625  unrelated    2\n",
      "168  0.761936  0.744011  unrelated    1\n",
      "169  0.637136  0.647419  unrelated    7\n",
      "170  0.699410  0.684604  unrelated    1\n",
      "171  0.716185  0.708079  unrelated    1\n",
      "172  0.587848  0.594015  unrelated    6\n",
      "173  0.719176  0.704104  unrelated    2\n",
      "174  0.529667  0.652256  unrelated    2\n",
      "175  0.677082  0.655158  unrelated    2\n",
      "176  0.734993  0.733591  unrelated    1\n",
      "177  0.619475  0.619475  unrelated    2\n",
      "178  0.714266  0.709223  unrelated    2\n",
      "179  0.735305  0.728322  unrelated    1\n",
      "180  0.663127  0.663127  unrelated    1\n",
      "181  0.663567  0.651134  unrelated    1\n",
      "182  0.811968  0.817353  unrelated    1\n",
      "183  0.667236  0.667236  unrelated    3\n",
      "184  0.704580  0.720730  unrelated    2\n",
      "185  0.621246  0.625359  unrelated    1\n",
      "186  0.737242  0.720658  unrelated    1\n",
      "187  0.802886  0.804385  unrelated    1\n",
      "188  0.712008  0.650142  unrelated    3\n",
      "189  0.712681  0.712681  unrelated    1\n",
      "190  0.661279  0.587561  unrelated    4\n",
      "191  0.645529  0.597175  unrelated    1\n",
      "192  0.709253  0.709253  unrelated    1\n",
      "193  0.715119  0.642421  unrelated    2\n",
      "194  0.749482  0.743461  unrelated    2\n",
      "195  0.735518  0.718605  unrelated    1\n",
      "196  0.735626  0.689871  unrelated    1\n",
      "197  0.696153  0.720015  unrelated    1\n",
      "198  0.750219  0.741556  unrelated    1\n",
      "199  0.810295  0.810295    related    1\n",
      "200  0.740454  0.707173  unrelated    1\n",
      "201  0.638090  0.761198  unrelated    1\n",
      "202  0.748357  0.673271  unrelated    1\n",
      "203  0.715704  0.732869  unrelated    1\n",
      "204  0.718331  0.689426  unrelated    1\n",
      "205  0.667448  0.696625  unrelated    1\n",
      "206  0.696853  0.688188  unrelated    1\n",
      "207  0.725197  0.705440  unrelated    1\n",
      "208  0.730400  0.728357  unrelated    1\n",
      "209  0.744617  0.724620  unrelated    1\n",
      "210  0.726600  0.731074  unrelated    1\n",
      "211  0.686816  0.692720  unrelated    1\n",
      "212  0.747795  0.681118  unrelated    1\n",
      "213  0.743745  0.697137  unrelated    1\n",
      "214  0.687632  0.687632  unrelated    1\n",
      "215  0.726600  0.731074  unrelated    1\n",
      "216  0.740547  0.698380  unrelated    1\n",
      "217  0.654358  0.658760  unrelated    1\n",
      "218  0.718331  0.689426  unrelated    1\n",
      "219  0.696769  0.682873  unrelated    1\n",
      "220  0.712744  0.712744  unrelated    1\n",
      "221  0.788011  0.774394  unrelated    1\n",
      "222  0.771874  0.771874  unrelated    1\n",
      "223  0.815321  0.804050  unrelated    2\n",
      "224  0.755909  0.766516  unrelated    1\n",
      "225  0.716537  0.648287  unrelated    2\n",
      "226  0.613669  0.617241  unrelated    1\n",
      "227  0.714297  0.683968  unrelated    1\n",
      "228  0.781705  0.781705  unrelated    1\n",
      "229  0.710261  0.683784  unrelated    1\n",
      "230  0.728207  0.736629  unrelated    1\n",
      "231  0.780032  0.733227  unrelated    1\n",
      "232  0.718516  0.668600  unrelated    1\n",
      "233  0.753157  0.724498  unrelated    1\n",
      "234  0.752095  0.749957  unrelated    1\n",
      "235  0.648850  0.668634  unrelated    1\n",
      "236  0.724198  0.689711  unrelated    1\n",
      "237  0.780062  0.785761  unrelated    1\n",
      "238  0.770588  0.752828  unrelated    1\n",
      "239  0.721955  0.725439  unrelated    1\n",
      "240  0.748924  0.751743  unrelated    1\n",
      "241  0.719077  0.683731    related    1\n",
      "242  0.793313  0.694733    related    1\n",
      "243  0.719125  0.719125  unrelated    1\n",
      "244  0.720375  0.712010  unrelated    1\n",
      "245  0.578201  0.575236  unrelated    5\n",
      "246  0.700113  0.704043  unrelated    1\n",
      "247  0.640977  0.633393  unrelated    2\n",
      "248  0.645431  0.686663  unrelated    1\n",
      "249  0.664082  0.629552  unrelated    2\n",
      "250  0.599572  0.592011  unrelated    4\n",
      "251  0.648638  0.648638  unrelated    1\n",
      "252  0.547424  0.550036  unrelated    4\n",
      "253  0.626229  0.617415  unrelated    2\n",
      "254  0.632571  0.626756  unrelated    3\n",
      "255  0.738537  0.751831  unrelated    2\n",
      "256  0.724164  0.724164  unrelated    2\n",
      "257  0.639381  0.643289  unrelated    1\n",
      "258  0.594152  0.594234  unrelated    6\n",
      "259  0.692948  0.690172  unrelated    2\n",
      "260  0.642924  0.580334  unrelated    1\n",
      "261  0.547594  0.577306  unrelated    3\n",
      "262  0.744918  0.744918    related    1\n",
      "263  0.767419  0.770516  unrelated    1\n",
      "264  0.543429  0.542147  unrelated    4\n",
      "265  0.566963  0.587174  unrelated    1\n",
      "266  0.693431  0.644558  unrelated    1\n",
      "267  0.636996  0.659461  unrelated    1\n",
      "268  0.777380  0.777380  unrelated    1\n",
      "269  0.599572  0.592011  unrelated    4\n",
      "270  0.608514  0.626649  unrelated    2\n",
      "271  0.629453  0.629453  unrelated    1\n",
      "272  0.806868  0.806868    related    1\n",
      "273  0.620853  0.601620  unrelated    5\n",
      "274  0.645056  0.672158  unrelated    2\n",
      "275  0.547594  0.577306  unrelated    2\n",
      "276  0.565512  0.597621  unrelated    4\n",
      "277  0.583839  0.612101  unrelated    5\n",
      "278  0.642044  0.640695  unrelated    4\n",
      "279  0.602582  0.595055  unrelated    5\n",
      "280  0.697682  0.647768  unrelated    1\n",
      "281  0.594959  0.568625  unrelated    2\n",
      "282  0.730761  0.730761  unrelated    1\n",
      "283  0.626108  0.637125  unrelated    7\n",
      "284  0.604859  0.618062  unrelated    4\n",
      "285  0.573338  0.541371  unrelated    2\n",
      "286  0.601569  0.622237  unrelated    5\n",
      "287  0.697150  0.698809  unrelated    1\n",
      "288  0.685452  0.663675  unrelated    1\n",
      "289  0.571219  0.524131  unrelated    2\n",
      "290  0.677873  0.685721  unrelated    1\n",
      "291  0.656736  0.654090  unrelated    3\n",
      "292  0.654168  0.611846  unrelated    1\n",
      "293  0.672727  0.667751  unrelated    4\n",
      "294  0.571288  0.539690  unrelated    3\n",
      "295  0.702586  0.679599  unrelated    3\n",
      "296  0.779514  0.763882    related    1\n",
      "297  0.618810  0.620429  unrelated    3\n",
      "298  0.675738  0.608267  unrelated    2\n",
      "299  0.699629  0.681803  unrelated    1\n",
      "300  0.682036  0.682036  unrelated    1\n",
      "301  0.667219  0.636078  unrelated    3\n",
      "302  0.773869  0.729186  unrelated    1\n",
      "303  0.701865  0.701865  unrelated    3\n",
      "304  0.682769  0.676261  unrelated    3\n",
      "305  0.688758  0.680736  unrelated    6\n",
      "306  0.586317  0.524755  unrelated    4\n",
      "307  0.633709  0.498330  unrelated    6\n",
      "308  0.690360  0.692024  unrelated    2\n",
      "309  0.681424  0.678817  unrelated    1\n",
      "310  0.651700  0.639801  unrelated    2\n",
      "311  0.701335  0.693895  unrelated    2\n",
      "312  0.664805  0.616965  unrelated    3\n",
      "313  0.690128  0.688708  unrelated    1\n",
      "314  0.661204  0.628656  unrelated    3\n",
      "315  0.717653  0.717653  unrelated    1\n",
      "316  0.632142  0.590994  unrelated    1\n",
      "317  0.647909  0.591565  unrelated    2\n",
      "318  0.657292  0.657292  unrelated    3\n",
      "319  0.715606  0.686035  unrelated    3\n",
      "320  0.697198  0.670210  unrelated    2\n",
      "321  0.720531  0.687558  unrelated    1\n",
      "322  0.633953  0.627592  unrelated    3\n",
      "323  0.665972  0.617694  unrelated    5\n",
      "324  0.692376  0.674290  unrelated    1\n",
      "325  0.619591  0.628019  unrelated    5\n",
      "326  0.653813  0.631051  unrelated    4\n",
      "327  0.715118  0.714006  unrelated    2\n",
      "328  0.628416  0.588176  unrelated    1\n",
      "329  0.636901  0.628123  unrelated    2\n",
      "330  0.651159  0.652832  unrelated    3\n",
      "331  0.740801  0.701860  unrelated    1\n",
      "332  0.662743  0.618519  unrelated    1\n",
      "333  0.659364  0.659364  unrelated    2\n",
      "334  0.622499  0.622499  unrelated    2\n",
      "335  0.580242  0.602386  unrelated    1\n",
      "336  0.653643  0.661799  unrelated    4\n",
      "337  0.676960  0.691148  unrelated    3\n",
      "338  0.724252  0.726456  unrelated    1\n",
      "339  0.696397  0.685699  unrelated    3\n",
      "340  0.694246  0.662867  unrelated    5\n",
      "341  0.679705  0.640352  unrelated    2\n",
      "342  0.664583  0.650012  unrelated    6\n",
      "343  0.608451  0.586975  unrelated    2\n",
      "344  0.676436  0.651231  unrelated    2\n",
      "345  0.634157  0.634157  unrelated    5\n",
      "346  0.635801  0.648573  unrelated    4\n",
      "347  0.714575  0.712386  unrelated    2\n",
      "348  0.699241  0.660785  unrelated    1\n",
      "349  0.688811  0.682738  unrelated    1\n",
      "350  0.702538  0.660768  unrelated    1\n",
      "351  0.641232  0.649477  unrelated    3\n",
      "352  0.627174  0.629320  unrelated    4\n",
      "353  0.594735  0.594735  unrelated    2\n",
      "354  0.713641  0.699337  unrelated    5\n",
      "355  0.592246  0.522814  unrelated    3\n",
      "356  0.622255  0.596032  unrelated    3\n",
      "357  0.616653  0.515427  unrelated    3\n",
      "358  0.636645  0.634054  unrelated    5\n",
      "359  0.595461  0.595461  unrelated    4\n",
      "360  0.699851  0.613625  unrelated    2\n",
      "361  0.658081  0.657560  unrelated    2\n",
      "362  0.641277  0.580012  unrelated    1\n",
      "363  0.915655  0.920901    related    1\n",
      "364  0.746607  0.725624  unrelated    1\n",
      "365  0.663430  0.669905  unrelated    1\n",
      "366  0.754779  0.741640  unrelated    1\n",
      "367  0.622678  0.616681  unrelated    4\n",
      "368  0.723877  0.663135  unrelated    2\n",
      "369  0.744090  0.730627  unrelated    1\n",
      "370  0.692101  0.705298  unrelated    2\n",
      "371  0.717587  0.717587  unrelated    1\n",
      "372  0.643245  0.623536  unrelated    2\n",
      "373  0.627175  0.654189  unrelated    7\n",
      "374  0.716966  0.714220  unrelated    2\n",
      "375  0.851319  0.863322    related    1\n",
      "376  0.712645  0.592111  unrelated    1\n",
      "377  0.686003  0.706122  unrelated    1\n",
      "378  0.687599  0.703049  unrelated    3\n",
      "379  0.632910  0.614212  unrelated    1\n",
      "380  0.650040  0.621159  unrelated    1\n",
      "381  0.653701  0.651060  unrelated    3\n",
      "382  0.712616  0.665458  unrelated    2\n",
      "383  0.652756  0.607681  unrelated    2\n",
      "384  0.725635  0.695533  unrelated    1\n",
      "385  0.757565  0.759622  unrelated    1\n",
      "386  0.775219  0.737626    related    1\n",
      "387  0.649698  0.611025  unrelated    3\n"
     ]
    }
   ],
   "source": [
    "print(bert_napredak_results_max_rank.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "berttry",
   "language": "python",
   "name": "berttry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
